{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# NOTEBOOK 1: Create and Save Merged Model\n# ==============================================================================\n\n# --- CELL 1: Setup ---\nimport os\n# Force the environment to see only one GPU to guarantee stability\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Install all necessary libraries\n!pip install \"transformers==4.54.1\" -qU\n!pip install \"wandb>=0.17.0\" -qU\n!pip install --no-deps \"bitsandbytes>=0.43.1\" \"accelerate>=0.31.0\" \"xformers==0.0.29.post3\" \"trl>=0.9.4\" triton -q\n!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth.git -q\n!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth-zoo.git -q\n!pip install -U peft -q\n!pip install \"timm>=1.0.16\" -qU\n\n# --- CELL 2: W&B Login ---\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nwandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\nwandb.login(key=wandb_api_key)\n\n# --- CELL 3: Download Adapters ---\nrun = wandb.init(project=\"mc-s-&-e-n\")\nartifact_path = 'jdmasciano2-university-of-lagos/mc-s-&-e-n/maize-adapters-icy-sweep-2:v0'\nartifact = run.use_artifact(artifact_path, type='model')\nchampion_adapters_path = artifact.download()\nrun.finish()\n\n\n# ==============================================================================\n# CELL 4: Load, Configure PEFT, and Save Merged with the Official Unsloth Method\n# ==============================================================================\nfrom unsloth import FastVisionModel\nimport torch\n\n# The full, 4B parameter model ID from Unsloth\nMODEL_NAME_E4B = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\"\n# Define a clean output path for Colab\nmerged_model_path = \"/kaggle/working/AuraMind-E4B-Finetuned-Merged/\"\n\nprint(\"--- Step 2: Loading Full E4B Base Model in 4-bit ---\")\nmodel, tokenizer = FastVisionModel.from_pretrained(\n    model_name = MODEL_NAME_E4B,\n    max_seq_length = 2048,\n    dtype = None,\n    load_in_4bit = True,\n    device_map = {\"\": \"cuda:0\"}\n)\nprint(\"✅ Base model loaded.\")\n\n# --- Step 3: MUST Apply PEFT Configuration to the Model FIRST ---\n# This is critical. It wraps the model in the PeftModel class.\nprint(\"\\n--- Step 3: Applying PEFT Configuration to the Model ---\")\nmodel = FastVisionModel.get_peft_model(\n    model,\n    r = 16, # Must match your champion adapter's config\n    lora_alpha = 16, # Must match your champion adapter's config\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n    finetune_vision_layers = True, # Must match your champion adapter's config\n    finetune_language_layers = True,\n)\nprint(\"✅ PEFT configuration applied.\")\n\nprint(\"\\n--- Step 4: Loading Our Champion Adapter Weights ---\")\n# Now, we load our trained weights into the correctly configured PEFT model.\nmodel.load_adapter(champion_adapters_path, adapter_name=\"default\")\nprint(\"✅ Champion LoRA adapters loaded.\")\n\n# --- Step 5: Merge and Save Using the OFFICIAL Unsloth Function ---\n# This single function handles the merge and save operation correctly for 4-bit models.\n# It replaces both `merge_and_unload` and `save_pretrained`.\nprint(f\"\\n--- Step 5: Merging and Saving to Float16 ---\")\nmodel.save_pretrained_merged(merged_model_path, tokenizer, save_method=\"float16\")\nprint(f\"✅ Merged model successfully saved to: {merged_model_path}\")","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: Strategic Cleanup\n# ==============================================================================\nimport shutil\nimport os\n\nprint(\"--- Starting final cleanup ---\")\n\n# List everything in the /kaggle/working/ directory\nfor item in os.listdir('/kaggle/working/'):\n  item_path = os.path.join('/kaggle/working/', item)\n  # Check if it's NOT our target directory\n  if item != 'AuraMind-E4B-Finetuned-Merged':\n    print(f\"Deleting: {item_path}\")\n    # Use shutil.rmtree for directories and os.remove for files\n    if os.path.isdir(item_path):\n      shutil.rmtree(item_path)\n    else:\n      os.remove(item_path)\n\nprint(\"\\n✅ Cleanup complete. Only the merged model folder remains.\")\n!ls -l /kaggle/working/","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}