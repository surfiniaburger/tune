{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12443751,"sourceType":"datasetVersion","datasetId":7849609},{"sourceId":12497851,"sourceType":"datasetVersion","datasetId":7887454},{"sourceId":12510604,"sourceType":"datasetVersion","datasetId":7896473},{"sourceId":12556219,"sourceType":"datasetVersion","datasetId":7928395},{"sourceId":12611397,"sourceType":"datasetVersion","datasetId":7966572}],"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# CELL 0: ENVIRONMENT SETUP (THE DEFINITIVE STABILITY FIX)\n# This cell MUST be run first.\n# ==============================================================================\nimport os\n\n# This command tells the CUDA driver to only make the first GPU (GPU 0) visible\n# to this notebook. All libraries (PyTorch, Unsloth, Accelerate) will now\n# believe this is a single-GPU machine, eliminating all device placement errors.\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\nprint(\"✅ Environment configured to use only a single GPU (GPU 0).\")\nprint(\"This will prevent multi-GPU errors.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:16:29.121361Z","iopub.execute_input":"2025-08-19T16:16:29.121693Z","iopub.status.idle":"2025-08-19T16:16:29.126138Z","shell.execute_reply.started":"2025-08-19T16:16:29.121667Z","shell.execute_reply":"2025-08-19T16:16:29.125459Z"}},"outputs":[{"name":"stdout","text":"✅ Environment configured to use only a single GPU (GPU 0).\nThis will prevent multi-GPU errors.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%capture\n# Install latest transformers for Gemma 3N\n!pip install transformers=4.45.1\n!pip install --no-deps git+https://github.com/huggingface/transformers.git -qU # Only for Gemma 3N \n!pip install --no-deps --upgrade timm # Only for Gemma 3N","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:16:29.126913Z","iopub.execute_input":"2025-08-19T16:16:29.127167Z","iopub.status.idle":"2025-08-19T16:17:06.212672Z","shell.execute_reply.started":"2025-08-19T16:16:29.127137Z","shell.execute_reply":"2025-08-19T16:17:06.211640Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"%%capture\n# ==============================================================================\n# CELL 1: Install all necessary libraries (Same as Colab)\n# ==============================================================================\n\n!pip install wandb -qU\n!pip install weave\n!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3  trl triton cut_cross_entropy \n!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n!pip install -U peft\n#!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth-zoo.git\n#!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install unsloth==2025.7.10 unsloth-zoo==2025.7.11 --no-cache -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:17:06.215052Z","iopub.execute_input":"2025-08-19T16:17:06.215320Z","iopub.status.idle":"2025-08-19T16:19:13.027758Z","shell.execute_reply.started":"2025-08-19T16:17:06.215295Z","shell.execute_reply":"2025-08-19T16:19:13.026755Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: Login to Weights & Biases\n# ==============================================================================\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\n# --- PRE-REQUISITE ---\n# 1. Add your W&B API key as a secret in Kaggle with the label \"wandb_api_key\".\n# 2. This keeps your key secure and private.\n# ---------------------\n\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n    wandb.login(key=wandb_api_key)\n    print(\"✅ Successfully logged into Weights & Biases.\")\nexcept Exception as e:\n    print(\"Could not log into W&B. Please ensure the 'wandb_api_key' secret is set in your Kaggle notebook.\")\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:19:13.028963Z","iopub.execute_input":"2025-08-19T16:19:13.029267Z","iopub.status.idle":"2025-08-19T16:19:21.134660Z","shell.execute_reply.started":"2025-08-19T16:19:13.029241Z","shell.execute_reply":"2025-08-19T16:19:21.134050Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjdmasciano2\u001b[0m (\u001b[33mjdmasciano2-university-of-lagos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"✅ Successfully logged into Weights & Biases.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"\n# ==============================================================================\n# CELL 3: Copy Datasets to Working Directory (No Changes)\n# ==============================================================================\nfrom datasets import Dataset, Image as HFImage\nfrom pathlib import Path\nimport os\n\n# --- NEW STEP: Copy data to the faster working directory ---\nsource_path = \"/kaggle/input/maize-dataset/\"\nlocal_path = \"/kaggle/working/local_datasets/\"\n\nif not os.path.exists(local_path):\n    print(f\"Copying data from {source_path} to {local_path} for faster access...\")\n    !cp -r {source_path} {local_path}\n    print(\"✅ Data copy complete.\")\nelse:\n    print(f\"✅ Data already copied to {local_path}\")\n# ---------------------------------------------------------\n\n\n# --- NEW STEP: Copy data to the faster working directory ---\nsource_path2 = \"/kaggle/input/aura-mind-maize-validation/\"\nlocal_path2 = \"/kaggle/working/validation_datasets/\"\n\nif not os.path.exists(local_path2):\n    print(f\"Copying data from {source_path2} to {local_path2} for faster access...\")\n    !cp -r {source_path2} {local_path2}\n    print(\"✅ Data copy complete.\")\nelse:\n    print(f\"✅ Data already copied to {local_path2}\")\n# ---------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:19:21.135384Z","iopub.execute_input":"2025-08-19T16:19:21.135885Z","iopub.status.idle":"2025-08-19T16:19:21.143260Z","shell.execute_reply.started":"2025-08-19T16:19:21.135865Z","shell.execute_reply":"2025-08-19T16:19:21.142624Z"}},"outputs":[{"name":"stdout","text":"✅ Data already copied to /kaggle/working/local_datasets/\n✅ Data already copied to /kaggle/working/validation_datasets/\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n# CELL 4: Prepare the Dataset as a Python List (FINAL CORRECTED VERSION)\n# ==============================================================================\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\n\n# --- DEFINE THE FUNCTION FIRST ---\ndef create_conversation_dict(image_path, class_name):\n    \"\"\"Creates the final dictionary structure for a single sample.\"\"\"\n    display_name = CLASS_NAME_MAPPING.get(class_name, \"Unknown Maize Condition\")\n    \n    # Load the actual image object here\n    pil_image = Image.open(image_path).convert(\"RGB\")\n    \n    return {\n        \"messages\": [\n            { \"role\": \"user\",\n              \"content\": [\n                {\"type\": \"text\", \"text\": \"Classify the condition of this maize plant. Choose from: Healthy Maize Plant, Maize Phosphorus Deficiency.\"},\n                # The PIL Image object goes directly here\n                {\"type\": \"image\", \"image\": pil_image}\n              ]\n            },\n            { \"role\": \"assistant\",\n              \"content\": [\n                {\"type\": \"text\", \"text\": f\"This is a {display_name}.\"}\n              ]\n            },\n        ]\n    }\n\n# --- THEN, DEFINE YOUR MAPPING ---\nCLASS_NAME_MAPPING = {\n    \"maize_healthy\": \"Healthy Maize Plant\",\n    \"phosphorus_deficiency\": \"Maize Phosphorus Deficiency\",\n}\n\n# --- FINALLY, RUN THE WORKFLOW ---\n\n# 1. Point to the directory and get the list of STRING paths\ndataset_path = Path(\"/kaggle/working/local_datasets/\")\nimage_paths = list(dataset_path.glob(\"**/*.jpg\")) + list(dataset_path.glob(\"**/*.jpeg\"))\nprint(f\"Found {len(image_paths)} images.\")\n\n# 2. Loop through the paths and create the final Python list directly\nprint(\"Creating the final dataset list...\")\nfinal_dataset_list = []\n# Use tqdm for a progress bar\nfor path in tqdm(image_paths, desc=\"Processing images\"):\n    class_folder_name = path.parent.name\n    final_dataset_list.append(create_conversation_dict(path, class_folder_name))\n\nprint(\"\\n✅ Dataset preparation complete!\")\nprint(\"\\nExample of the final data format:\")\n# We print the structure to confirm the PIL Image object is now inside\nprint(final_dataset_list[0])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:19:21.143949Z","iopub.execute_input":"2025-08-19T16:19:21.144216Z","iopub.status.idle":"2025-08-19T16:19:30.606741Z","shell.execute_reply.started":"2025-08-19T16:19:21.144174Z","shell.execute_reply":"2025-08-19T16:19:30.605896Z"}},"outputs":[{"name":"stdout","text":"Found 176 images.\nCreating the final dataset list...\n","output_type":"stream"},{"name":"stderr","text":"Processing images: 100%|██████████| 176/176 [00:09<00:00, 18.66it/s]","output_type":"stream"},{"name":"stdout","text":"\n✅ Dataset preparation complete!\n\nExample of the final data format:\n{'messages': [{'role': 'user', 'content': [{'type': 'text', 'text': 'Classify the condition of this maize plant. Choose from: Healthy Maize Plant, Maize Phosphorus Deficiency.'}, {'type': 'image', 'image': <PIL.Image.Image image mode=RGB size=4160x3120 at 0x7B04BA2EA990>}]}, {'role': 'assistant', 'content': [{'type': 'text', 'text': 'This is a Maize Phosphorus Deficiency.'}]}]}\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: Define the W&B Sweep Configuration\n# ==============================================================================\n# ==============================================================================\n# CELL 5: New Sweep Config using num_train_epochs\n# ==============================================================================\nimport yaml\nimport wandb\n\nsweep_config = {\n    'method': 'bayes',\n    'metric': {\n        'name': 'eval_accuracy',\n        'goal': 'maximize'\n    },\n    'parameters': {\n        'learning_rate': {\n            'distribution': 'log_uniform_values',\n            'min': 5e-6,\n            'max': 1e-4\n        },\n        # We now use a fixed number of epochs, which is a great idea for this small dataset.\n        'num_train_epochs': {\n            'values': [1] # Let's test 1 and 2 epochs\n        },\n        'lora_r': {\n            'values': [16, 32, 64]\n        },\n        'lora_alpha_multiplier': {\n             'values': [1, 2]\n        },\n        'lora_dropout': {\n            'values': [0.05, 0.1]\n        },\n        'weight_decay': {\n            'values': [0.01, 0.05]\n        }\n    }\n}\n\nprint(\"New Sweep Configuration (Epochs-based):\")\nprint(yaml.dump(sweep_config))\n\n# Initialize a new sweep with this configuration\n#sweep_id_epochs = wandb.sweep(sweep_config_epochs, project=\"e4b-correct-test-final-sweep\")\n\n# Make sure to create a new sweep ID with this config\n# sweep_id = wandb.sweep(sweep_config, project=\"your-project-name\")\n\n# Initialize the sweep on the W&B server\nsweep_id = wandb.sweep(sweep_config, project=\"e4b-correct-test-x4\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:19:30.609760Z","iopub.execute_input":"2025-08-19T16:19:30.610175Z","iopub.status.idle":"2025-08-19T16:19:51.948483Z","shell.execute_reply.started":"2025-08-19T16:19:30.610150Z","shell.execute_reply":"2025-08-19T16:19:51.947811Z"}},"outputs":[{"name":"stdout","text":"Sweep Configuration:\nmethod: bayes\nmetric:\n  goal: minimize\n  name: train/loss\nparameters:\n  learning_rate:\n    distribution: log_uniform_values\n    max: 0.0001\n    min: 5.0e-06\n  lora_alpha_multiplier:\n    values:\n    - 1\n    - 2\n  lora_r:\n    values:\n    - 8\n    - 16\n    - 32\n  num_train_epochs:\n    values:\n    - 1\n    - 2\n    - 3\n\nCreate sweep with ID: 3vfe4mo5\nSweep URL: https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-3/sweeps/3vfe4mo5\n","output_type":"stream"}],"execution_count":9},{"cell_type":"code","source":"# ==============================================================================\n# CELL 6: EVALUATION FRAMEWORK (ULTIMATE SIMPLICITY VERSION)\n# ==============================================================================\nimport weave\nfrom PIL import Image\nfrom pathlib import Path\nfrom tqdm import tqdm\nimport torch\nimport transformers\n\n# --- All setup code remains the same ---\nCLASS_NAME_MAPPING = {\n    \"maize_healthy\": \"This is a Healthy Maize Plant.\",\n    \"phosphorus_deficiency\": \"This is a Maize Phosphorus Deficiency.\",\n}\nprint(\"Building evaluation dataset from validation files...\")\nvalidation_data_path = Path(\"/kaggle/working/validation_datasets/\")\neval_image_paths = list(validation_data_path.glob(\"**/*.jpg\")) + list(validation_data_path.glob(\"**/*.jpeg\"))\neval_dataset = []\nfor path in tqdm(eval_image_paths, desc=\"Processing validation images\"):\n    class_folder_name = path.parent.name\n    target_label = CLASS_NAME_MAPPING.get(class_folder_name)\n    if target_label:\n        eval_dataset.append({\n            \"image_path\": str(path),\n            \"question\": \"Classify the condition of this maize plant. Choose from: Healthy Maize Plant, Maize Phosphorus Deficiency.\",\n            \"target\": target_label,\n        })\nprint(f\"✅ Created an evaluation dataset with {len(eval_dataset)} examples.\")\n\n# --- Define the Weave Model (WITHOUT TYPE HINTS) ---\nclass MaizeExpertModel(weave.Model):\n    # THE FINAL FIX: By removing the strict type hints, we avoid the Pydantic\n    # validation error. The code will rely on \"duck typing\" - as long as the\n    # objects have the right methods, it will work.\n    model: any\n    processor: any\n\n    @weave.op()\n    @torch.inference_mode()\n    def predict(self, image_path: str, question: str) -> dict:\n        image = Image.open(image_path).convert(\"RGB\")\n        messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": question}, {\"type\": \"image\", \"image\": image}]}]\n        text_prompt = self.processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n        inputs = self.processor(text=text_prompt, images=image, return_tensors=\"pt\").to(self.model.device)\n        outputs = self.model.generate(**inputs, max_new_tokens=20, use_cache=True)\n        response = self.processor.batch_decode(outputs, skip_special_tokens=True)[0]\n        prompt_marker = \"model\\n\"\n        answer_start_index = response.rfind(prompt_marker)\n        final_answer = response[answer_start_index + len(prompt_marker):].strip() if answer_start_index != -1 else \"Could not parse.\"\n        return {\"generated_text\": final_answer}\n\n# --- 4. Define the Intelligent Scorer ---\n@weave.op()\ndef calculate_accuracy(target: str, output: dict) -> dict:\n    \"\"\"\n    Calculates accuracy by checking for keywords (\"Healthy\", \"Phosphorus\", \"Maize\")\n    in the model's prediction, making it robust to phrasing changes.\n    \"\"\"\n    prediction = output.get('generated_text', '').lower() # Convert to lowercase for case-insensitivity\n    \n    # Extract the key diagnostic word from the target\n    # Ex: \"This is a Healthy Maize Plant.\" -> \"healthy\"\n    # Ex: \"This is a Maize Phosphorus Deficiency.\" -> \"phosphorus\"\n    target_keyword = \"\"\n    if \"healthy\" in target.lower():\n        target_keyword = \"healthy\"\n    elif \"phosphorus\" in target.lower():\n        target_keyword = \"phosphorus\"\n    \n    # A prediction is correct if it contains BOTH \"maize\" and the target keyword\n    is_correct = 1 if \"maize\" in prediction and target_keyword in prediction else 0\n    return {\"accuracy\": is_correct}\n\n# --- 5. Define the Async Evaluation Wrapper ---\nasync def evaluate_and_log(model, processor, eval_dataset):\n    print(\"\\n🔬 Starting evaluation...\")\n    eval_model = MaizeExpertModel(model=model, processor=processor)\n    evaluation = weave.Evaluation(dataset=eval_dataset, scorers=[calculate_accuracy])\n    results = await evaluation.evaluate(eval_model)\n    print(f\"✅ Evaluation complete. Full results object: {results}\")\n\nprint(\"✅ W&B Weave evaluation components are updated and ready.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:19:51.949262Z","iopub.execute_input":"2025-08-19T16:19:51.949518Z","iopub.status.idle":"2025-08-19T16:19:58.167017Z","shell.execute_reply.started":"2025-08-19T16:19:51.949490Z","shell.execute_reply":"2025-08-19T16:19:58.166240Z"}},"outputs":[{"name":"stderr","text":"error: XDG_RUNTIME_DIR not set in the environment.\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\nALSA lib confmisc.c:855:(parse_card) cannot find card '0'\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_card_inum returned error: No such file or directory\nALSA lib confmisc.c:422:(snd_func_concat) error evaluating strings\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_concat returned error: No such file or directory\nALSA lib confmisc.c:1334:(snd_func_refer) error evaluating name\nALSA lib conf.c:5178:(_snd_config_evaluate) function snd_func_refer returned error: No such file or directory\nALSA lib conf.c:5701:(snd_config_expand) Evaluate error: No such file or directory\nALSA lib pcm.c:2664:(snd_pcm_open_noupdate) Unknown PCM default\n","output_type":"stream"},{"name":"stdout","text":"Building evaluation dataset from validation files...\n","output_type":"stream"},{"name":"stderr","text":"Processing validation images: 100%|██████████| 21/21 [00:00<00:00, 90061.74it/s]","output_type":"stream"},{"name":"stdout","text":"✅ Created an evaluation dataset with 21 examples.\n✅ W&B Weave evaluation components are updated and ready.\n","output_type":"stream"},{"name":"stderr","text":"\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# ==============================================================================\n# CELL 7: Create the Main Training Function for the W&B Agent (CORRECTED)\n# ==============================================================================\nfrom unsloth import FastVisionModel, FastModel\nfrom transformers import AutoProcessor\nfrom trl import SFTTrainer, SFTConfig\nfrom unsloth.trainer import UnslothVisionDataCollator\nimport torch\nimport gc # Import the garbage collector module\nimport asyncio \n\n\ndef train():\n    \"\"\"\n    This function is called by the W&B agent. It contains the entire\n    model setup, training, and saving logic.\n    A `try...finally` block has been added to ensure robust memory cleanup\n    after each run completes or fails.\n    \"\"\"\n    # Initialize variables to None to ensure they exist for the 'finally' block\n    model, processor, trainer, run = None, None, None, None\n    \n    try:\n        run = wandb.init()\n        \n        WANDB_CONFIG = wandb.config\n        lora_r_value = WANDB_CONFIG.lora_r\n        learning_rate_value = WANDB_CONFIG.learning_rate\n        epochs_value = WANDB_CONFIG.num_train_epochs\n        lora_alpha_value = lora_r_value * WANDB_CONFIG.lora_alpha_multiplier\n        #max_steps_value = WANDB_CONFIG.max_steps\n        lora_dropout_value = WANDB_CONFIG.lora_dropout\n        weight_decay_value = WANDB_CONFIG.weight_decay\n\n        MODEL_NAME = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\"\n\n        print(\"--- New W&B Run ---\")\n        print(f\"Parameters: LR={learning_rate_value}, Epochs Value={epochs_value}, LoRA r={lora_r_value}, LoRA alpha={lora_alpha_value}, Dropout={lora_dropout_value}, Weight Decay={weight_decay_value}\")\n\n        model, tokenizer = FastVisionModel.from_pretrained(\n            model_name=MODEL_NAME,\n            max_seq_length=2048, # max_seq_length is correctly set here\n            dtype=None,\n            load_in_4bit=True,\n            device_map={\"\":\"cuda:0\"}\n            \n        )\n        processor = AutoProcessor.from_pretrained(MODEL_NAME)\n        print(\"✅ Base model, tokenizer, and processor loaded.\")\n\n        model = FastVisionModel.get_peft_model(\n            model,\n            r=lora_r_value,\n            lora_alpha=lora_alpha_value,\n            finetune_vision_layers=True,\n            finetune_language_layers=True,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        )\n        print(\"✅ PEFT adapters added.\")\n\n        FastModel.for_training(model)\n        trainer = SFTTrainer(\n            model=model,\n            train_dataset=final_dataset_list,\n            # The 'processing_class' argument is deprecated; data_collator handles this.\n            # Removed for clarity.\n            data_collator=UnslothVisionDataCollator(model, processor=processor),\n            args=SFTConfig(\n                output_dir=f\"./outputs_{run.name}\",\n                report_to=\"wandb\",\n                num_train_epochs=epochs_value,\n                learning_rate=learning_rate_value,\n                per_device_train_batch_size=2,\n                gradient_accumulation_steps=4,\n                gradient_checkpointing=False,\n                remove_unused_columns=False,\n                dataset_text_field=\"\",\n                dataset_kwargs={\"skip_prepare_dataset\": True},\n                # THIS LINE IS THE FIX: 'max_seq_length' has been removed.\n                warmup_ratio=0.1,\n                optim=\"adamw_torch_fused\",\n                save_strategy=\"no\",\n                seed=3407,\n            ),\n        )\n\n        print(f\"\\n🔥 Starting training run: {run.name}...\")\n        trainer.train()\n        print(\"✅ Training complete!\")\n\n        output_save_dir = f\"/kaggle/working/maize_expert_adapters_{run.name}\"\n        model.save_pretrained(output_save_dir)\n        tokenizer.save_pretrained(output_save_dir)\n        print(f\"✅ Model adapters saved to {output_save_dir}\")\n\n        artifact = wandb.Artifact(f'maize-adapters-{run.name}', type='model')\n        artifact.add_dir(output_save_dir)\n        run.log_artifact(artifact)\n        print(\"✅ Adapters logged as a W&B Artifact.\")\n        \n        # 2. THE FIX: Use asyncio.run() to execute the async function\n        # Call the isolated async evaluation function\n        asyncio.run(evaluate_and_log(model, processor, eval_dataset))\n        \n\n            \n    finally:\n        print(\"\\n🧹 Starting cleanup for next run...\")\n        if run:\n            run.finish()\n\n        # THE FINAL FIX: Safely delete each variable only if it was created.\n        if 'eval_model' in locals() and eval_model is not None: del eval_model\n        if 'trainer' in locals() and trainer is not None: del trainer\n        if 'model' in locals() and model is not None: del model\n        if 'processor' in locals() and processor is not None: del processor\n        if 'tokenizer' in locals() and tokenizer is not None: del tokenizer        \n        \n        gc.collect()\n        torch.cuda.empty_cache()\n        print(\"✅ Memory cleared. Ready for the next agent run.\")\n\n\n\n# Execute the sweep agent\n# This remains the same\nwandb.agent(sweep_id, function=train, count=10)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-19T16:19:58.167890Z","iopub.execute_input":"2025-08-19T16:19:58.168134Z","iopub.status.idle":"2025-08-19T16:28:49.409547Z","shell.execute_reply.started":"2025-08-19T16:19:58.168117Z","shell.execute_reply":"2025-08-19T16:28:49.408756Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-19 16:20:05.515341: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755620405.734343      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755620405.797632      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"name":"stdout","text":"🦥 Unsloth Zoo will now patch everything to make training faster!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Changes to your `wandb` environment variables will be ignored because your `wandb` session has already started. For more information on how to modify your settings with `wandb.init()` arguments, please refer to <a href='https://wandb.me/wandb-init' target=\"_blank\">the W&B docs</a>."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Tracking run with wandb version 0.21.1"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Run data is saved locally in <code>/kaggle/working/wandb/run-20250819_162027-hsyhgmxb</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Syncing run <strong><a href='https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4/runs/hsyhgmxb' target=\"_blank\">diagnostic-run-fp16-fix</a></strong> to <a href='https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View project at <a href='https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4</a>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run at <a href='https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4/runs/hsyhgmxb' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4/runs/hsyhgmxb</a>"},"metadata":{}},{"name":"stdout","text":"--- Starting Diagnostic Run (with Mixed Precision Fix) ---\nFixed Parameters: LR=2e-05, Epochs=1, LoRA r=16, LoRA alpha=32\n==((====))==  Unsloth 2025.7.10: Fast Gemma3N patching. Transformers: 4.56.0.dev0.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"26ef600876ef4715826c7a7434522036"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Fetching 3 files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3a1b8f9984984de087a1ddf5ea3aef06"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00003.safetensors:   0%|          | 0.00/3.72G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea14b926a2ad472a9eafc15fcd6f6efd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00003-of-00003.safetensors:   0%|          | 0.00/1.15G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9675aecd077342fcb7b360dc936cc385"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00003.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"34365506d0654adcb08473946a5800b4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"252cd9218dfc4139b6dc32eba0c8977d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/210 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c1bd163520ec49a3926df283273c3dda"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"771c876d58af41138326b5245931893b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ff645e1b984047d9b211ee2ea0e16124"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2eaa248d34ee42aa8143f6accf37a51d"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d3dd6df7d5c4a3c8c614b427250d916"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ea00ded06df24a54a658879e49276ee0"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"6d09cb0b6f3e4af196ac41fc2ec1de96"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e9c1f5860edb492a80f68bbee905151e"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"processor_config.json:   0%|          | 0.00/98.0 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"17e8a1bc8a2a441a8b08ae7001b1fa73"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7d740664525c4602acf340607e05eb94"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"preprocessor_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"36330417c6364ab4b4ddffcdc06643c8"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f599a06cbda04dbbbfdec354789836c4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b0149ddc6a544f03a7ca793383079133"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70f52daadcd94813b3f937f6bd232606"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"1c2058bcd8a34a9f83608a50bc814be3"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/777 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"256631485dde42a7894faddb0979fd6e"}},"metadata":{}},{"name":"stdout","text":"✅ Base model, tokenizer, and processor loaded.\nUnsloth: Making `model.base_model.model.model.language_model` require gradients\n✅ PEFT adapters added.\nUnsloth: Model does not have a default image size - using 512\n\n🔥 Starting training run: diagnostic-run-fp16-fix...\n","output_type":"stream"},{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 176 | Num Epochs = 1 | Total steps = 22\nO^O/ \\_/ \\    Batch size per device = 2 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (2 x 4 x 1) = 8\n \"-____-\"     Trainable parameters = 40,189,952 of 7,890,168,144 (0.51% trained)\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n","output_type":"stream"},{"name":"stdout","text":"Unsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='22' max='22' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [22/22 03:24, Epoch 1/1]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>1</td>\n      <td>14.398300</td>\n    </tr>\n    <tr>\n      <td>2</td>\n      <td>14.347800</td>\n    </tr>\n    <tr>\n      <td>3</td>\n      <td>14.626900</td>\n    </tr>\n    <tr>\n      <td>4</td>\n      <td>14.529100</td>\n    </tr>\n    <tr>\n      <td>5</td>\n      <td>14.244500</td>\n    </tr>\n    <tr>\n      <td>6</td>\n      <td>14.497100</td>\n    </tr>\n    <tr>\n      <td>7</td>\n      <td>14.472600</td>\n    </tr>\n    <tr>\n      <td>8</td>\n      <td>14.353500</td>\n    </tr>\n    <tr>\n      <td>9</td>\n      <td>14.430200</td>\n    </tr>\n    <tr>\n      <td>10</td>\n      <td>14.452400</td>\n    </tr>\n    <tr>\n      <td>11</td>\n      <td>14.391100</td>\n    </tr>\n    <tr>\n      <td>12</td>\n      <td>14.521100</td>\n    </tr>\n    <tr>\n      <td>13</td>\n      <td>14.301100</td>\n    </tr>\n    <tr>\n      <td>14</td>\n      <td>14.549600</td>\n    </tr>\n    <tr>\n      <td>15</td>\n      <td>14.375900</td>\n    </tr>\n    <tr>\n      <td>16</td>\n      <td>14.353800</td>\n    </tr>\n    <tr>\n      <td>17</td>\n      <td>14.375000</td>\n    </tr>\n    <tr>\n      <td>18</td>\n      <td>14.346600</td>\n    </tr>\n    <tr>\n      <td>19</td>\n      <td>4.102900</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>2.685300</td>\n    </tr>\n    <tr>\n      <td>21</td>\n      <td>1.680900</td>\n    </tr>\n    <tr>\n      <td>22</td>\n      <td>1.299600</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"✅ Training complete!\n\n🧹 Starting cleanup...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":""},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br>    <style><br>        .wandb-row {<br>            display: flex;<br>            flex-direction: row;<br>            flex-wrap: wrap;<br>            justify-content: flex-start;<br>            width: 100%;<br>        }<br>        .wandb-col {<br>            display: flex;<br>            flex-direction: column;<br>            flex-basis: 100%;<br>            flex: 1;<br>            padding: 10px;<br>        }<br>    </style><br><div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>train/epoch</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▂▃▃▃▄▄▄▅▅▅▆▆▆▇▇▇███</td></tr><tr><td>train/grad_norm</td><td>                 █▁▁▁▁</td></tr><tr><td>train/learning_rate</td><td>▁▃▆██▇▇▇▆▆▅▅▅▄▄▄▃▃▂▂▂▁</td></tr><tr><td>train/loss</td><td>██████████████████▂▂▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>total_flos</td><td>1574542651646976.0</td></tr><tr><td>train/epoch</td><td>1</td></tr><tr><td>train/global_step</td><td>22</td></tr><tr><td>train/grad_norm</td><td>5354.84082</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.2996</td></tr><tr><td>train_loss</td><td>12.24251</td></tr><tr><td>train_runtime</td><td>395.3886</td></tr><tr><td>train_samples_per_second</td><td>0.445</td></tr><tr><td>train_steps_per_second</td><td>0.056</td></tr></table><br/></div></div>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":" View run <strong style=\"color:#cdcd00\">diagnostic-run-fp16-fix</strong> at: <a href='https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4/runs/hsyhgmxb' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4/runs/hsyhgmxb</a><br> View project at: <a href='https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4' target=\"_blank\">https://wandb.ai/jdmasciano2-university-of-lagos/e4b-correct-test-4</a><br>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find logs at: <code>./wandb/run-20250819_162027-hsyhgmxb/logs</code>"},"metadata":{}},{"name":"stdout","text":"✅ Memory cleared.\n","output_type":"stream"}],"execution_count":11},{"cell_type":"markdown","source":"# ==============================================================================\n# CELL 7: Create the Main Training Function for the W&B Agent (CORRECTED)\n# ==============================================================================\nfrom unsloth import FastVisionModel, FastModel\nfrom transformers import AutoProcessor\nfrom trl import SFTTrainer, SFTConfig\nfrom unsloth.trainer import UnslothVisionDataCollator\nimport torch\nimport gc # Import the garbage collector module\nimport asyncio \n\n\ndef train():\n    \"\"\"\n    This function is called by the W&B agent. It contains the entire\n    model setup, training, and saving logic.\n    A `try...finally` block has been added to ensure robust memory cleanup\n    after each run completes or fails.\n    \"\"\"\n    # Initialize variables to None to ensure they exist for the 'finally' block\n    model, processor, trainer, run = None, None, None, None\n    \n    try:\n        run = wandb.init()\n        \n        WANDB_CONFIG = wandb.config\n        lora_r_value = WANDB_CONFIG.lora_r\n        learning_rate_value = WANDB_CONFIG.learning_rate\n        epochs_value = WANDB_CONFIG.num_train_epochs\n        lora_alpha_value = lora_r_value * WANDB_CONFIG.lora_alpha_multiplier\n\n        MODEL_NAME = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\"\n\n        print(\"--- New W&B Run ---\")\n        print(f\"Parameters: LR={learning_rate_value}, Epochs={epochs_value}, LoRA r={lora_r_value}, LoRA alpha={lora_alpha_value}\")\n\n        model, tokenizer = FastVisionModel.from_pretrained(\n            model_name=MODEL_NAME,\n            max_seq_length=2048, # max_seq_length is correctly set here\n            dtype=None,\n            load_in_4bit=True,\n            device_map={\"\":\"cuda:0\"}\n            \n        )\n        processor = AutoProcessor.from_pretrained(MODEL_NAME)\n        print(\"✅ Base model, tokenizer, and processor loaded.\")\n\n        model = FastVisionModel.get_peft_model(\n            model,\n            r=lora_r_value,\n            lora_alpha=lora_alpha_value,\n            finetune_vision_layers=True,\n            finetune_language_layers=True,\n            target_modules=[\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\", \"gate_proj\", \"up_proj\", \"down_proj\"],\n        )\n        print(\"✅ PEFT adapters added.\")\n\n        FastModel.for_training(model)\n        trainer = SFTTrainer(\n            model=model,\n            train_dataset=final_dataset_list,\n            # The 'processing_class' argument is deprecated; data_collator handles this.\n            # Removed for clarity.\n            data_collator=UnslothVisionDataCollator(model, processor=processor),\n            args=SFTConfig(\n                output_dir=f\"./outputs_{run.name}\",\n                report_to=\"wandb\",\n                num_train_epochs=epochs_value,\n                learning_rate=learning_rate_value,\n                per_device_train_batch_size=2,\n                gradient_accumulation_steps=4,\n                gradient_checkpointing=False,\n                remove_unused_columns=False,\n                dataset_text_field=\"\",\n                dataset_kwargs={\"skip_prepare_dataset\": True},\n                # THIS LINE IS THE FIX: 'max_seq_length' has been removed.\n                warmup_ratio=0.1,\n                optim=\"adamw_torch_fused\",\n                save_strategy=\"no\",\n                seed=3407,\n            ),\n        )\n\n        print(f\"\\n🔥 Starting training run: {run.name}...\")\n        trainer.train()\n        print(\"✅ Training complete!\")\n\n        output_save_dir = f\"/kaggle/working/maize_expert_adapters_{run.name}\"\n        model.save_pretrained(output_save_dir)\n        tokenizer.save_pretrained(output_save_dir)\n        print(f\"✅ Model adapters saved to {output_save_dir}\")\n\n        artifact = wandb.Artifact(f'maize-adapters-{run.name}', type='model')\n        artifact.add_dir(output_save_dir)\n        run.log_artifact(artifact)\n        print(\"✅ Adapters logged as a W&B Artifact.\")\n        \n        # 2. THE FIX: Use asyncio.run() to execute the async function\n        # Call the isolated async evaluation function\n        results = asyncio.run(evaluate_and_log(model, processor, eval_dataset))\n        \n        # THE FINAL FIX: Explicitly log the accuracy and latency to W&B\n        try:\n            # This extracts the numerical value from the nested dictionary\n            mean_accuracy = results['calculate_accuracy']['accuracy']['mean']\n            model_latency = results['model_latency']['mean']\n            wandb.log({\n                \"eval_accuracy\": mean_accuracy,\n                \"eval_latency_ms\": model_latency\n            })\n            print(f\"✅ Logged evaluation metrics to W&B: Accuracy={mean_accuracy}\")\n        except (KeyError, TypeError) as e:\n            print(f\"⚠️ Could not log evaluation metrics to W&B: {e}\")\n            \n    finally:\n        print(\"\\n🧹 Starting cleanup for next run...\")\n        if run:\n            run.finish()\n\n        # THE FINAL FIX: Safely delete each variable only if it was created.\n        if 'eval_model' in locals() and eval_model is not None: del eval_model\n        if 'trainer' in locals() and trainer is not None: del trainer\n        if 'model' in locals() and model is not None: del model\n        if 'processor' in locals() and processor is not None: del processor\n        if 'tokenizer' in locals() and tokenizer is not None: del tokenizer        \n        \n        gc.collect()\n        torch.cuda.empty_cache()\n        print(\"✅ Memory cleared. Ready for the next agent run.\")\n\n\n\n# Execute the sweep agent\n# This remains the same\n#wandb.agent(sweep_id, function=train, count=5)","metadata":{}},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}