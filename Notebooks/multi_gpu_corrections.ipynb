{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12544913,"sourceType":"datasetVersion","datasetId":7920300},{"sourceId":12548828,"sourceType":"datasetVersion","datasetId":7923075},{"sourceId":12548854,"sourceType":"datasetVersion","datasetId":7923091}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip install accelerate==1.7.0\n!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3  trl triton cut_cross_entropy \n!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n!pip install -U peft\n!pip install --no-deps --upgrade timm # Only for Gemma 3N\n!pip install torch torchvision torchaudio --index-url https://download.pytorch.org/whl/cu121","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:16:49.716159Z","iopub.execute_input":"2025-08-20T17:16:49.716382Z","iopub.status.idle":"2025-08-20T17:18:39.003966Z","shell.execute_reply.started":"2025-08-20T17:16:49.716364Z","shell.execute_reply":"2025-08-20T17:18:39.003090Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install opensloth \n!pip install trl==0.19.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:18:39.005422Z","iopub.execute_input":"2025-08-20T17:18:39.005697Z","iopub.status.idle":"2025-08-20T17:18:55.831733Z","shell.execute_reply.started":"2025-08-20T17:18:39.005674Z","shell.execute_reply":"2025-08-20T17:18:55.830872Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"%%capture\n#!pip install --no-deps git+https://github.com/huggingface/transformers.git -qU\n#!pip install transformers -U\n!pip install transformers==4.51.0 #4.50.0","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:18:55.835550Z","iopub.execute_input":"2025-08-20T17:18:55.835805Z","iopub.status.idle":"2025-08-20T17:19:09.223984Z","shell.execute_reply.started":"2025-08-20T17:18:55.835780Z","shell.execute_reply":"2025-08-20T17:19:09.222939Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"%%capture\n#!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth-zoo.git\n#!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth.git\n!pip install unsloth==2025.7.10 unsloth-zoo==2025.7.11 --no-cache -q","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:19:09.225168Z","iopub.execute_input":"2025-08-20T17:19:09.225441Z","iopub.status.idle":"2025-08-20T17:19:22.728346Z","shell.execute_reply.started":"2025-08-20T17:19:09.225405Z","shell.execute_reply":"2025-08-20T17:19:22.727497Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nfrom datasets import Dataset, Image as HFImage\nfrom pathlib import Path\nimport os\n\n# --- NEW STEP: Copy data to the faster working directory ---\nsource_path = \"/kaggle/input/maize1-dataset/\"\nlocal_path = \"/kaggle/working/local_datasets/\"\n\nif not os.path.exists(local_path):\n    print(f\"Copying data from {source_path} to {local_path} for faster access...\")\n    !cp -r {source_path} {local_path}\n    print(\"✅ Data copy complete.\")\nelse:\n    print(f\"✅ Data already copied to {local_path}\")\n# ---------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:19:22.729557Z","iopub.execute_input":"2025-08-20T17:19:22.729891Z","iopub.status.idle":"2025-08-20T17:19:28.904950Z","shell.execute_reply.started":"2025-08-20T17:19:22.729857Z","shell.execute_reply":"2025-08-20T17:19:28.904032Z"}},"outputs":[{"name":"stdout","text":"Copying data from /kaggle/input/maize1-dataset/ to /kaggle/working/local_datasets/ for faster access...\n✅ Data copy complete.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile cache_vision_dataset.py\n# ==============================================================================\n# STEP 1: Cache Vision Dataset to Disk (FINAL - MANUAL COMPONENT VERSION)\n# ==============================================================================\n\n\"\"\"\nPre-processes and caches a vision dataset by calling the tokenizer and image\nprocessor components separately. This is the most robust method and bypasses\nthe error-prone main processor call.\n\"\"\"\n\nfrom datasets import Dataset\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import AutoProcessor\nimport os\n\n# --- Mappings and Functions ---\nCLASS_NAME_MAPPING = {\n    \"maize_healthy\": \"Healthy Maize Plant\",\n    \"phosphorus_deficiency\": \"Maize Phosphorus Deficiency\",\n}\n\ndef create_conversation_dict(image_path, class_name):\n    \"\"\"Creates the 'messages' dictionary structure for a single sample.\"\"\"\n    display_name = CLASS_NAME_MAPPING.get(class_name, \"Unknown Maize Condition\")\n    pil_image = Image.open(image_path).convert(\"RGB\")\n    return {\n        \"messages\": [\n            { \"role\": \"user\",\n              \"content\": [\n                {\"type\": \"text\", \"text\": \"What is the condition of this maize plant?\"},\n                {\"type\": \"image\", \"image\": pil_image}\n              ]\n            },\n            { \"role\": \"assistant\",\n              \"content\": [\n                {\"type\": \"text\", \"text\": f\"This is a {display_name}.\"}\n              ]\n            },\n        ]\n    }\n\ndef dump_vision_data():\n    MODEL_NAME = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n    print(f\"Loading processor for '{MODEL_NAME}'...\")\n    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n    \n    dataset_path = Path(\"/kaggle/working/local_datasets/\")\n    image_paths = list(dataset_path.glob(\"**/*.jpg\")) + list(dataset_path.glob(\"**/*.jpeg\"))\n    print(f\"Found {len(image_paths)} images.\")\n    \n    print(\"Creating dataset with 'messages' format...\")\n    raw_dataset_list = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        raw_dataset_list.append(create_conversation_dict(path, path.parent.name))\n        \n    # ** FINAL STRATEGY: A MANUAL BATCHING LOOP USING SEPARATE COMPONENTS **\n    print(\"Processing dataset manually using separate tokenizer and image processor...\")\n    \n    batch_size = 8\n    processed_list = []\n\n    for i in tqdm(range(0, len(raw_dataset_list), batch_size), desc=\"Processing batches\"):\n        batch = raw_dataset_list[i : i + batch_size]\n        \n        # 1. Prepare lists of texts and images for this batch\n        batch_texts = []\n        batch_images = []\n        for sample in batch:\n            text = processor.tokenizer.apply_chat_template(\n                sample[\"messages\"], tokenize=False, add_generation_prompt=False\n            )\n            image = sample[\"messages\"][0]['content'][1]['image']\n            batch_texts.append(text)\n            batch_images.append(image)\n\n        # 2. **THE FIX**: Call tokenizer and image_processor SEPARATELY\n        # Process the text part\n        text_inputs = processor.tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        # Process the image part\n        image_inputs = processor.image_processor(\n            images=batch_images,\n            return_tensors=\"pt\"\n        )\n        \n        # 3. Manually combine the results\n        # The text_inputs dictionary already has 'input_ids' and 'attention_mask'\n        # We just need to add the 'pixel_values' to it.\n        combined_inputs = {\n            \"input_ids\": text_inputs.input_ids,\n            \"attention_mask\": text_inputs.attention_mask,\n            \"pixel_values\": image_inputs.pixel_values,\n        }\n        \n        # 4. Unpack the processed batch back into individual samples for our list\n        for j in range(len(batch_texts)):\n            processed_list.append({\n                \"input_ids\": combined_inputs[\"input_ids\"][j],\n                \"attention_mask\": combined_inputs[\"attention_mask\"][j],\n                \"pixel_values\": combined_inputs[\"pixel_values\"][j],\n            })\n\n    # 5. Create the final dataset from the list of processed dictionaries\n    print(\"\\nCreating final dataset from processed list...\")\n    processed_dataset = Dataset.from_list(processed_list)\n\n    # 6. Save the final, processed dataset to disk\n    output_dir = \"data/cached_vision_dataset_hf\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(f\"Saving processed dataset to directory: {output_dir}\")\n    processed_dataset.save_to_disk(output_dir)\n    \n    print(f\"\\n✅ Dataset successfully processed and saved to '{output_dir}'.\")\n    print(f\"The dataset now contains the required columns: {processed_dataset.column_names}\")\n\nif __name__ == \"__main__\":\n    dump_vision_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:19:28.906946Z","iopub.execute_input":"2025-08-20T17:19:28.907544Z","iopub.status.idle":"2025-08-20T17:19:28.915099Z","shell.execute_reply.started":"2025-08-20T17:19:28.907521Z","shell.execute_reply":"2025-08-20T17:19:28.914484Z"}},"outputs":[{"name":"stdout","text":"Writing cache_vision_dataset.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Run the caching script\n!python cache_vision_dataset.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:19:28.915783Z","iopub.execute_input":"2025-08-20T17:19:28.915972Z","iopub.status.idle":"2025-08-20T17:20:58.831742Z","shell.execute_reply.started":"2025-08-20T17:19:28.915957Z","shell.execute_reply":"2025-08-20T17:20:58.830973Z"}},"outputs":[{"name":"stdout","text":"2025-08-20 17:19:42.999517: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755710383.386151     177 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755710383.496770     177 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nLoading processor for 'unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit'...\nprocessor_config.json: 100%|██████████████████| 98.0/98.0 [00:00<00:00, 841kB/s]\nchat_template.jinja: 1.63kB [00:00, 9.74MB/s]\npreprocessor_config.json: 1.09kB [00:00, 7.05MB/s]\nconfig.json: 5.21kB [00:00, 23.8MB/s]\ntokenizer_config.json: 1.20MB [00:00, 179MB/s]\ntokenizer.model: 100%|█████████████████████| 4.70M/4.70M [00:00<00:00, 5.36MB/s]\ntokenizer.json: 100%|██████████████████████| 33.4M/33.4M [00:00<00:00, 53.3MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 777/777 [00:00<00:00, 7.02MB/s]\nFound 176 images.\nCreating dataset with 'messages' format...\nProcessing images: 100%|██████████████████████| 176/176 [00:09<00:00, 18.34it/s]\nProcessing dataset manually using separate tokenizer and image processor...\nProcessing batches: 100%|███████████████████████| 22/22 [00:18<00:00,  1.19it/s]\n\nCreating final dataset from processed list...\nSaving processed dataset to directory: data/cached_vision_dataset_hf\nSaving the dataset (3/3 shards): 100%|█| 176/176 [00:00<00:00, 191.69 examples/s\n\n✅ Dataset successfully processed and saved to 'data/cached_vision_dataset_hf'.\nThe dataset now contains the required columns: ['input_ids', 'attention_mask', 'pixel_values']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: Login to Weights & Biases\n# ==============================================================================\nimport wandb\nfrom kaggle_secrets import UserSecretsClient\n\n# --- PRE-REQUISITE ---\n# 1. Add your W&B API key as a secret in Kaggle with the label \"wandb_api_key\".\n# 2. This keeps your key secure and private.\n# ---------------------\n\ntry:\n    user_secrets = UserSecretsClient()\n    wandb_api_key = user_secrets.get_secret(\"wandb_api_key\")\n    wandb.login(key=wandb_api_key)\n    print(\"✅ Successfully logged into Weights & Biases.\")\nexcept Exception as e:\n    print(\"Could not log into W&B. Please ensure the 'wandb_api_key' secret is set in your Kaggle notebook.\")\n    print(f\"Error: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:20:58.833374Z","iopub.execute_input":"2025-08-20T17:20:58.833607Z","iopub.status.idle":"2025-08-20T17:21:06.395392Z","shell.execute_reply.started":"2025-08-20T17:20:58.833584Z","shell.execute_reply":"2025-08-20T17:21:06.394815Z"}},"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjdmasciano2\u001b[0m (\u001b[33mjdmasciano2-university-of-lagos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"},{"name":"stdout","text":"✅ Successfully logged into Weights & Biases.\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile train_vision_multiGPU.py\n# ==============================================================================\n# STEP 2: Multi-GPU Training Script\n# ==============================================================================\n\n\"\"\"\nMulti-GPU Vision Model Training with OpenSloth\n\"\"\"\n\nimport os\nimport pickle\nfrom opensloth.opensloth_config import (\n    FastModelArgs,\n    LoraArgs,\n    OpenSlothConfig,\n    TrainingArguments,\n)\nfrom opensloth.scripts.opensloth_sft_trainer import run_mp_training, setup_envs\n\n\n# Set PyTorch memory allocation configuration\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Multi-GPU Configuration\nGLOBAL_BZ = 16\nDEVICES = [0, 1]\nBZ = 1\n\n# OpenSloth Configuration for Vision Models\nopensloth_config = OpenSlothConfig(\n    data_cache_path=\"data/cached_vision_dataset_hf\",\n    devices=DEVICES,\n    fast_model_args=FastModelArgs(\n        model_name=\"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\",\n        max_seq_length=2048,\n        load_in_4bit=True,\n        dtype=None,\n        use_gradient_checkpointing=\"unsloth\",  # Use Unsloth's optimized gradient checkpointing\n    ),\n    lora_args=LoraArgs(\n        r=16, # Consider reducing to 8 if memory is still an issue\n        lora_alpha=16,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"gate_proj\",\n            \"up_proj\",\n            \"down_proj\",\n        ],\n        lora_dropout=0,\n        bias=\"none\",\n        use_rslora=False,\n        finetune_vision_layers=True,\n        finetune_language_layers=True,\n    ),\n    sequence_packing=False,\n)\n\ntraining_config = TrainingArguments(\n    output_dir=\"outputs/vision_multiGPU_experiment\",\n    per_device_train_batch_size=BZ,\n    gradient_accumulation_steps=GLOBAL_BZ // (len(DEVICES) * BZ),\n    learning_rate=2e-4,\n    logging_steps=10,\n    num_train_epochs=1,\n    lr_scheduler_type=\"linear\",\n    warmup_ratio=0.1,\n    save_total_limit=2,\n    save_steps=100,\n    weight_decay=0.01,\n    optim=\"adamw_torch_fused\",\n    seed=3407,\n    remove_unused_columns=False,\n    dataset_text_field=\"\",\n    max_seq_length=1024,\n    dataloader_pin_memory=True,\n    fp16=True,  # Enable mixed-precision training\n    report_to=\"wandb\",\n    resume_from_checkpoint=\"\",\n    torch_compile=False,\n\n\n)\n\nif __name__ == \"__main__\":\n    # Setup environment variables for logging\n    os.environ[\"WANDB_PROJECT\"] = \"open-maize-vision2\"\n    os.environ[\"WANDB_NAME\"] = f\"vision_multiGPU_globalbz{GLOBAL_BZ}_epochs{training_config.num_train_epochs}\"\n\n    print(f\"Global batch size: {len(DEVICES) * BZ * training_config.gradient_accumulation_steps}\")\n    print(f\"Gradient accumulation steps: {training_config.gradient_accumulation_steps}\")\n\n    setup_envs(opensloth_config, training_config)\n    run_mp_training(opensloth_config.devices, opensloth_config, training_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:24:10.079848Z","iopub.execute_input":"2025-08-20T17:24:10.080120Z","iopub.status.idle":"2025-08-20T17:24:10.086315Z","shell.execute_reply.started":"2025-08-20T17:24:10.080094Z","shell.execute_reply":"2025-08-20T17:24:10.085672Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_vision_multiGPU.py\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# Run the training script\n!python train_vision_multiGPU.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-20T17:24:15.072758Z","iopub.execute_input":"2025-08-20T17:24:15.073439Z","iopub.status.idle":"2025-08-20T17:32:39.428720Z","shell.execute_reply.started":"2025-08-20T17:24:15.073417Z","shell.execute_reply":"2025-08-20T17:32:39.428027Z"}},"outputs":[{"name":"stdout","text":"Global batch size: 16\nGradient accumulation steps: 8\nGlobal batch size: 16\n[MP] Running on 2 GPUs\n2025-08-20 17:24:22.397988: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-08-20 17:24:22.398014: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1755710662.419850     231 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755710662.419847     230 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1755710662.426601     230 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1755710662.426603     231 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[32m17:24:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:41\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/vision_multiGPU_experiment\u001b[0m\n\u001b[32m17:24:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:44\u001b[0m | \u001b[1m🚀 Starting total training timer\u001b[0m\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n🦥 Unsloth Zoo will now patch everything to make training faster!\nUsing compiler location: .cache/unsloth_compiled_cache_0\nUsing compiler location: .cache/unsloth_compiled_cache_1\n==((====))==  Unsloth 2025.7.10: Fast Gemma3N patching. Transformers: 4.55.2.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n==((====))==  Unsloth 2025.7.10: Fast Gemma3N patching. Transformers: 4.55.2.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\nmodel.safetensors.index.json: 370kB [00:00, 303MB/s]\nFetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\nmodel-00002-of-00003.safetensors:   0%|             | 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|             | 0.00/1.15G [00:00<?, ?B/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   0%|             | 0.00/3.72G [00:00<?, ?B/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|      | 268k/1.15G [00:00<59:19, 323kB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   1%|    | 8.90M/1.15G [00:01<01:46, 10.7MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   0%|    | 3.13M/4.99G [00:01<30:41, 2.71MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:   2%|    | 27.3M/1.15G [00:01<00:33, 33.1MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   3%|    | 35.3M/1.15G [00:01<00:31, 35.7MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   9%|▍    | 102M/1.15G [00:01<00:11, 91.8MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   0%|    | 6.33M/4.99G [00:01<23:24, 3.55MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   0%|    | 12.0M/4.99G [00:02<12:14, 6.77MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   1%|    | 30.7M/4.99G [00:03<06:44, 12.2MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  11%|▌    | 124M/1.15G [00:03<00:29, 34.2MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   0%|    | 4.31M/3.72G [00:03<51:25, 1.21MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   2%|    | 75.3M/4.99G [00:04<03:22, 24.2MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  14%|▋    | 159M/1.15G [00:05<00:38, 26.0MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   2%|     | 101M/4.99G [00:05<03:19, 24.5MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  20%|▉    | 226M/1.15G [00:05<00:20, 45.0MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  25%|█▎   | 293M/1.15G [00:06<00:12, 66.6MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   0%|   | 5.28M/3.72G [00:06<1:19:53, 776kB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  31%|█▌   | 360M/1.15G [00:06<00:09, 87.5MB/s]\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   1%|    | 34.1M/3.72G [00:06<08:10, 7.53MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   3%|▏    | 168M/4.99G [00:06<02:25, 33.1MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   5%|▏    | 235M/4.99G [00:07<01:33, 50.7MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   1%|    | 37.1M/3.72G [00:07<08:56, 6.87MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   6%|▎    | 302M/4.99G [00:07<01:08, 68.9MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   1%|    | 38.9M/3.72G [00:07<09:13, 6.66MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:   7%|▎    | 369M/4.99G [00:08<00:53, 85.8MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  36%|█▊   | 411M/1.15G [00:08<00:14, 52.5MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  10%|▌     | 487M/4.99G [00:08<00:32, 139MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  11%|▋     | 539M/4.99G [00:08<00:30, 145MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   2%|    | 80.9M/3.72G [00:08<03:25, 17.7MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   3%|▏    | 127M/3.72G [00:09<01:56, 30.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  12%|▋     | 623M/4.99G [00:09<00:38, 113MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  42%|██   | 478M/1.15G [00:10<00:15, 42.3MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  14%|▊     | 690M/4.99G [00:10<00:41, 105MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  53%|██▋  | 612M/1.15G [00:11<00:07, 75.1MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  65%|███▉  | 746M/1.15G [00:11<00:03, 123MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  17%|▉     | 824M/4.99G [00:11<00:31, 131MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  18%|█     | 891M/4.99G [00:11<00:27, 148MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  71%|████▏ | 813M/1.15G [00:11<00:02, 127MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  21%|█    | 1.02G/4.99G [00:11<00:17, 225MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  22%|█    | 1.09G/4.99G [00:11<00:15, 255MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  25%|█▏   | 1.23G/4.99G [00:12<00:10, 350MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   4%|▏    | 139M/3.72G [00:12<03:44, 16.0MB/s]\u001b[A\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  77%|████▌ | 880M/1.15G [00:12<00:02, 125MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  29%|█▍   | 1.43G/4.99G [00:12<00:07, 498MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  31%|█▌   | 1.56G/4.99G [00:12<00:06, 511MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  82%|████▉ | 947M/1.15G [00:12<00:01, 136MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  33%|█▋   | 1.63G/4.99G [00:12<00:08, 382MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  88%|████▍| 1.01G/1.15G [00:12<00:00, 146MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  35%|█▊   | 1.76G/4.99G [00:13<00:06, 472MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  37%|█▊   | 1.83G/4.99G [00:13<00:08, 372MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  94%|████▋| 1.08G/1.15G [00:13<00:00, 146MB/s]\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  41%|██   | 2.03G/4.99G [00:13<00:06, 453MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   4%|▏    | 143M/3.72G [00:13<05:14, 11.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   5%|▏    | 168M/3.72G [00:13<03:18, 17.9MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  43%|██▏  | 2.16G/4.99G [00:14<00:06, 412MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  46%|██▎  | 2.30G/4.99G [00:14<00:05, 476MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors: 100%|████| 1.15G/1.15G [00:14<00:00, 79.6MB/s]\u001b[A\u001b[A\n\n\n\nmodel-00001-of-00003.safetensors:   5%|▏    | 179M/3.72G [00:14<03:19, 17.8MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  47%|██▎  | 2.37G/4.99G [00:14<00:06, 384MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  49%|██▍  | 2.43G/4.99G [00:14<00:06, 417MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  50%|██▌  | 2.50G/4.99G [00:14<00:06, 394MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   5%|▎    | 190M/3.72G [00:14<03:03, 19.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   6%|▎    | 222M/3.72G [00:15<01:42, 34.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  51%|██▌  | 2.57G/4.99G [00:15<00:07, 332MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   7%|▎    | 253M/3.72G [00:15<01:15, 45.8MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   8%|▍    | 306M/3.72G [00:15<00:44, 76.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  53%|██▋  | 2.64G/4.99G [00:15<00:11, 209MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  54%|██▋  | 2.71G/4.99G [00:16<00:09, 237MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  56%|██▊  | 2.77G/4.99G [00:16<00:10, 218MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:   9%|▍    | 321M/3.72G [00:16<01:10, 48.4MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  11%|▌    | 397M/3.72G [00:16<00:41, 80.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  57%|██▊  | 2.84G/4.99G [00:17<00:11, 184MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  58%|██▉  | 2.91G/4.99G [00:17<00:09, 230MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  11%|▌    | 415M/3.72G [00:17<00:39, 83.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  12%|▌    | 433M/3.72G [00:17<00:40, 81.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  60%|██▉  | 2.98G/4.99G [00:17<00:09, 205MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  62%|███  | 3.11G/4.99G [00:18<00:08, 234MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  13%|▋    | 477M/3.72G [00:18<00:45, 71.2MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  64%|███▏ | 3.18G/4.99G [00:18<00:06, 271MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  13%|▋    | 490M/3.72G [00:18<00:45, 70.6MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  65%|███▎ | 3.24G/4.99G [00:18<00:07, 238MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  14%|▋    | 506M/3.72G [00:18<00:48, 66.3MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  14%|▋    | 524M/3.72G [00:18<00:43, 73.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  14%|▋    | 537M/3.72G [00:18<00:41, 76.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  66%|███▎ | 3.31G/4.99G [00:18<00:08, 204MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  15%|▋    | 556M/3.72G [00:19<00:38, 82.6MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  15%|▊    | 569M/3.72G [00:19<00:35, 87.9MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  16%|▊    | 586M/3.72G [00:19<00:38, 81.7MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  17%|█     | 649M/3.72G [00:19<00:18, 162MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  19%|█▏    | 718M/3.72G [00:19<00:14, 206MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  68%|██▋ | 3.38G/4.99G [00:20<00:16, 99.2MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  21%|█▎    | 782M/3.72G [00:20<00:23, 125MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  22%|█▎    | 801M/3.72G [00:20<00:22, 128MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  69%|███▍ | 3.45G/4.99G [00:21<00:14, 109MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  22%|█▎    | 834M/3.72G [00:21<00:21, 131MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  70%|███▌ | 3.51G/4.99G [00:21<00:12, 117MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  23%|█▏   | 854M/3.72G [00:21<00:34, 82.1MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  24%|█▏   | 877M/3.72G [00:21<00:31, 90.7MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  73%|███▋ | 3.65G/4.99G [00:22<00:08, 156MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  24%|█▏   | 892M/3.72G [00:22<00:34, 82.0MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  25%|█▌    | 948M/3.72G [00:22<00:19, 139MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  27%|█▌    | 991M/3.72G [00:22<00:17, 158MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  27%|█▎   | 1.02G/3.72G [00:22<00:15, 171MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  28%|█▍   | 1.04G/3.72G [00:22<00:15, 169MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  29%|█▍   | 1.07G/3.72G [00:22<00:16, 161MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  31%|█▌   | 1.14G/3.72G [00:23<00:15, 165MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  33%|█▋   | 1.22G/3.72G [00:23<00:09, 253MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  34%|█▋   | 1.26G/3.72G [00:23<00:10, 233MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  35%|█▋   | 1.30G/3.72G [00:23<00:09, 247MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  36%|█▊   | 1.34G/3.72G [00:23<00:08, 267MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  74%|██▉ | 3.71G/4.99G [00:23<00:15, 83.5MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  37%|█▊   | 1.38G/3.72G [00:24<00:10, 230MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  76%|███▊ | 3.78G/4.99G [00:24<00:11, 103MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  77%|███▊ | 3.85G/4.99G [00:24<00:08, 131MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  78%|███▉ | 3.91G/4.99G [00:24<00:06, 161MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  39%|█▉   | 1.45G/3.72G [00:24<00:13, 171MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  80%|███▉ | 3.98G/4.99G [00:24<00:05, 190MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  81%|████ | 4.05G/4.99G [00:24<00:04, 219MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  41%|██   | 1.54G/3.72G [00:25<00:12, 179MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  83%|████▏| 4.12G/4.99G [00:25<00:03, 223MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  43%|██▏  | 1.60G/3.72G [00:25<00:10, 210MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  45%|██▎  | 1.68G/3.72G [00:25<00:08, 246MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  46%|██▎  | 1.71G/3.72G [00:25<00:08, 232MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  47%|██▎  | 1.75G/3.72G [00:25<00:07, 248MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  84%|████▏| 4.18G/4.99G [00:26<00:05, 143MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  85%|████▎| 4.25G/4.99G [00:26<00:04, 177MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  49%|██▍  | 1.83G/3.72G [00:26<00:08, 221MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  51%|██▌  | 1.90G/3.72G [00:26<00:11, 154MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  52%|██▌  | 1.92G/3.72G [00:27<00:16, 110MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  87%|███▍| 4.32G/4.99G [00:28<00:09, 67.4MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  89%|███▌| 4.45G/4.99G [00:29<00:06, 88.9MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  92%|████▌| 4.58G/4.99G [00:29<00:02, 141MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  53%|██  | 1.96G/3.72G [00:29<00:36, 48.5MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors:  93%|████▋| 4.65G/4.99G [00:29<00:02, 157MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  95%|████▋| 4.72G/4.99G [00:30<00:01, 150MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  96%|████▊| 4.79G/4.99G [00:30<00:01, 144MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  97%|████▊| 4.85G/4.99G [00:31<00:00, 180MB/s]\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  55%|██▏ | 2.03G/3.72G [00:31<00:34, 49.5MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  57%|██▎ | 2.11G/3.72G [00:31<00:21, 76.0MB/s]\u001b[A\u001b[A\u001b[A\nmodel-00002-of-00003.safetensors: 100%|█████| 4.99G/4.99G [00:31<00:00, 220MB/s]\u001b[A\n\n\nmodel-00002-of-00003.safetensors: 100%|█████| 4.99G/4.99G [00:31<00:00, 158MB/s]\u001b[A\u001b[A\u001b[A\n\n\n\nmodel-00001-of-00003.safetensors:  62%|███  | 2.30G/3.72G [00:31<00:10, 139MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  63%|███▏ | 2.35G/3.72G [00:32<00:08, 161MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  66%|███▎ | 2.45G/3.72G [00:32<00:05, 233MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  68%|███▍ | 2.52G/3.72G [00:32<00:04, 257MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  70%|███▌ | 2.62G/3.72G [00:32<00:03, 346MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  73%|███▋ | 2.73G/3.72G [00:32<00:02, 385MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  75%|███▋ | 2.79G/3.72G [00:32<00:02, 323MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  78%|███▉ | 2.90G/3.72G [00:33<00:02, 410MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  81%|████ | 3.01G/3.72G [00:33<00:01, 488MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  84%|████▏| 3.12G/3.72G [00:33<00:01, 522MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  86%|████▎| 3.22G/3.72G [00:33<00:01, 492MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  88%|████▍| 3.28G/3.72G [00:33<00:00, 439MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  90%|████▌| 3.35G/3.72G [00:34<00:00, 405MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  91%|████▌| 3.40G/3.72G [00:34<00:00, 390MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  93%|████▋| 3.46G/3.72G [00:35<00:01, 146MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors:  96%|████▊| 3.59G/3.72G [00:35<00:00, 240MB/s]\u001b[A\u001b[A\u001b[A\n\n\nmodel-00001-of-00003.safetensors: 100%|████| 3.72G/3.72G [00:37<00:00, 99.1MB/s]\u001b[A\u001b[A\u001b[A\nFetching 3 files: 100%|███████████████████████████| 3/3 [00:37<00:00, 12.61s/it]\nFetching 3 files: 100%|███████████████████████████| 3/3 [00:37<00:00, 12.60s/it]\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:10<00:00,  3.65s/it]\ngeneration_config.json: 100%|██████████████████| 210/210 [00:00<00:00, 1.76MB/s]\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:11<00:00,  3.83s/it]\nprocessor_config.json: 100%|██████████████████| 98.0/98.0 [00:00<00:00, 896kB/s]\nchat_template.jinja: 1.63kB [00:00, 9.06MB/s]\npreprocessor_config.json: 1.09kB [00:00, 6.88MB/s]\ntokenizer_config.json: 1.20MB [00:00, 166MB/s]\ntokenizer.model: 100%|█████████████████████| 4.70M/4.70M [00:00<00:00, 8.37MB/s]\ntokenizer.json: 100%|██████████████████████| 33.4M/33.4M [00:00<00:00, 57.5MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 777/777 [00:00<00:00, 8.67MB/s]\n[RANK=1] environ({'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.5.3.2-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NV_NVML_DEV_VERSION': '12.5.82-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn9-cuda-12', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.22.3-1+cuda12.5', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.22.3-1', 'VM_GCE_METADATA_HOST': '169.254.169.254', 'HOSTNAME': '6625b279dd49', 'LANGUAGE': 'en_US', 'KAGGLE_DATA_PROXY_TOKEN': 'eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.Z9cBWq15QHy0FTjgDXstx8jupWaddVlPZzgy8R7tG64FYA2Or862_Q.zpscssotA_oPUYgv9LIXgA.MELI6e-WcrtCOfbSKpSyQgJ6p09QeCD0JQYeuZKplboDq_iCup-uSVwkD62__i-nsJJEHx54hDeeDABu0c-z6J9R9J_VKkZDMCEGPzZbo7H9yJumKH1VwBJKIaHKgpdvOVSQlhzEVu_XE_2zPBYt8RgMu6kFTPzQVfb61crmC_iaiKkQnzpnQHsfLCqg-haOFSArS-DGetNjIWurM6dr4Oxi7b5aXMnhIg8Ku5q3_VZqefAX1lleeVzYO0khalUIbfTYTZqQbjg5oQ6LD9eL1cUpCIEHGd77lBxmhe6lgqOODGipMZ_rPUa0Xnogvchr.Rsah7sFDHv5BZ1iUScrbkg', 'COLAB_TPU_1VM': '', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-12-5=12.5.3.2-1', 'NV_NVTX_VERSION': '12.5.82-1', 'TF_CPP_MIN_LOG_LEVEL': '2', 'COLAB_JUPYTER_IP': '127.0.0.1', 'NV_CUDA_CUDART_DEV_VERSION': '12.5.82-1', 'NV_LIBCUSPARSE_VERSION': '12.5.1.3-1', 'KAGGLE_URL_BASE': 'https://www.kaggle.com', 'NV_LIBNPP_VERSION': '12.3.0.159-1', 'NCCL_VERSION': '2.22.3-1', 'KAGGLE_DOCKER_IMAGE': 'gcr.io/kaggle-gpu-images/python@sha256:320043e14c68293f1c946585b9257123385205a58af4b94b17d31868cae4e868', 'KAGGLE_KERNEL_INTEGRATIONS': '', 'KMP_LISTEN_PORT': '6000', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'COLAB_HUMAN_READABLE_NODE_LOGS': '1', 'ENV': '/root/.bashrc', 'PWD': '/kaggle/working', 'TESSERACT_PATH': '/usr/bin/tesseract', 'NV_CUDNN_PACKAGE': 'libcudnn9-cuda-12=9.2.1.18-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'LAST_FORCED_REBUILD': '20250623', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-12-5=12.5.82-1', 'NV_LIBNPP_PACKAGE': 'libnpp-12-5=12.3.0.159-1', 'BUILD_DATE': '20250701-214057', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20', '_': '/usr/local/bin/python', 'NV_LIBCUBLAS_DEV_VERSION': '12.5.3.2-1', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'UV_BUILD_CONSTRAINT': '', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-12-5', 'NV_CUDA_CUDART_VERSION': '12.5.82-1', 'COLAB_JUPYTER_ALLOW_ORIGIN_PAT': 'https://colab\\\\.(sandbox|research)\\\\.google\\\\.com', 'COLAB_WARMUP_DEFAULTS': '1', 'HOME': '/root', 'LANG': 'en_US.UTF-8', 'CUDA_VERSION': '12.5.1', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-5=12.5.3.2-1', 'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-12-5=12.5.1-1', 'UV_SYSTEM_PYTHON': 'true', 'COLAB_RELEASE_TAG': 'release-colab_20250626-060053_RC00', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'KMP_TARGET_PORT': '9000', 'CLICOLOR': '1', 'UV_INSTALL_DIR': '/usr/local/bin', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-12-5=12.3.0.159-1', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-5', 'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000', 'CLOUDSDK_PYTHON': 'python3', 'NV_LIBNPP_DEV_VERSION': '12.3.0.159-1', 'JPY_PARENT_PID': '1', 'PYTHONPATH': '/kaggle/lib/kagglegym:/kaggle/lib', 'TERM': 'xterm-color', 'NV_LIBCUSPARSE_DEV_VERSION': '12.5.1.3-1', 'KAGGLE_DATA_PROXY_PROJECT': 'kaggle-161607', 'GIT_PAGER': 'cat', 'KAGGLE_USER_SECRETS_TOKEN': 'eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..y_WuAgHdfwYJf1u0yEU4jA.-5QAIF5MNSJMxHQL-r2hjZSy6rinYQY9rgXD9P1pIthqtFsTdjbVkrrhIZAfZE8kOMNAHmkPT_P4S99-5o16sO4FYgDX7fk_LIqdbePwr1Gh6wvyjx8GJO4qqtO3670XbmAoxtnPUpLW5fbDgxvz0w.6L4tiBD8Jj2rE_83dLQj5g', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_CUDNN_VERSION': '9.2.1.18-1', 'SHLVL': '0', 'PAGER': 'cat', 'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service', 'NV_CUDA_LIB_VERSION': '12.5.1-1', 'NVARCH': 'x86_64', 'KAGGLE_KERNEL_RUN_TYPE': 'Interactive', 'UV_CONSTRAINT': '', 'PYTHONUTF8': '1', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn9-dev-cuda-12=9.2.1.18-1', 'KAGGLE_DISABLE_GOOGLE_GENERATIVE_AI_INTEGRATION': 'True', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.22.3-1+cuda12.5', 'LD_LIBRARY_PATH': '/usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'KAGGLE_GCP_ZONE': 'us-central1-a', 'MKL_THREADING_LAYER': 'GNU', 'NV_CUDA_NSIGHT_COMPUTE_VERSION': '12.5.1-1', 'GIT_COMMIT': '0eb38ee5b80e4003101469c19d6da1f4353d5960', 'NV_NVPROF_VERSION': '12.5.82-1', 'LC_ALL': 'en_US.UTF-8', '_PYVIZ_COMMS_INSTALLED': '1', 'CUDA_HOME': '/usr/local/cuda', 'COLAB_FILE_HANDLER_ADDR': 'localhost:3453', 'KAGGLE_CONTAINER_NAME': 'kaggle_UX0lGClkIGGqkM4s56Ax0QWrtOr7jERQ9HqqHg4CgI-257140063-webtier', 'PATH': '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'PYTHONUSERBASE': '/root/.local', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer', 'NV_LIBNCCL_PACKAGE_VERSION': '2.22.3-1', 'KAGGLE_API_V1_TOKEN': '/etc/secrets/kaggle/api-v1-token', 'DEBIAN_FRONTEND': 'noninteractive', 'KAGGLE_GRPC_DATA_PROXY_URL': 'dp.kaggle.net:443', 'KAGGLE_DATA_PROXY_URL': 'https://dp.kaggle.net', 'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True,roundup_power2_divisions:[32:256,64:128,256:64,>:32]', 'WANDB_PROJECT': 'open-maize-vision2', 'WANDB_NAME': 'vision_multiGPU_globalbz16_epochs1', 'OPENSLOTH_WORLD_SIZE': '2', 'OPENSLOTH_FORWARD_BZ': '2', 'OPENSLOTH_GLOBAL_BZ': '16', 'OPENSLOTH_ACCUMULATION_STEPS': '8', 'OPENSLOTH_PER_DEVICE_TRAIN_BZ': '1', 'OPENSLOTH_OUTPUT_DIR': 'outputs/vision_multiGPU_experiment', 'OPENSLOTH_LOG_LEVEL': 'info', 'CUDA_VISIBLE_DEVICES': '1', 'CUDA_MODULE_LOADING': 'LAZY', 'ENABLE_RUNTIME_UPTIME_TELEMETRY': '1', 'TF2_BEHAVIOR': '1', 'TPU_ML_PLATFORM': 'Tensorflow', 'TPU_ML_PLATFORM_VERSION': '2.18.0', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'TF_USE_LEGACY_KERAS': '1', 'OPENSLOTH_LOCAL_RANK': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'python', 'UNSLOTH_IS_PRESENT': '1', 'HF_HUB_ENABLE_HF_TRANSFER': '1', 'HF_XET_HIGH_PERFORMANCE': '1', 'HF_XET_CHUNK_CACHE_SIZE_BYTES': '0', 'HF_XET_RECONSTRUCT_WRITE_SEQUENTIALLY': '0', 'HF_XET_NUM_CONCURRENT_RANGE_GETS': '64', 'TRITON_DISABLE_LINE_INFO': '1', 'TRITON_FRONT_END_DEBUGGING': '0', 'UNSLOTH_ZOO_IS_PRESENT': '1', 'UNSLOTH_PATCHED': '1', 'TORCHINDUCTOR_FX_GRAPH_CACHE': '1', 'TORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE': '1', 'ENABLE_AOT_AUTOGRAD_CACHE': '1', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'UNSLOTH_IGNORED_TOKENIZER_NAMES': 'unsloth/qwen2.5-coder-7b-instruct-bnb-4bit\\nunsloth/qwen2.5-coder-7b-instruct\\nunsloth/qwen2.5-coder-1.5b-instruct\\nunsloth/qwen2.5-coder-1.5b-instruct-bnb-4bit', 'UNSLOTH_DISABLE_STATIC_GENERATION': '1', 'UNSLOTH_FORCE_CUSTOM_DTYPE': \"float16;torch.float16;torch.float16;if name.endswith(('.conv')): module;from unsloth_zoo.temporary_patches.gemma3n import patch_Gemma3nConvNormAct_forward; patch_Gemma3nConvNormAct_forward()\", 'UNSLOTH_FORCE_FLOAT32': '0', 'UNSLOTH_RETURN_LOGITS': '0', 'UNSLOTH_FULLGRAPH': '1', 'UNSLOTH_USE_NEW_MODEL': '1', 'UNSLOTH_ENABLE_FULL_FINETUNING': '0', 'UNSLOTH_MIXED_PRECISION': 'float32', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29501'})\n\u001b[32m17:25:51\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_loading: 1.1m\u001b[0m\n[RANK=0] environ({'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.5.3.2-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NV_NVML_DEV_VERSION': '12.5.82-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn9-cuda-12', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.22.3-1+cuda12.5', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.22.3-1', 'VM_GCE_METADATA_HOST': '169.254.169.254', 'HOSTNAME': '6625b279dd49', 'LANGUAGE': 'en_US', 'KAGGLE_DATA_PROXY_TOKEN': 'eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.Z9cBWq15QHy0FTjgDXstx8jupWaddVlPZzgy8R7tG64FYA2Or862_Q.zpscssotA_oPUYgv9LIXgA.MELI6e-WcrtCOfbSKpSyQgJ6p09QeCD0JQYeuZKplboDq_iCup-uSVwkD62__i-nsJJEHx54hDeeDABu0c-z6J9R9J_VKkZDMCEGPzZbo7H9yJumKH1VwBJKIaHKgpdvOVSQlhzEVu_XE_2zPBYt8RgMu6kFTPzQVfb61crmC_iaiKkQnzpnQHsfLCqg-haOFSArS-DGetNjIWurM6dr4Oxi7b5aXMnhIg8Ku5q3_VZqefAX1lleeVzYO0khalUIbfTYTZqQbjg5oQ6LD9eL1cUpCIEHGd77lBxmhe6lgqOODGipMZ_rPUa0Xnogvchr.Rsah7sFDHv5BZ1iUScrbkg', 'COLAB_TPU_1VM': '', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-12-5=12.5.3.2-1', 'NV_NVTX_VERSION': '12.5.82-1', 'TF_CPP_MIN_LOG_LEVEL': '2', 'COLAB_JUPYTER_IP': '127.0.0.1', 'NV_CUDA_CUDART_DEV_VERSION': '12.5.82-1', 'NV_LIBCUSPARSE_VERSION': '12.5.1.3-1', 'KAGGLE_URL_BASE': 'https://www.kaggle.com', 'NV_LIBNPP_VERSION': '12.3.0.159-1', 'NCCL_VERSION': '2.22.3-1', 'KAGGLE_DOCKER_IMAGE': 'gcr.io/kaggle-gpu-images/python@sha256:320043e14c68293f1c946585b9257123385205a58af4b94b17d31868cae4e868', 'KAGGLE_KERNEL_INTEGRATIONS': '', 'KMP_LISTEN_PORT': '6000', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'COLAB_HUMAN_READABLE_NODE_LOGS': '1', 'ENV': '/root/.bashrc', 'PWD': '/kaggle/working', 'TESSERACT_PATH': '/usr/bin/tesseract', 'NV_CUDNN_PACKAGE': 'libcudnn9-cuda-12=9.2.1.18-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'LAST_FORCED_REBUILD': '20250623', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-12-5=12.5.82-1', 'NV_LIBNPP_PACKAGE': 'libnpp-12-5=12.3.0.159-1', 'BUILD_DATE': '20250701-214057', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20', '_': '/usr/local/bin/python', 'NV_LIBCUBLAS_DEV_VERSION': '12.5.3.2-1', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'UV_BUILD_CONSTRAINT': '', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-12-5', 'NV_CUDA_CUDART_VERSION': '12.5.82-1', 'COLAB_JUPYTER_ALLOW_ORIGIN_PAT': 'https://colab\\\\.(sandbox|research)\\\\.google\\\\.com', 'COLAB_WARMUP_DEFAULTS': '1', 'HOME': '/root', 'LANG': 'en_US.UTF-8', 'CUDA_VERSION': '12.5.1', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-5=12.5.3.2-1', 'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-12-5=12.5.1-1', 'UV_SYSTEM_PYTHON': 'true', 'COLAB_RELEASE_TAG': 'release-colab_20250626-060053_RC00', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'KMP_TARGET_PORT': '9000', 'CLICOLOR': '1', 'UV_INSTALL_DIR': '/usr/local/bin', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-12-5=12.3.0.159-1', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-5', 'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000', 'CLOUDSDK_PYTHON': 'python3', 'NV_LIBNPP_DEV_VERSION': '12.3.0.159-1', 'JPY_PARENT_PID': '1', 'PYTHONPATH': '/kaggle/lib/kagglegym:/kaggle/lib', 'TERM': 'xterm-color', 'NV_LIBCUSPARSE_DEV_VERSION': '12.5.1.3-1', 'KAGGLE_DATA_PROXY_PROJECT': 'kaggle-161607', 'GIT_PAGER': 'cat', 'KAGGLE_USER_SECRETS_TOKEN': 'eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..y_WuAgHdfwYJf1u0yEU4jA.-5QAIF5MNSJMxHQL-r2hjZSy6rinYQY9rgXD9P1pIthqtFsTdjbVkrrhIZAfZE8kOMNAHmkPT_P4S99-5o16sO4FYgDX7fk_LIqdbePwr1Gh6wvyjx8GJO4qqtO3670XbmAoxtnPUpLW5fbDgxvz0w.6L4tiBD8Jj2rE_83dLQj5g', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_CUDNN_VERSION': '9.2.1.18-1', 'SHLVL': '0', 'PAGER': 'cat', 'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service', 'NV_CUDA_LIB_VERSION': '12.5.1-1', 'NVARCH': 'x86_64', 'KAGGLE_KERNEL_RUN_TYPE': 'Interactive', 'UV_CONSTRAINT': '', 'PYTHONUTF8': '1', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn9-dev-cuda-12=9.2.1.18-1', 'KAGGLE_DISABLE_GOOGLE_GENERATIVE_AI_INTEGRATION': 'True', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.22.3-1+cuda12.5', 'LD_LIBRARY_PATH': '/usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'KAGGLE_GCP_ZONE': 'us-central1-a', 'MKL_THREADING_LAYER': 'GNU', 'NV_CUDA_NSIGHT_COMPUTE_VERSION': '12.5.1-1', 'GIT_COMMIT': '0eb38ee5b80e4003101469c19d6da1f4353d5960', 'NV_NVPROF_VERSION': '12.5.82-1', 'LC_ALL': 'en_US.UTF-8', '_PYVIZ_COMMS_INSTALLED': '1', 'CUDA_HOME': '/usr/local/cuda', 'COLAB_FILE_HANDLER_ADDR': 'localhost:3453', 'KAGGLE_CONTAINER_NAME': 'kaggle_UX0lGClkIGGqkM4s56Ax0QWrtOr7jERQ9HqqHg4CgI-257140063-webtier', 'PATH': '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'PYTHONUSERBASE': '/root/.local', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer', 'NV_LIBNCCL_PACKAGE_VERSION': '2.22.3-1', 'KAGGLE_API_V1_TOKEN': '/etc/secrets/kaggle/api-v1-token', 'DEBIAN_FRONTEND': 'noninteractive', 'KAGGLE_GRPC_DATA_PROXY_URL': 'dp.kaggle.net:443', 'KAGGLE_DATA_PROXY_URL': 'https://dp.kaggle.net', 'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True,roundup_power2_divisions:[32:256,64:128,256:64,>:32]', 'WANDB_PROJECT': 'open-maize-vision2', 'WANDB_NAME': 'vision_multiGPU_globalbz16_epochs1', 'OPENSLOTH_WORLD_SIZE': '2', 'OPENSLOTH_FORWARD_BZ': '2', 'OPENSLOTH_GLOBAL_BZ': '16', 'OPENSLOTH_ACCUMULATION_STEPS': '8', 'OPENSLOTH_PER_DEVICE_TRAIN_BZ': '1', 'OPENSLOTH_OUTPUT_DIR': 'outputs/vision_multiGPU_experiment', 'OPENSLOTH_LOG_LEVEL': 'info', 'CUDA_VISIBLE_DEVICES': '0', 'CUDA_MODULE_LOADING': 'LAZY', 'ENABLE_RUNTIME_UPTIME_TELEMETRY': '1', 'TF2_BEHAVIOR': '1', 'TPU_ML_PLATFORM': 'Tensorflow', 'TPU_ML_PLATFORM_VERSION': '2.18.0', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'TF_USE_LEGACY_KERAS': '1', 'OPENSLOTH_LOCAL_RANK': '0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'python', 'UNSLOTH_IS_PRESENT': '1', 'HF_HUB_ENABLE_HF_TRANSFER': '1', 'HF_XET_HIGH_PERFORMANCE': '1', 'HF_XET_CHUNK_CACHE_SIZE_BYTES': '0', 'HF_XET_RECONSTRUCT_WRITE_SEQUENTIALLY': '0', 'HF_XET_NUM_CONCURRENT_RANGE_GETS': '64', 'TRITON_DISABLE_LINE_INFO': '1', 'TRITON_FRONT_END_DEBUGGING': '0', 'UNSLOTH_ZOO_IS_PRESENT': '1', 'UNSLOTH_PATCHED': '1', 'TORCHINDUCTOR_FX_GRAPH_CACHE': '1', 'TORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE': '1', 'ENABLE_AOT_AUTOGRAD_CACHE': '1', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'UNSLOTH_IGNORED_TOKENIZER_NAMES': 'unsloth/qwen2.5-coder-7b-instruct\\nunsloth/qwen2.5-coder-7b-instruct-bnb-4bit\\nunsloth/qwen2.5-coder-1.5b-instruct-bnb-4bit\\nunsloth/qwen2.5-coder-1.5b-instruct', 'UNSLOTH_DISABLE_STATIC_GENERATION': '1', 'UNSLOTH_FORCE_CUSTOM_DTYPE': \"float16;torch.float16;torch.float16;if name.endswith(('.conv')): module;from unsloth_zoo.temporary_patches.gemma3n import patch_Gemma3nConvNormAct_forward; patch_Gemma3nConvNormAct_forward()\", 'UNSLOTH_FORCE_FLOAT32': '0', 'UNSLOTH_RETURN_LOGITS': '0', 'UNSLOTH_FULLGRAPH': '1', 'UNSLOTH_USE_NEW_MODEL': '1', 'UNSLOTH_ENABLE_FULL_FINETUNING': '0', 'UNSLOTH_MIXED_PRECISION': 'float32', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29501'})\n\u001b[32m17:25:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:70\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Gemma3nProcessor\u001b[0m\nUnsloth: Making `model.base_model.model.model.language_model` require gradients\nUnsloth: Making `model.base_model.model.model.language_model` require gradients\n\u001b[32m17:25:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  lora_setup: 5.40s\u001b[0m\n\u001b[32m17:25:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_init: 1.2m\u001b[0m\n\u001b[32m17:25:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:195\u001b[0m | \u001b[1mLoading dataset from data/cached_vision_dataset_hf\u001b[0m\n\u001b[32m17:25:57\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:172\u001b[0m | \u001b[33m\u001b[1mDataset does not have 'labels' feature. This may affect training. Please check your dataset preparation.\u001b[0m\n[LOCAL_RANK=1] Patching log. Dir: outputs/vision_multiGPU_experiment, GPUs: 2\n\u001b[32m17:26:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:219\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n[LOCAL_RANK=0] Patching log. Dir: outputs/vision_multiGPU_experiment, GPUs: 2\n[LOCAL_RANK=1] Log patch initialization complete.\n🔧 Patching Trainer to use RandomSamplerSeededByEpoch\n[RANK=1] Patching trainer.save_model, trainer._save, and trainer._maybe_log_save_evaluate to no-op on non-master rank.\n[LOCAL_RANK=0] Log patch initialization complete.\n🔧 Patching Trainer to use RandomSamplerSeededByEpoch\n\u001b[32m17:26:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:151\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n\u001b[32m17:26:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  total_setup: 1.6m\u001b[0m\n\u001b[32m17:26:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_and_training_setup: 1.6m\u001b[0m\n\u001b[32m17:26:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:68\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjdmasciano2\u001b[0m (\u001b[33mjdmasciano2-university-of-lagos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.20.1\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250820_172602-umnzhvhk\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mvision_multiGPU_globalbz16_epochs1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jdmasciano2-university-of-lagos/open-maize-vision2\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jdmasciano2-university-of-lagos/open-maize-vision2/runs/umnzhvhk\u001b[0m\n  0%|                                                    | 0/11 [00:00<?, ?it/s]\u001b[32m17:26:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 1\u001b[0m\n\u001b[32m17:26:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 0: emitting 176 indices\nFirst ids dataset samples: [148, 36, 15, 124, 13, 155, 128, 121, 103, 19]\n...Last ids: [22, 139, 26, 35, 57, 62, 70, 6, 28, 163]\u001b[0m\n\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Maize Phosphorus Deficiency.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m17:26:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m17:26:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 0: emitting 176 indices\nFirst ids dataset samples: [148, 36, 15, 124, 13, 155, 128, 121, 103, 19]\n...Last ids: [22, 139, 26, 35, 57, 62, 70, 6, 28, 163]\u001b[0m\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n[Step 0] Effective Tokens: 0.0M/0.0M\n[Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 13.324899673461914, 'grad_norm': nan, 'learning_rate': 6.666666666666667e-05, 'epoch': 0.91}\n 91%|███████████████████████████████████████    | 10/11 [05:53<00:28, 28.15s/it]\u001b[32m17:32:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 0: dataset_size=176\n   📋 First 10 indices: [148, 36, 15, 124, 13, 155, 128, 121, 103, 19]\n   📋 Last 10 indices: [22, 139, 26, 35, 57, 62, 70, 6, 28, 163]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n100%|███████████████████████████████████████████| 11/11 [06:20<00:00, 34.61s/it]\n[rank1]:[W820 17:32:24.113329998 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n{'train_runtime': 385.6356, 'train_samples_per_second': 0.456, 'train_steps_per_second': 0.029, 'train_loss': 6.659385377710516, 'epoch': 1.0}\n100%|███████████████████████████████████████████| 11/11 [06:24<00:00, 34.91s/it]\n\u001b[32m17:32:28\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  actual_training: 6.5m\u001b[0m\n\u001b[3m                        \u001b[0m\u001b[1;3;32m⏱️  Training Step Timing Summary\u001b[0m\u001b[3m                         \u001b[0m\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃\u001b[1;35m                         \u001b[0m┃\u001b[1;35m        \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mAvg        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mTotal     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m                \u001b[0m┃\n┃\u001b[1;35m \u001b[0m\u001b[1;35mStep                   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mCount \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDuration   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDuration  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMin/Max       \u001b[0m\u001b[1;35m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36mmodel_and_training_set…\u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1     \u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1.6m       \u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m1.6m      \u001b[0m\u001b[34m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m1.6m/1.6m     \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36mactual_training        \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1     \u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m6.5m       \u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m6.5m      \u001b[0m\u001b[34m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m6.5m/6.5m     \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[1;36mTOTAL TRAINING\u001b[0m\u001b[36m         \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m-     \u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-          \u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[1;34m8.1m\u001b[0m\u001b[34m      \u001b[0m\u001b[34m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m-             \u001b[0m\u001b[35m \u001b[0m│\n└─────────────────────────┴────────┴─────────────┴────────────┴────────────────┘\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mvision_multiGPU_globalbz16_epochs1\u001b[0m at: \u001b[34mhttps://wandb.ai/jdmasciano2-university-of-lagos/open-maize-vision2/runs/umnzhvhk\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250820_172602-umnzhvhk/logs\u001b[0m\n[rank0]:[W820 17:32:35.576516143 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nAll processes finished\n","output_type":"stream"}],"execution_count":12},{"cell_type":"code","source":"# ==============================================================================\n# ALTERNATIVE: Using torchrun for proper multi-GPU training\n# ==============================================================================\n\n%%writefile run_multiGPU_training.sh\n#!/bin/bash\n\n# Proper way to run multi-GPU training with torchrun\ntorchrun --nproc_per_node=2 --nnodes=1 train_vision_multiGPU.py\n\n# Make the script executable and run it\n!chmod +x run_multiGPU_training.sh\n# !./run_multiGPU_training.sh  # Uncomment to run with torchrun","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile test_trained_model.py\n# ==============================================================================\n# STEP 3: Inference with the Fine-Tuned LoRA Model (DEFINITIVE FINAL VERSION)\n# ==============================================================================\n\n\"\"\"\nThis script uses the definitive, correct method for Gemma3N inference by\nexplicitly separating chat templating from data processing to resolve the\ntoken/image mismatch error.\n\"\"\"\n\nfrom unsloth import FastVisionModel\nfrom transformers import AutoProcessor\nfrom PIL import Image\nimport torch\nimport requests\nfrom io import BytesIO\n\ndef load_image_from_url(url: str) -> Image:\n    \"\"\"A helper function to load an image from a URL.\"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n        print(\"Image loaded successfully from URL.\")\n        return image\n    except requests.exceptions.RequestException as e:\n        print(f\"Error loading image from URL: {e}\")\n        return None\n    except Image.UnidentifiedImageError:\n        print(\"Error: The content downloaded from the URL is not a valid image.\")\n        return None\n\ndef run_inference():\n    # Define paths\n    base_model_name = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n    adapter_path = \"outputs/vision_multiGPU_experiment\"\n    \n    # Load base model and processor\n    print(f\"Loading base model: {base_model_name}\")\n    model, processor = FastVisionModel.from_pretrained(\n        model_name = base_model_name,\n        max_seq_length = 2048,\n        load_in_4bit = True,\n        dtype = None,\n    )\n    \n    print(\"\\nPreparing model for inference...\")\n    FastVisionModel.for_inference(model)\n    \n    # Load LoRA adapter\n    print(f\"Loading adapter from: {adapter_path}\")\n    model.load_adapter(adapter_path)\n    \n    print(\"\\nModel and adapter loaded successfully!\")\n    \n    # --- Test with a sample image ---\n    test_image_url = \"https://github.com/surfiniaburger/tune/blob/main/sample_images/phosphorus_deficiency_test_2.jpg?raw=true\"\n    image = load_image_from_url(test_image_url)\n    \n    if image is None:\n        return\n\n    # ** THE FINAL, CRUCIAL FIX IS HERE **\n\n    # 1. First, create the full multimodal message structure.\n    #    This includes the image object, which is critical.\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is the condition of this maize plant?\"},\n                {\"type\": \"image\", \"image\": image},\n            ],\n        }\n    ]\n    \n    # 2. Use the tokenizer's templating engine to generate the prompt string.\n    #    This is the step that will correctly insert the `<image>` token into the text.\n    text_prompt = processor.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    # 3. Now, call the main processor with the correctly formatted text and the image.\n    #    This will tokenize the text (including the `<image>` token) and process the image.\n    inputs = processor(\n        text=text_prompt,\n        images=image, # The processor can handle a single image here\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    print(\"\\nGenerating response...\")\n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            use_cache=True,\n        )\n    \n    response = processor.batch_decode(outputs, skip_special_tokens=True)\n    full_response = response[0]\n    \n    prompt_marker = \"model\\n\"\n    answer_start_index = full_response.rfind(prompt_marker)\n    \n    if answer_start_index != -1:\n        final_answer = full_response[answer_start_index + len(prompt_marker):].strip()\n    else:\n        final_answer = \"Could not parse the model's response.\"\n\n    print(\"=\"*40)\n    print(f\"✅ Model's Answer: {final_answer}\")\n    print(\"=\"*40)\n\nif __name__ == \"__main__\":\n    run_inference()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python test_trained_model.py  # Uncomment to test the model","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile app.py\n# ==============================================================================\n# GRADIO APP FOR MAIZE DIAGNOSIS (DEFINITIVE CORRECTED VERSION)\n# ==============================================================================\n\n\"\"\"\nThis script launches a Gradio web interface for the fine-tuned maize\nvision model, using the definitive, correct two-step processing logic.\n\"\"\"\n\nimport gradio as gr\nfrom unsloth import FastVisionModel\nfrom transformers import AutoProcessor\nfrom PIL import Image\nimport torch\nimport os\n\n# --- 1. Global Setup: Load Model and Processor ---\n# This section runs only ONCE when the application starts.\n\nprint(\"Performing initial model setup...\")\n\nBASE_MODEL_NAME = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\nADAPTER_PATH = \"outputs/vision_multiGPU_experiment\"\nmodel = None\nprocessor = None\n\ntry:\n    print(f\"Loading base model: {BASE_MODEL_NAME}\")\n    model, processor = FastVisionModel.from_pretrained(\n        model_name=BASE_MODEL_NAME,\n        max_seq_length=2048,\n        load_in_4bit=True,\n        dtype=None,\n    )\n    FastVisionModel.for_inference(model)\n    print(f\"Loading adapter from: {ADAPTER_PATH}\")\n    model.load_adapter(ADAPTER_PATH)\n    print(\"\\n✅ Model and adapter loaded successfully!\")\n\nexcept Exception as e:\n    print(f\"❌ Critical error during model loading: {e}\")\n\n\n# --- 2. Define the Core Prediction Function (Corrected Logic) ---\n\ndef diagnose_maize_plant(uploaded_image: Image.Image) -> str:\n    \"\"\"\n    Takes a PIL Image, runs it through the model, and returns the diagnosis.\n    \"\"\"\n    if model is None or processor is None or uploaded_image is None:\n        return \"Model is not loaded or no image was uploaded. Please check the console for errors.\"\n\n    image = uploaded_image.convert(\"RGB\")\n    \n    # ** THE FINAL, CRUCIAL FIX IS HERE **\n\n    # 1. Create the multimodal message structure including the image object.\n    #    This is the required input for the chat templating engine.\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is the condition of this maize plant?\"},\n                {\"type\": \"image\", \"image\": image},\n            ],\n        }\n    ]\n\n    # 2. Use the tokenizer to apply the chat template.\n    #    This correctly creates the final prompt string with the `<image>` placeholder.\n    text_prompt = processor.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    # 3. Call the main processor with the pre-formatted text and the image.\n    #    This is the robust method that provides the exact inputs the model needs.\n    inputs = processor(\n        text=text_prompt,\n        images=image,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    # Generate the response\n    with torch.inference_mode():\n        outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n    \n    # Decode and clean the final answer\n    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n    prompt_marker = \"model\\n\"\n    answer_start_index = response.rfind(prompt_marker)\n    \n    if answer_start_index != -1:\n        final_answer = response[answer_start_index + len(prompt_marker):].strip()\n    else:\n        final_answer = \"Could not parse model's response. Raw output: \" + response\n\n    return final_answer\n\n\n# --- 3. Build and Launch the Gradio Interface ---\n\nprint(\"Building Gradio interface...\")\n\nexample_images = [\n    [\"https://raw.githubusercontent.com/surfiniaburger/tune/main/sample_images/healthy_maize_test_1.jpg\"],\n    [\"https://raw.githubusercontent.com/surfiniaburger/tune/main/sample_images/phosphorus_deficiency_test_2.jpg\"]\n]\n\ndemo = gr.Interface(\n    fn=diagnose_maize_plant,\n    inputs=gr.Image(type=\"pil\", label=\"Upload Maize Plant Image\"),\n    outputs=gr.Textbox(label=\"Diagnosis\", lines=3),\n    title=\"🌽 Maize Health Diagnosis Assistant\",\n    description=\"Upload an image of a maize plant, and the AI will analyze its condition. This tool is powered by a fine-tuned Gemma3N vision model.\",\n    article=\"Built with Unsloth, OpenSloth, and Gradio.\",\n    examples=example_images,\n    allow_flagging=\"never\",\n)\n\nprint(\"Launching Gradio app... Access it at the URL provided below.\")\ndemo.launch(share=True)","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python app.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile upload_adapter.py\n\nfrom huggingface_hub import HfApi, create_repo\n\n# --- CONFIGURATION ---\n# Your username and a new name for this 2-GPU model version.\nHF_USERNAME = \"surfiniaburger\"\nREPO_NAME = \"AuraMind-Maize-2GPU\"\n\n# The local folder containing your adapter\nLOCAL_ADAPTER_FOLDER = \"outputs/vision_multiGPU_experiment\"\nHF_REPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n\n# --- SCRIPT LOGIC ---\napi = HfApi()\n\nprint(f\"Creating repository '{HF_REPO_ID}' on the Hugging Face Hub...\")\ntry:\n    create_repo(repo_id=HF_REPO_ID, repo_type=\"model\", exist_ok=True)\n    print(\"Repository created successfully (or already exists).\")\nexcept Exception as e:\n    print(f\"Error creating repository: {e}\")\n    exit()\n\nprint(f\"\\nUploading files from '{LOCAL_ADAPTER_FOLDER}' to '{HF_REPO_ID}'...\")\ntry:\n    api.upload_folder(\n        folder_path=LOCAL_ADAPTER_FOLDER,\n        repo_id=HF_REPO_ID,\n        repo_type=\"model\",\n    )\n    print(f\"\\n✅ Successfully uploaded adapter to: https://huggingface.co/{HF_REPO_ID}\")\nexcept Exception as e:\n    print(f\"Error uploading files: {e}\")\n","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"!python upload_adapter.py","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"# ==============================================================================\n# STEP 1: Securely Upload Your Adapter to the Hugging Face Hub\n# ==============================================================================\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login, HfApi, create_repo\n\n# --- 1. Secure Login ---\nprint(\"Attempting secure login to Hugging Face Hub...\")\ntry:\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n    login(token=secret_value)\n    print(\"✅ Secure login successful!\")\nexcept Exception as e:\n    print(f\"❌ Could not log in. Please ensure 'HUGGINGFACE_API_KEY' is set in Kaggle Secrets. Error: {e}\")\n    # We exit here if login fails, as the rest cannot proceed.\n    exit()\n\n# --- 2. Define the Upload Script Configuration ---\nHF_USERNAME = \"surfiniaburger\"\nREPO_NAME = \"AuraMind-Maize-2GPU\"\nLOCAL_ADAPTER_FOLDER = \"outputs/vision_multiGPU_experiment\"\nHF_REPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n\n# --- 3. Run the Upload Logic ---\napi = HfApi()\n\nprint(f\"\\nCreating repository '{HF_REPO_ID}' on the Hugging Face Hub...\")\ntry:\n    create_repo(repo_id=HF_REPO_ID, repo_type=\"model\", exist_ok=True)\n    print(\"Repository created successfully (or already exists).\")\nexcept Exception as e:\n    print(f\"Error creating repository: {e}\")\n    exit()\n\nprint(f\"\\nUploading files from '{LOCAL_ADAPTER_FOLDER}' to '{HF_REPO_ID}'...\")\ntry:\n    api.upload_folder(\n        folder_path=LOCAL_ADAPTER_FOLDER,\n        repo_id=HF_REPO_ID,\n        repo_type=\"model\",\n    )\n    print(f\"\\n✅✅ Successfully uploaded adapter to: https://huggingface.co/{HF_REPO_ID}\")\nexcept Exception as e:\n    print(f\"Error uploading files: {e}\")","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile app.py\n# ==============================================================================\n# AURA-MIND: MAIZE HEALTH DIAGNOSIS APP (DEPLOYMENT-READY)\n# ==============================================================================\n\nimport gradio as gr\nfrom unsloth import FastVisionModel\nfrom transformers import AutoProcessor\nfrom PIL import Image\nimport torch\nimport os\n\n# --- 1. Global Setup: Load Model and Processor from Hub ---\n\nprint(\"Performing initial AuraMind model setup...\")\n\nBASE_MODEL_NAME = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\nADAPTER_PATH = \"surfiniaburger/AuraMind-Maize-2GPU\"\n\nmodel = None\nprocessor = None\n\ntry:\n    print(f\"Loading base model: {BASE_MODEL_NAME}\")\n    model, processor = FastVisionModel.from_pretrained(\n        model_name=BASE_MODEL_NAME, max_seq_length=2048, load_in_4bit=True, dtype=None\n    )\n    FastVisionModel.for_inference(model)\n    \n    print(f\"Loading AuraMind adapter from Hub: {ADAPTER_PATH}\")\n    model.load_adapter(ADAPTER_PATH)\n    \n    print(\"\\n✅ AuraMind model and adapter loaded successfully!\")\nexcept Exception as e:\n    print(f\"❌ Critical error during model loading: {e}\")\n\n\n# --- 2. Define the Core Prediction Function ---\ndef diagnose_maize_plant(uploaded_image: Image.Image) -> str:\n    if model is None or processor is None or uploaded_image is None:\n        return \"Model is not loaded or no image was uploaded. Please check the console for errors.\"\n\n    image = uploaded_image.convert(\"RGB\")\n    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the condition of this maize plant?\"}, {\"type\": \"image\", \"image\": image}]}]\n    text_prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(model.device)\n\n    with torch.inference_mode():\n        outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n    \n    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n    prompt_marker = \"model\\n\"\n    answer_start_index = response.rfind(prompt_marker)\n    \n    final_answer = response[answer_start_index + len(prompt_marker):].strip() if answer_start_index != -1 else \"Could not parse model's response.\"\n    return final_answer\n\n# --- 3. Build and Launch the Gradio Interface ---\nprint(\"Building Gradio interface...\")\n\ndemo = gr.Interface(\n    fn=diagnose_maize_plant,\n    inputs=gr.Image(type=\"pil\", label=\"Upload Maize Plant Image\"),\n    outputs=gr.Textbox(label=\"Diagnosis\", lines=3),\n    title=\"🌽 AuraMind: Maize Health Diagnosis (2-GPU Ver.)\",\n    description=\"Upload an image of a maize plant, and the AuraMind AI will analyze its condition. This model was fine-tuned on two GPUs for enhanced performance.\",\n    article=\"Built with Unsloth and Gradio by surfiniaburger.\",\n    allow_flagging=\"never\",\n)\n\nif __name__ == \"__main__\":\n    print(\"Launching Gradio app for deployment...\")\n    # For deployment, we don't need a share link. Gradio deploy handles it.\n    demo.launch()","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"","metadata":{"trusted":true},"outputs":[],"execution_count":null}]}