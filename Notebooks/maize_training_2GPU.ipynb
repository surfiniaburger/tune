{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12544913,"sourceType":"datasetVersion","datasetId":7920300},{"sourceId":12548828,"sourceType":"datasetVersion","datasetId":7923075},{"sourceId":12548854,"sourceType":"datasetVersion","datasetId":7923091}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"%%capture\n!pip install pip3-autoremove\n!pip install torch torchvision torchaudio xformers --index-url https://download.pytorch.org/whl/cu124\n!pip install accelerate==1.7.0\n!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3  trl triton cut_cross_entropy \n!pip install sentencepiece protobuf \"datasets>=3.4.1\" huggingface_hub hf_transfer\n!pip install -U peft\n!pip install --no-deps --upgrade timm # Only for Gemma 3N","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:56:31.507260Z","iopub.execute_input":"2025-07-22T20:56:31.507407Z","iopub.status.idle":"2025-07-22T20:58:10.150289Z","shell.execute_reply.started":"2025-07-22T20:56:31.507393Z","shell.execute_reply":"2025-07-22T20:58:10.149331Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"%%capture\n!pip install opensloth ","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:58:13.967934Z","iopub.execute_input":"2025-07-22T20:58:13.968607Z","iopub.status.idle":"2025-07-22T20:58:25.267422Z","shell.execute_reply.started":"2025-07-22T20:58:13.968574Z","shell.execute_reply":"2025-07-22T20:58:25.266637Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"!pip install --no-deps git+https://github.com/huggingface/transformers.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:58:29.418937Z","iopub.execute_input":"2025-07-22T20:58:29.419193Z","iopub.status.idle":"2025-07-22T20:59:00.578086Z","shell.execute_reply.started":"2025-07-22T20:58:29.419170Z","shell.execute_reply":"2025-07-22T20:59:00.577083Z"}},"outputs":[{"name":"stdout","text":"Collecting git+https://github.com/huggingface/transformers.git\n  Cloning https://github.com/huggingface/transformers.git to /tmp/pip-req-build-bjdeldb_\n  Running command git clone --filter=blob:none --quiet https://github.com/huggingface/transformers.git /tmp/pip-req-build-bjdeldb_\n  Resolved https://github.com/huggingface/transformers.git to commit c6d0500d15b9eedc33e9131a6bec6db56282b875\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\nBuilding wheels for collected packages: transformers\n  Building wheel for transformers (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Created wheel for transformers: filename=transformers-4.54.0.dev0-py3-none-any.whl size=11988536 sha256=071afb39a71f72d06523b2f9360dace82e28c94ce5101bff2f0f48a794bd3816\n  Stored in directory: /tmp/pip-ephem-wheel-cache-b5wa9raz/wheels/32/4b/78/f195c684dd3a9ed21f3b39fe8f85b48df7918581b6437be143\nSuccessfully built transformers\nInstalling collected packages: transformers\n  Attempting uninstall: transformers\n    Found existing installation: transformers 4.52.4\n    Uninstalling transformers-4.52.4:\n      Successfully uninstalled transformers-4.52.4\nSuccessfully installed transformers-4.54.0.dev0\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"%%capture\n!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth-zoo.git\n!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth.git","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:59:00.579654Z","iopub.execute_input":"2025-07-22T20:59:00.580513Z","iopub.status.idle":"2025-07-22T20:59:17.897698Z","shell.execute_reply.started":"2025-07-22T20:59:00.580479Z","shell.execute_reply":"2025-07-22T20:59:17.896790Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"\nfrom datasets import Dataset, Image as HFImage\nfrom pathlib import Path\nimport os\n\n# --- NEW STEP: Copy data to the faster working directory ---\nsource_path = \"/kaggle/input/maize1-dataset/\"\nlocal_path = \"/kaggle/working/local_datasets/\"\n\nif not os.path.exists(local_path):\n    print(f\"Copying data from {source_path} to {local_path} for faster access...\")\n    !cp -r {source_path} {local_path}\n    print(\"✅ Data copy complete.\")\nelse:\n    print(f\"✅ Data already copied to {local_path}\")\n# ---------------------------------------------------------","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:59:29.058085Z","iopub.execute_input":"2025-07-22T20:59:29.058372Z","iopub.status.idle":"2025-07-22T20:59:35.118740Z","shell.execute_reply.started":"2025-07-22T20:59:29.058346Z","shell.execute_reply":"2025-07-22T20:59:35.117939Z"}},"outputs":[{"name":"stdout","text":"Copying data from /kaggle/input/maize1-dataset/ to /kaggle/working/local_datasets/ for faster access...\n✅ Data copy complete.\n","output_type":"stream"}],"execution_count":5},{"cell_type":"code","source":"%%writefile cache_vision_dataset.py\n# ==============================================================================\n# STEP 1: Cache Vision Dataset to Disk (FINAL - MANUAL COMPONENT VERSION)\n# ==============================================================================\n\n\"\"\"\nPre-processes and caches a vision dataset by calling the tokenizer and image\nprocessor components separately. This is the most robust method and bypasses\nthe error-prone main processor call.\n\"\"\"\n\nfrom datasets import Dataset\nfrom pathlib import Path\nfrom PIL import Image\nfrom tqdm import tqdm\nfrom transformers import AutoProcessor\nimport os\n\n# --- Mappings and Functions ---\nCLASS_NAME_MAPPING = {\n    \"maize_healthy\": \"Healthy Maize Plant\",\n    \"phosphorus_deficiency\": \"Maize Phosphorus Deficiency\",\n}\n\ndef create_conversation_dict(image_path, class_name):\n    \"\"\"Creates the 'messages' dictionary structure for a single sample.\"\"\"\n    display_name = CLASS_NAME_MAPPING.get(class_name, \"Unknown Maize Condition\")\n    pil_image = Image.open(image_path).convert(\"RGB\")\n    return {\n        \"messages\": [\n            { \"role\": \"user\",\n              \"content\": [\n                {\"type\": \"text\", \"text\": \"What is the condition of this maize plant?\"},\n                {\"type\": \"image\", \"image\": pil_image}\n              ]\n            },\n            { \"role\": \"assistant\",\n              \"content\": [\n                {\"type\": \"text\", \"text\": f\"This is a {display_name}.\"}\n              ]\n            },\n        ]\n    }\n\ndef dump_vision_data():\n    MODEL_NAME = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n    print(f\"Loading processor for '{MODEL_NAME}'...\")\n    processor = AutoProcessor.from_pretrained(MODEL_NAME)\n    \n    dataset_path = Path(\"/kaggle/working/local_datasets/\")\n    image_paths = list(dataset_path.glob(\"**/*.jpg\")) + list(dataset_path.glob(\"**/*.jpeg\"))\n    print(f\"Found {len(image_paths)} images.\")\n    \n    print(\"Creating dataset with 'messages' format...\")\n    raw_dataset_list = []\n    for path in tqdm(image_paths, desc=\"Processing images\"):\n        raw_dataset_list.append(create_conversation_dict(path, path.parent.name))\n        \n    # ** FINAL STRATEGY: A MANUAL BATCHING LOOP USING SEPARATE COMPONENTS **\n    print(\"Processing dataset manually using separate tokenizer and image processor...\")\n    \n    batch_size = 8\n    processed_list = []\n\n    for i in tqdm(range(0, len(raw_dataset_list), batch_size), desc=\"Processing batches\"):\n        batch = raw_dataset_list[i : i + batch_size]\n        \n        # 1. Prepare lists of texts and images for this batch\n        batch_texts = []\n        batch_images = []\n        for sample in batch:\n            text = processor.tokenizer.apply_chat_template(\n                sample[\"messages\"], tokenize=False, add_generation_prompt=False\n            )\n            image = sample[\"messages\"][0]['content'][1]['image']\n            batch_texts.append(text)\n            batch_images.append(image)\n\n        # 2. **THE FIX**: Call tokenizer and image_processor SEPARATELY\n        # Process the text part\n        text_inputs = processor.tokenizer(\n            batch_texts,\n            padding=True,\n            truncation=True,\n            return_tensors=\"pt\",\n        )\n        # Process the image part\n        image_inputs = processor.image_processor(\n            images=batch_images,\n            return_tensors=\"pt\"\n        )\n        \n        # 3. Manually combine the results\n        # The text_inputs dictionary already has 'input_ids' and 'attention_mask'\n        # We just need to add the 'pixel_values' to it.\n        combined_inputs = {\n            \"input_ids\": text_inputs.input_ids,\n            \"attention_mask\": text_inputs.attention_mask,\n            \"pixel_values\": image_inputs.pixel_values,\n        }\n        \n        # 4. Unpack the processed batch back into individual samples for our list\n        for j in range(len(batch_texts)):\n            processed_list.append({\n                \"input_ids\": combined_inputs[\"input_ids\"][j],\n                \"attention_mask\": combined_inputs[\"attention_mask\"][j],\n                \"pixel_values\": combined_inputs[\"pixel_values\"][j],\n            })\n\n    # 5. Create the final dataset from the list of processed dictionaries\n    print(\"\\nCreating final dataset from processed list...\")\n    processed_dataset = Dataset.from_list(processed_list)\n\n    # 6. Save the final, processed dataset to disk\n    output_dir = \"data/cached_vision_dataset_hf\"\n    os.makedirs(output_dir, exist_ok=True)\n    \n    print(f\"Saving processed dataset to directory: {output_dir}\")\n    processed_dataset.save_to_disk(output_dir)\n    \n    print(f\"\\n✅ Dataset successfully processed and saved to '{output_dir}'.\")\n    print(f\"The dataset now contains the required columns: {processed_dataset.column_names}\")\n\nif __name__ == \"__main__\":\n    dump_vision_data()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:59:40.948295Z","iopub.execute_input":"2025-07-22T20:59:40.949054Z","iopub.status.idle":"2025-07-22T20:59:40.957518Z","shell.execute_reply.started":"2025-07-22T20:59:40.949028Z","shell.execute_reply":"2025-07-22T20:59:40.956795Z"}},"outputs":[{"name":"stdout","text":"Writing cache_vision_dataset.py\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# Run the caching script\n!python cache_vision_dataset.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T20:59:49.867666Z","iopub.execute_input":"2025-07-22T20:59:49.868010Z","iopub.status.idle":"2025-07-22T21:01:07.843304Z","shell.execute_reply.started":"2025-07-22T20:59:49.867987Z","shell.execute_reply":"2025-07-22T21:01:07.842539Z"}},"outputs":[{"name":"stdout","text":"2025-07-22 20:59:59.200956: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753217999.386851     281 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753217999.443120     281 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nLoading processor for 'unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit'...\nprocessor_config.json: 100%|██████████████████| 98.0/98.0 [00:00<00:00, 830kB/s]\nchat_template.jinja: 1.63kB [00:00, 7.29MB/s]\npreprocessor_config.json: 1.09kB [00:00, 8.03MB/s]\nconfig.json: 5.21kB [00:00, 23.0MB/s]\ntokenizer_config.json: 1.20MB [00:00, 201MB/s]\ntokenizer.model: 100%|█████████████████████| 4.70M/4.70M [00:00<00:00, 7.29MB/s]\ntokenizer.json: 100%|██████████████████████| 33.4M/33.4M [00:00<00:00, 57.2MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 777/777 [00:00<00:00, 9.08MB/s]\nFound 176 images.\nCreating dataset with 'messages' format...\nProcessing images: 100%|██████████████████████| 176/176 [00:09<00:00, 18.43it/s]\nProcessing dataset manually using separate tokenizer and image processor...\nProcessing batches: 100%|███████████████████████| 22/22 [00:17<00:00,  1.23it/s]\n\nCreating final dataset from processed list...\nSaving processed dataset to directory: data/cached_vision_dataset_hf\nSaving the dataset (3/3 shards): 100%|█| 176/176 [00:00<00:00, 190.63 examples/s\n\n✅ Dataset successfully processed and saved to 'data/cached_vision_dataset_hf'.\nThe dataset now contains the required columns: ['input_ids', 'attention_mask', 'pixel_values']\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"!wandb login token","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:01:08.203191Z","iopub.execute_input":"2025-07-22T21:01:08.204068Z","iopub.status.idle":"2025-07-22T21:01:10.208233Z","shell.execute_reply.started":"2025-07-22T21:01:08.204036Z","shell.execute_reply":"2025-07-22T21:01:10.207284Z"}},"outputs":[{"name":"stdout","text":"\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"%%writefile train_vision_multiGPU.py\n# ==============================================================================\n# STEP 2: Multi-GPU Training Script\n# ==============================================================================\n\n\"\"\"\nMulti-GPU Vision Model Training with OpenSloth\n\"\"\"\n\nimport os\nimport pickle\nfrom opensloth.opensloth_config import (\n    FastModelArgs,\n    LoraArgs,\n    OpenSlothConfig,\n    TrainingArguments,\n)\nfrom opensloth.scripts.opensloth_sft_trainer import run_mp_training, setup_envs\n\n# Set PyTorch memory allocation configuration\nos.environ[\"PYTORCH_CUDA_ALLOC_CONF\"] = \"expandable_segments:True\"\n\n# Multi-GPU Configuration\nGLOBAL_BZ = 16\nDEVICES = [0, 1]\nBZ = 1\n\n# OpenSloth Configuration for Vision Models\nopensloth_config = OpenSlothConfig(\n    data_cache_path=\"data/cached_vision_dataset_hf\",\n    devices=DEVICES,\n    fast_model_args=FastModelArgs(\n        model_name=\"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\",\n        max_seq_length=2048,\n        load_in_4bit=True,\n        dtype=None,\n        use_gradient_checkpointing=\"unsloth\",  # Use Unsloth's optimized gradient checkpointing\n    ),\n    lora_args=LoraArgs(\n        r=16, # Consider reducing to 8 if memory is still an issue\n        lora_alpha=16,\n        target_modules=[\n            \"q_proj\",\n            \"k_proj\",\n            \"v_proj\",\n            \"o_proj\",\n            \"gate_proj\",\n            \"up_proj\",\n            \"down_proj\",\n        ],\n        lora_dropout=0,\n        bias=\"none\",\n        use_rslora=False,\n        finetune_vision_layers=True,\n        finetune_language_layers=True,\n    ),\n    sequence_packing=False,\n)\n\ntraining_config = TrainingArguments(\n    output_dir=\"outputs/vision_multiGPU_experiment\",\n    per_device_train_batch_size=BZ,\n    gradient_accumulation_steps=GLOBAL_BZ // (len(DEVICES) * BZ),\n    learning_rate=1e-4,\n    logging_steps=10,\n    num_train_epochs=18,\n    lr_scheduler_type=\"linear\",\n    warmup_ratio=0.1,\n    save_total_limit=2,\n    save_steps=100,\n    weight_decay=0.01,\n    optim=\"adamw_torch_fused\",\n    seed=3407,\n    remove_unused_columns=False,\n    dataset_text_field=\"\",\n    max_seq_length=1024,\n    dataloader_pin_memory=True,\n    fp16=True,  # Enable mixed-precision training\n    report_to=\"wandb\",\n    resume_from_checkpoint=\"\",\n\n)\n\nif __name__ == \"__main__\":\n    # Setup environment variables for logging\n    os.environ[\"WANDB_PROJECT\"] = \"open-maize-vision1\"\n    os.environ[\"WANDB_NAME\"] = f\"vision_multiGPU_globalbz{GLOBAL_BZ}_epochs{training_config.num_train_epochs}\"\n\n    print(f\"Global batch size: {len(DEVICES) * BZ * training_config.gradient_accumulation_steps}\")\n    print(f\"Gradient accumulation steps: {training_config.gradient_accumulation_steps}\")\n\n    setup_envs(opensloth_config, training_config)\n    run_mp_training(opensloth_config.devices, opensloth_config, training_config)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:01:50.975005Z","iopub.execute_input":"2025-07-22T21:01:50.975527Z","iopub.status.idle":"2025-07-22T21:01:50.981031Z","shell.execute_reply.started":"2025-07-22T21:01:50.975506Z","shell.execute_reply":"2025-07-22T21:01:50.980245Z"}},"outputs":[{"name":"stdout","text":"Overwriting train_vision_multiGPU.py\n","output_type":"stream"}],"execution_count":10},{"cell_type":"code","source":"# Run the training script\n!python train_vision_multiGPU.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-22T21:01:59.281512Z","iopub.execute_input":"2025-07-22T21:01:59.281786Z","iopub.status.idle":"2025-07-22T22:35:27.646039Z","shell.execute_reply.started":"2025-07-22T21:01:59.281765Z","shell.execute_reply":"2025-07-22T22:35:27.645207Z"}},"outputs":[{"name":"stdout","text":"Global batch size: 16\nGradient accumulation steps: 8\nGlobal batch size: 16\n[MP] Running on 2 GPUs\n2025-07-22 21:02:07.046301: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n2025-07-22 21:02:07.046302: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753218127.069956     320 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753218127.069990     321 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753218127.076952     320 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\nE0000 00:00:1753218127.076957     321 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n\u001b[32m21:02:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:41\u001b[0m | \u001b[1mTraining on GPU 0 with output_dir outputs/vision_multiGPU_experiment\u001b[0m\n\u001b[32m21:02:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:44\u001b[0m | \u001b[1m🚀 Starting total training timer\u001b[0m\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n🦥 Unsloth Zoo will now patch everything to make training faster!\n🦥 Unsloth Zoo will now patch everything to make training faster!\nUsing compiler location: .cache/unsloth_compiled_cache_0\nUsing compiler location: .cache/unsloth_compiled_cache_1\n==((====))==  Unsloth 2025.7.7: Fast Gemma3N patching. Transformers: 4.54.0.dev0.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n==((====))==  Unsloth 2025.7.7: Fast Gemma3N patching. Transformers: 4.54.0.dev0.\n   \\\\   /|    Tesla T4. Num GPUs = 1. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\nmodel.safetensors.index.json: 330kB [00:00, 272MB/s]\nFetching 3 files:   0%|                                   | 0/3 [00:00<?, ?it/s]\nmodel-00002-of-00003.safetensors:   0%|             | 0.00/4.99G [00:00<?, ?B/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|              | 0.00/469M [00:00<?, ?B/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   0%|             | 0.00/2.65G [00:00<?, ?B/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:   0%|       | 268k/469M [00:00<16:36, 470kB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:   2%|     | 7.53M/469M [00:00<00:34, 13.4MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  10%|▌    | 48.2M/469M [00:00<00:05, 82.8MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  21%|█▎    | 99.2M/469M [00:01<00:02, 126MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  41%|██▊    | 191M/469M [00:01<00:01, 209MB/s]\u001b[A\u001b[A\n\nmodel-00003-of-00003.safetensors:  62%|████▎  | 293M/469M [00:01<00:00, 287MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   0%|    | 215k/2.65G [00:01<5:37:18, 131kB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  71%|████▉  | 335M/469M [00:01<00:00, 281MB/s]\u001b[A\u001b[A\nmodel-00001-of-00003.safetensors:   0%|    | 7.18M/2.65G [00:01<08:02, 5.47MB/s]\u001b[A\n\nmodel-00003-of-00003.safetensors:  86%|█████▉ | 402M/469M [00:01<00:00, 323MB/s]\u001b[A\u001b[A\nmodel-00003-of-00003.safetensors: 100%|███████| 469M/469M [00:02<00:00, 230MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors:   2%|    | 48.2M/2.65G [00:02<00:54, 47.8MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   4%|▏     | 102M/2.65G [00:02<00:21, 117MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   5%|▎     | 129M/2.65G [00:02<00:19, 127MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   6%|▎     | 155M/2.65G [00:02<00:20, 119MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   7%|▍     | 183M/2.65G [00:02<00:17, 145MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:   8%|▍     | 217M/2.65G [00:02<00:15, 158MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  12%|▋     | 306M/2.65G [00:02<00:08, 283MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  13%|▊     | 350M/2.65G [00:03<00:07, 306MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  15%|▉     | 387M/2.65G [00:03<00:07, 317MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  16%|▉     | 428M/2.65G [00:03<00:07, 316MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  18%|█     | 489M/2.65G [00:03<00:05, 379MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  21%|█▏    | 545M/2.65G [00:03<00:05, 415MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  22%|█▎    | 589M/2.65G [00:03<00:05, 406MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  24%|█▍    | 637M/2.65G [00:03<00:04, 416MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  26%|█▌    | 683M/2.65G [00:03<00:05, 357MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  29%|█▊    | 775M/2.65G [00:04<00:03, 491MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  31%|█▉    | 830M/2.65G [00:04<00:04, 447MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  35%|██    | 915M/2.65G [00:04<00:03, 527MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  37%|██▏   | 972M/2.65G [00:04<00:03, 464MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  39%|█▉   | 1.03G/2.65G [00:04<00:04, 398MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  41%|██   | 1.08G/2.65G [00:04<00:05, 292MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   0%| | 8.13k/4.99G [00:05<872:25:27, 1.59kB/s\u001b[A\nmodel-00002-of-00003.safetensors:   0%|   | 1.09M/4.99G [00:05<4:40:57, 296kB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   0%|   | 1.86M/4.99G [00:05<2:25:32, 572kB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  42%|██   | 1.12G/2.65G [00:05<00:07, 206MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   0%|   | 2.56M/4.99G [00:05<1:39:09, 839kB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   0%|    | 8.82M/4.99G [00:05<17:27, 4.76MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  43%|██▏  | 1.15G/2.65G [00:06<00:11, 125MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   0%|    | 11.4M/4.99G [00:06<17:42, 4.69MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  46%|██▎  | 1.22G/2.65G [00:06<00:09, 159MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  48%|██▍  | 1.28G/2.65G [00:06<00:06, 203MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   1%|    | 57.7M/4.99G [00:06<02:28, 33.3MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  53%|██▋  | 1.41G/2.65G [00:06<00:04, 263MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  56%|██▊  | 1.47G/2.65G [00:06<00:04, 265MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   1%|    | 74.3M/4.99G [00:07<02:51, 28.7MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  58%|██▉  | 1.54G/2.65G [00:07<00:04, 227MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  62%|███  | 1.64G/2.65G [00:07<00:04, 244MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  64%|███▏ | 1.70G/2.65G [00:08<00:04, 223MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   2%|     | 104M/4.99G [00:08<03:03, 26.7MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   3%|▏    | 171M/4.99G [00:08<01:29, 54.2MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  68%|███▍ | 1.80G/2.65G [00:09<00:05, 152MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  70%|███▌ | 1.87G/2.65G [00:09<00:04, 190MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   5%|▏    | 238M/4.99G [00:09<00:59, 79.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  73%|███▋ | 1.93G/2.65G [00:09<00:03, 202MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  75%|███▊ | 1.99G/2.65G [00:10<00:03, 167MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   6%|▎    | 305M/4.99G [00:10<00:59, 78.4MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  78%|███▉ | 2.06G/2.65G [00:10<00:03, 170MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  83%|████▏| 2.19G/2.65G [00:10<00:02, 218MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  85%|████▎| 2.26G/2.65G [00:10<00:01, 247MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  88%|████▍| 2.33G/2.65G [00:11<00:01, 170MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   7%|▎    | 373M/4.99G [00:11<01:19, 58.5MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  90%|████▌| 2.39G/2.65G [00:11<00:01, 201MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:   9%|▍    | 440M/4.99G [00:12<01:00, 74.9MB/s]\u001b[A\nmodel-00001-of-00003.safetensors:  93%|████▋| 2.46G/2.65G [00:12<00:01, 167MB/s]\u001b[A\n\nmodel-00001-of-00003.safetensors: 100%|█████| 2.65G/2.65G [00:12<00:00, 206MB/s]\u001b[A\nFetching 3 files:  33%|█████████                  | 1/3 [00:13<00:26, 13.23s/it]\nmodel-00002-of-00003.safetensors:  12%|▋     | 612M/4.99G [00:13<00:36, 121MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  14%|▊     | 679M/4.99G [00:13<00:31, 135MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  15%|▉     | 747M/4.99G [00:13<00:26, 158MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  16%|▉     | 814M/4.99G [00:13<00:21, 196MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  18%|█     | 882M/4.99G [00:14<00:29, 141MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  19%|█▏    | 949M/4.99G [00:15<00:25, 160MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  20%|█    | 1.02G/4.99G [00:15<00:20, 196MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  22%|█    | 1.09G/4.99G [00:15<00:17, 225MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  23%|█▏   | 1.15G/4.99G [00:15<00:14, 273MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  24%|█▏   | 1.22G/4.99G [00:15<00:12, 296MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  26%|█▎   | 1.29G/4.99G [00:15<00:10, 350MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  27%|█▎   | 1.35G/4.99G [00:15<00:09, 398MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  28%|█▍   | 1.42G/4.99G [00:16<00:08, 399MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  30%|█▍   | 1.49G/4.99G [00:16<00:08, 434MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  31%|█▌   | 1.56G/4.99G [00:16<00:07, 435MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  33%|█▋   | 1.62G/4.99G [00:16<00:10, 330MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  34%|█▋   | 1.69G/4.99G [00:16<00:09, 366MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  35%|█▊   | 1.76G/4.99G [00:16<00:07, 412MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  37%|█▊   | 1.82G/4.99G [00:17<00:08, 379MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  38%|█▉   | 1.89G/4.99G [00:17<00:07, 419MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  39%|█▉   | 1.96G/4.99G [00:17<00:06, 447MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  41%|██   | 2.03G/4.99G [00:17<00:06, 436MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  42%|██   | 2.09G/4.99G [00:17<00:06, 451MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  43%|██▏  | 2.16G/4.99G [00:17<00:06, 472MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  45%|██▏  | 2.23G/4.99G [00:17<00:05, 520MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  46%|██▎  | 2.30G/4.99G [00:18<00:05, 480MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  47%|█▉  | 2.37G/4.99G [00:20<00:29, 89.4MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  50%|██▌  | 2.51G/4.99G [00:20<00:19, 125MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  52%|██▌  | 2.57G/4.99G [00:21<00:17, 138MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  53%|██▋  | 2.66G/4.99G [00:21<00:17, 134MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  56%|██▊  | 2.79G/4.99G [00:23<00:19, 111MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  58%|██▉  | 2.92G/4.99G [00:23<00:13, 155MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  60%|██▉  | 2.99G/4.99G [00:23<00:11, 182MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  61%|███  | 3.05G/4.99G [00:23<00:09, 203MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  62%|███  | 3.12G/4.99G [00:24<00:07, 240MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  64%|███▏ | 3.19G/4.99G [00:24<00:08, 223MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  68%|███▍ | 3.39G/4.99G [00:24<00:05, 316MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  72%|███▌ | 3.59G/4.99G [00:25<00:03, 392MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  75%|███▋ | 3.72G/4.99G [00:25<00:03, 402MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  77%|███▊ | 3.86G/4.99G [00:25<00:02, 478MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  79%|███▉ | 3.92G/4.99G [00:26<00:02, 376MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  81%|████ | 4.06G/4.99G [00:26<00:02, 461MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  83%|████▏| 4.13G/4.99G [00:26<00:01, 453MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  84%|████▏| 4.19G/4.99G [00:26<00:01, 462MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  87%|████▎| 4.33G/4.99G [00:26<00:01, 559MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  88%|████▍| 4.39G/4.99G [00:26<00:01, 516MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  89%|████▍| 4.46G/4.99G [00:26<00:01, 494MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  91%|████▌| 4.53G/4.99G [00:27<00:00, 509MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  93%|████▋| 4.66G/4.99G [00:27<00:00, 498MB/s]\u001b[A\nmodel-00002-of-00003.safetensors:  96%|████▊| 4.79G/4.99G [00:27<00:00, 624MB/s]\u001b[A\nmodel-00002-of-00003.safetensors: 100%|█████| 4.99G/4.99G [00:27<00:00, 180MB/s]\u001b[A\nFetching 3 files: 100%|███████████████████████████| 3/3 [00:28<00:00,  9.33s/it]\nFetching 3 files: 100%|███████████████████████████| 3/3 [00:28<00:00,  9.34s/it]\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:04<00:00,  1.52s/it]\ngeneration_config.json: 100%|██████████████████| 210/210 [00:00<00:00, 1.68MB/s]\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:05<00:00,  1.81s/it]\nprocessor_config.json: 100%|██████████████████| 98.0/98.0 [00:00<00:00, 629kB/s]\nchat_template.jinja: 1.63kB [00:00, 8.06MB/s]\npreprocessor_config.json: 1.09kB [00:00, 6.18MB/s]\ntokenizer_config.json: 1.20MB [00:00, 147MB/s]\ntokenizer.model: 100%|█████████████████████| 4.70M/4.70M [00:00<00:00, 8.78MB/s]\ntokenizer.json: 100%|██████████████████████| 33.4M/33.4M [00:00<00:00, 64.8MB/s]\nspecial_tokens_map.json: 100%|█████████████████| 777/777 [00:00<00:00, 5.15MB/s]\n[RANK=1] environ({'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.5.3.2-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NV_NVML_DEV_VERSION': '12.5.82-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn9-cuda-12', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.22.3-1+cuda12.5', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.22.3-1', 'VM_GCE_METADATA_HOST': '169.254.169.254', 'HOSTNAME': 'fa495356cf1d', 'LANGUAGE': 'en_US', 'KAGGLE_DATA_PROXY_TOKEN': 'eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.m587TydWsmxVQ20nYhrB12BBGtjHsxcr6jCN_ELy5DPiQWC8giUt6Q.WC_TmcSF_rCGYFoszuKGJg.IlFkoB7aIm5XrtDHVuxV_hvfc60dneyqcJvnKJW8i8Zv_aixxoQNmXsywmgBYSNi9JZGiv7SilqMQDS4BmyXNAd5vrHWx1A71EiOK4MP424OHL3HqHuptaxfNkG1oypYZtUy-lFxtdxVb9DJDt0Ko2397RtR4YNVTMY7cORWJtnlinqfTrEqB5BCqzXRm-qVdWx_L0-gvool0gKb3Xe6zU_CBmQvnFBluv7F-47zYkyPjPJQeZSfKuyFmFipLutdxPvIYxM_Segd7lb7KfPZQWGJbaqL9D1nnSZD9JRn5Mbs81fn06XHuUOtcDsmFrMl.DsuwWhH7sFILdoTv6a-QLg', 'COLAB_TPU_1VM': '', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-12-5=12.5.3.2-1', 'NV_NVTX_VERSION': '12.5.82-1', 'TF_CPP_MIN_LOG_LEVEL': '2', 'COLAB_JUPYTER_IP': '127.0.0.1', 'NV_CUDA_CUDART_DEV_VERSION': '12.5.82-1', 'NV_LIBCUSPARSE_VERSION': '12.5.1.3-1', 'KAGGLE_URL_BASE': 'https://www.kaggle.com', 'NV_LIBNPP_VERSION': '12.3.0.159-1', 'NCCL_VERSION': '2.22.3-1', 'KAGGLE_DOCKER_IMAGE': 'gcr.io/kaggle-gpu-images/python@sha256:320043e14c68293f1c946585b9257123385205a58af4b94b17d31868cae4e868', 'KAGGLE_KERNEL_INTEGRATIONS': '', 'KMP_LISTEN_PORT': '6000', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'COLAB_HUMAN_READABLE_NODE_LOGS': '1', 'ENV': '/root/.bashrc', 'PWD': '/kaggle/working', 'TESSERACT_PATH': '/usr/bin/tesseract', 'NV_CUDNN_PACKAGE': 'libcudnn9-cuda-12=9.2.1.18-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'LAST_FORCED_REBUILD': '20250623', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-12-5=12.5.82-1', 'NV_LIBNPP_PACKAGE': 'libnpp-12-5=12.3.0.159-1', 'BUILD_DATE': '20250701-214057', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20', '_': '/usr/local/bin/python', 'NV_LIBCUBLAS_DEV_VERSION': '12.5.3.2-1', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'UV_BUILD_CONSTRAINT': '', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-12-5', 'NV_CUDA_CUDART_VERSION': '12.5.82-1', 'COLAB_JUPYTER_ALLOW_ORIGIN_PAT': 'https://colab\\\\.(sandbox|research)\\\\.google\\\\.com', 'COLAB_WARMUP_DEFAULTS': '1', 'HOME': '/root', 'LANG': 'en_US.UTF-8', 'CUDA_VERSION': '12.5.1', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-5=12.5.3.2-1', 'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-12-5=12.5.1-1', 'UV_SYSTEM_PYTHON': 'true', 'COLAB_RELEASE_TAG': 'release-colab_20250626-060053_RC00', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'KMP_TARGET_PORT': '9000', 'CLICOLOR': '1', 'UV_INSTALL_DIR': '/usr/local/bin', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-12-5=12.3.0.159-1', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-5', 'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000', 'CLOUDSDK_PYTHON': 'python3', 'NV_LIBNPP_DEV_VERSION': '12.3.0.159-1', 'JPY_PARENT_PID': '1', 'PYTHONPATH': '/kaggle/lib/kagglegym:/kaggle/lib', 'TERM': 'xterm-color', 'NV_LIBCUSPARSE_DEV_VERSION': '12.5.1.3-1', 'KAGGLE_DATA_PROXY_PROJECT': 'kaggle-161607', 'GIT_PAGER': 'cat', 'KAGGLE_USER_SECRETS_TOKEN': 'eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..OJs8xn2cFfADOKVbkA1wIg.0y4wjK7BvWZauU6v_X6NLQwzQorhcm2-BpN7l6LqYtp9PkdXnCXgHouETHFWMmj8IbLEyMdYF3NU8byqDFWFjAjRe6uwgQINF-9X6DXf1cAo9gfkkM6oQ8OzlQ5ilwn12zvZSRLXoIaauolKQSiqbg.XXutV8nyeBumfXz4qYfLhg', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_CUDNN_VERSION': '9.2.1.18-1', 'SHLVL': '0', 'PAGER': 'cat', 'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service', 'NV_CUDA_LIB_VERSION': '12.5.1-1', 'NVARCH': 'x86_64', 'KAGGLE_KERNEL_RUN_TYPE': 'Interactive', 'UV_CONSTRAINT': '', 'PYTHONUTF8': '1', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn9-dev-cuda-12=9.2.1.18-1', 'KAGGLE_DISABLE_GOOGLE_GENERATIVE_AI_INTEGRATION': 'True', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.22.3-1+cuda12.5', 'LD_LIBRARY_PATH': '/usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'KAGGLE_GCP_ZONE': 'us-central1-f', 'MKL_THREADING_LAYER': 'GNU', 'NV_CUDA_NSIGHT_COMPUTE_VERSION': '12.5.1-1', 'GIT_COMMIT': '0eb38ee5b80e4003101469c19d6da1f4353d5960', 'NV_NVPROF_VERSION': '12.5.82-1', 'LC_ALL': 'en_US.UTF-8', '_PYVIZ_COMMS_INSTALLED': '1', 'CUDA_HOME': '/usr/local/cuda', 'COLAB_FILE_HANDLER_ADDR': 'localhost:3453', 'KAGGLE_CONTAINER_NAME': 'kaggle_gVFNF9baTMw6oRYoW6AaLgCS6k5qUbSD2bg77hjY-251918004-webtier', 'PATH': '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'PYTHONUSERBASE': '/root/.local', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer', 'NV_LIBNCCL_PACKAGE_VERSION': '2.22.3-1', 'KAGGLE_API_V1_TOKEN': '/etc/secrets/kaggle/api-v1-token', 'DEBIAN_FRONTEND': 'noninteractive', 'KAGGLE_GRPC_DATA_PROXY_URL': 'dp.kaggle.net:443', 'KAGGLE_DATA_PROXY_URL': 'https://dp.kaggle.net', 'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True,roundup_power2_divisions:[32:256,64:128,256:64,>:32]', 'WANDB_PROJECT': 'open-maize-vision1', 'WANDB_NAME': 'vision_multiGPU_globalbz16_epochs18', 'OPENSLOTH_WORLD_SIZE': '2', 'OPENSLOTH_FORWARD_BZ': '2', 'OPENSLOTH_GLOBAL_BZ': '16', 'OPENSLOTH_ACCUMULATION_STEPS': '8', 'OPENSLOTH_PER_DEVICE_TRAIN_BZ': '1', 'OPENSLOTH_OUTPUT_DIR': 'outputs/vision_multiGPU_experiment', 'OPENSLOTH_LOG_LEVEL': 'info', 'CUDA_VISIBLE_DEVICES': '1', 'CUDA_MODULE_LOADING': 'LAZY', 'ENABLE_RUNTIME_UPTIME_TELEMETRY': '1', 'TF2_BEHAVIOR': '1', 'TPU_ML_PLATFORM': 'Tensorflow', 'TPU_ML_PLATFORM_VERSION': '2.18.0', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'TF_USE_LEGACY_KERAS': '1', 'OPENSLOTH_LOCAL_RANK': '1', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'python', 'UNSLOTH_IS_PRESENT': '1', 'HF_HUB_ENABLE_HF_TRANSFER': '1', 'HF_XET_HIGH_PERFORMANCE': '1', 'HF_XET_CHUNK_CACHE_SIZE_BYTES': '0', 'HF_XET_RECONSTRUCT_WRITE_SEQUENTIALLY': '0', 'HF_XET_NUM_CONCURRENT_RANGE_GETS': '64', 'TRITON_DISABLE_LINE_INFO': '1', 'TRITON_FRONT_END_DEBUGGING': '0', 'UNSLOTH_ZOO_IS_PRESENT': '1', 'UNSLOTH_PATCHED': '1', 'TORCHINDUCTOR_FX_GRAPH_CACHE': '1', 'TORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE': '1', 'ENABLE_AOT_AUTOGRAD_CACHE': '1', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'UNSLOTH_IGNORED_TOKENIZER_NAMES': 'unsloth/qwen2.5-coder-7b-instruct\\nunsloth/qwen2.5-coder-7b-instruct-bnb-4bit\\nunsloth/qwen2.5-coder-1.5b-instruct-bnb-4bit\\nunsloth/qwen2.5-coder-1.5b-instruct', 'UNSLOTH_DISABLE_STATIC_GENERATION': '1', 'UNSLOTH_FORCE_CUSTOM_DTYPE': \"float16;torch.float16;torch.float16;if name.endswith(('.conv')): module;from unsloth_zoo.temporary_patches.gemma3n import patch_Gemma3nConvNormAct_forward; patch_Gemma3nConvNormAct_forward()\", 'UNSLOTH_FORCE_FLOAT32': '0', 'UNSLOTH_RETURN_LOGITS': '0', 'UNSLOTH_FULLGRAPH': '1', 'UNSLOTH_USE_NEW_MODEL': '1', 'UNSLOTH_ENABLE_FULL_FINETUNING': '0', 'UNSLOTH_MIXED_PRECISION': 'float32', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29501'})\n\u001b[32m21:03:18\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_loading: 53.82s\u001b[0m\n[RANK=0] environ({'SHELL': '/bin/bash', 'NV_LIBCUBLAS_VERSION': '12.5.3.2-1', 'NVIDIA_VISIBLE_DEVICES': 'all', 'NV_NVML_DEV_VERSION': '12.5.82-1', 'NV_CUDNN_PACKAGE_NAME': 'libcudnn9-cuda-12', 'NV_LIBNCCL_DEV_PACKAGE': 'libnccl-dev=2.22.3-1+cuda12.5', 'NV_LIBNCCL_DEV_PACKAGE_VERSION': '2.22.3-1', 'VM_GCE_METADATA_HOST': '169.254.169.254', 'HOSTNAME': 'fa495356cf1d', 'LANGUAGE': 'en_US', 'KAGGLE_DATA_PROXY_TOKEN': 'eyJhbGciOiJBMTI4S1ciLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0.m587TydWsmxVQ20nYhrB12BBGtjHsxcr6jCN_ELy5DPiQWC8giUt6Q.WC_TmcSF_rCGYFoszuKGJg.IlFkoB7aIm5XrtDHVuxV_hvfc60dneyqcJvnKJW8i8Zv_aixxoQNmXsywmgBYSNi9JZGiv7SilqMQDS4BmyXNAd5vrHWx1A71EiOK4MP424OHL3HqHuptaxfNkG1oypYZtUy-lFxtdxVb9DJDt0Ko2397RtR4YNVTMY7cORWJtnlinqfTrEqB5BCqzXRm-qVdWx_L0-gvool0gKb3Xe6zU_CBmQvnFBluv7F-47zYkyPjPJQeZSfKuyFmFipLutdxPvIYxM_Segd7lb7KfPZQWGJbaqL9D1nnSZD9JRn5Mbs81fn06XHuUOtcDsmFrMl.DsuwWhH7sFILdoTv6a-QLg', 'COLAB_TPU_1VM': '', 'NVIDIA_REQUIRE_CUDA': 'cuda>=12.5 brand=unknown,driver>=470,driver<471 brand=grid,driver>=470,driver<471 brand=tesla,driver>=470,driver<471 brand=nvidia,driver>=470,driver<471 brand=quadro,driver>=470,driver<471 brand=quadrortx,driver>=470,driver<471 brand=nvidiartx,driver>=470,driver<471 brand=vapps,driver>=470,driver<471 brand=vpc,driver>=470,driver<471 brand=vcs,driver>=470,driver<471 brand=vws,driver>=470,driver<471 brand=cloudgaming,driver>=470,driver<471 brand=unknown,driver>=535,driver<536 brand=grid,driver>=535,driver<536 brand=tesla,driver>=535,driver<536 brand=nvidia,driver>=535,driver<536 brand=quadro,driver>=535,driver<536 brand=quadrortx,driver>=535,driver<536 brand=nvidiartx,driver>=535,driver<536 brand=vapps,driver>=535,driver<536 brand=vpc,driver>=535,driver<536 brand=vcs,driver>=535,driver<536 brand=vws,driver>=535,driver<536 brand=cloudgaming,driver>=535,driver<536 brand=unknown,driver>=550,driver<551 brand=grid,driver>=550,driver<551 brand=tesla,driver>=550,driver<551 brand=nvidia,driver>=550,driver<551 brand=quadro,driver>=550,driver<551 brand=quadrortx,driver>=550,driver<551 brand=nvidiartx,driver>=550,driver<551 brand=vapps,driver>=550,driver<551 brand=vpc,driver>=550,driver<551 brand=vcs,driver>=550,driver<551 brand=vws,driver>=550,driver<551 brand=cloudgaming,driver>=550,driver<551', 'NV_LIBCUBLAS_DEV_PACKAGE': 'libcublas-dev-12-5=12.5.3.2-1', 'NV_NVTX_VERSION': '12.5.82-1', 'TF_CPP_MIN_LOG_LEVEL': '2', 'COLAB_JUPYTER_IP': '127.0.0.1', 'NV_CUDA_CUDART_DEV_VERSION': '12.5.82-1', 'NV_LIBCUSPARSE_VERSION': '12.5.1.3-1', 'KAGGLE_URL_BASE': 'https://www.kaggle.com', 'NV_LIBNPP_VERSION': '12.3.0.159-1', 'NCCL_VERSION': '2.22.3-1', 'KAGGLE_DOCKER_IMAGE': 'gcr.io/kaggle-gpu-images/python@sha256:320043e14c68293f1c946585b9257123385205a58af4b94b17d31868cae4e868', 'KAGGLE_KERNEL_INTEGRATIONS': '', 'KMP_LISTEN_PORT': '6000', 'TF_FORCE_GPU_ALLOW_GROWTH': 'true', 'COLAB_HUMAN_READABLE_NODE_LOGS': '1', 'ENV': '/root/.bashrc', 'PWD': '/kaggle/working', 'TESSERACT_PATH': '/usr/bin/tesseract', 'NV_CUDNN_PACKAGE': 'libcudnn9-cuda-12=9.2.1.18-1', 'NVIDIA_DRIVER_CAPABILITIES': 'compute,utility', 'LAST_FORCED_REBUILD': '20250623', 'NV_NVPROF_DEV_PACKAGE': 'cuda-nvprof-12-5=12.5.82-1', 'NV_LIBNPP_PACKAGE': 'libnpp-12-5=12.3.0.159-1', 'BUILD_DATE': '20250701-214057', 'NV_LIBNCCL_DEV_PACKAGE_NAME': 'libnccl-dev', 'TCLLIBPATH': '/usr/share/tcltk/tcllib1.20', '_': '/usr/local/bin/python', 'NV_LIBCUBLAS_DEV_VERSION': '12.5.3.2-1', 'NVIDIA_PRODUCT_NAME': 'CUDA', 'UV_BUILD_CONSTRAINT': '', 'NV_LIBCUBLAS_DEV_PACKAGE_NAME': 'libcublas-dev-12-5', 'NV_CUDA_CUDART_VERSION': '12.5.82-1', 'COLAB_JUPYTER_ALLOW_ORIGIN_PAT': 'https://colab\\\\.(sandbox|research)\\\\.google\\\\.com', 'COLAB_WARMUP_DEFAULTS': '1', 'HOME': '/root', 'LANG': 'en_US.UTF-8', 'CUDA_VERSION': '12.5.1', 'CLOUDSDK_CONFIG': '/content/.config', 'NV_LIBCUBLAS_PACKAGE': 'libcublas-12-5=12.5.3.2-1', 'NV_CUDA_NSIGHT_COMPUTE_DEV_PACKAGE': 'cuda-nsight-compute-12-5=12.5.1-1', 'UV_SYSTEM_PYTHON': 'true', 'COLAB_RELEASE_TAG': 'release-colab_20250626-060053_RC00', 'PYDEVD_USE_FRAME_EVAL': 'NO', 'KMP_TARGET_PORT': '9000', 'CLICOLOR': '1', 'UV_INSTALL_DIR': '/usr/local/bin', 'NV_LIBNPP_DEV_PACKAGE': 'libnpp-dev-12-5=12.3.0.159-1', 'NV_LIBCUBLAS_PACKAGE_NAME': 'libcublas-12-5', 'COLAB_KERNEL_MANAGER_PROXY_PORT': '6000', 'CLOUDSDK_PYTHON': 'python3', 'NV_LIBNPP_DEV_VERSION': '12.3.0.159-1', 'JPY_PARENT_PID': '1', 'PYTHONPATH': '/kaggle/lib/kagglegym:/kaggle/lib', 'TERM': 'xterm-color', 'NV_LIBCUSPARSE_DEV_VERSION': '12.5.1.3-1', 'KAGGLE_DATA_PROXY_PROJECT': 'kaggle-161607', 'GIT_PAGER': 'cat', 'KAGGLE_USER_SECRETS_TOKEN': 'eyJhbGciOiJkaXIiLCJlbmMiOiJBMTI4Q0JDLUhTMjU2In0..OJs8xn2cFfADOKVbkA1wIg.0y4wjK7BvWZauU6v_X6NLQwzQorhcm2-BpN7l6LqYtp9PkdXnCXgHouETHFWMmj8IbLEyMdYF3NU8byqDFWFjAjRe6uwgQINF-9X6DXf1cAo9gfkkM6oQ8OzlQ5ilwn12zvZSRLXoIaauolKQSiqbg.XXutV8nyeBumfXz4qYfLhg', 'LIBRARY_PATH': '/usr/local/cuda/lib64/stubs', 'NV_CUDNN_VERSION': '9.2.1.18-1', 'SHLVL': '0', 'PAGER': 'cat', 'COLAB_LANGUAGE_SERVER_PROXY': '/usr/colab/bin/language_service', 'NV_CUDA_LIB_VERSION': '12.5.1-1', 'NVARCH': 'x86_64', 'KAGGLE_KERNEL_RUN_TYPE': 'Interactive', 'UV_CONSTRAINT': '', 'PYTHONUTF8': '1', 'NV_CUDNN_PACKAGE_DEV': 'libcudnn9-dev-cuda-12=9.2.1.18-1', 'KAGGLE_DISABLE_GOOGLE_GENERATIVE_AI_INTEGRATION': 'True', 'MPLBACKEND': 'module://matplotlib_inline.backend_inline', 'NV_LIBNCCL_PACKAGE': 'libnccl2=2.22.3-1+cuda12.5', 'LD_LIBRARY_PATH': '/usr/local/lib/python3.11/dist-packages/cv2/../../lib64:/usr/local/nvidia/lib:/usr/local/nvidia/lib64', 'KAGGLE_GCP_ZONE': 'us-central1-f', 'MKL_THREADING_LAYER': 'GNU', 'NV_CUDA_NSIGHT_COMPUTE_VERSION': '12.5.1-1', 'GIT_COMMIT': '0eb38ee5b80e4003101469c19d6da1f4353d5960', 'NV_NVPROF_VERSION': '12.5.82-1', 'LC_ALL': 'en_US.UTF-8', '_PYVIZ_COMMS_INSTALLED': '1', 'CUDA_HOME': '/usr/local/cuda', 'COLAB_FILE_HANDLER_ADDR': 'localhost:3453', 'KAGGLE_CONTAINER_NAME': 'kaggle_gVFNF9baTMw6oRYoW6AaLgCS6k5qUbSD2bg77hjY-251918004-webtier', 'PATH': '/opt/bin:/usr/local/nvidia/bin:/usr/local/cuda/bin:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/tools/node/bin:/tools/google-cloud-sdk/bin', 'PYTHONUSERBASE': '/root/.local', 'NV_LIBNCCL_PACKAGE_NAME': 'libnccl2', 'COLAB_DEBUG_ADAPTER_MUX_PATH': '/usr/local/bin/dap_multiplexer', 'NV_LIBNCCL_PACKAGE_VERSION': '2.22.3-1', 'KAGGLE_API_V1_TOKEN': '/etc/secrets/kaggle/api-v1-token', 'DEBIAN_FRONTEND': 'noninteractive', 'KAGGLE_GRPC_DATA_PROXY_URL': 'dp.kaggle.net:443', 'KAGGLE_DATA_PROXY_URL': 'https://dp.kaggle.net', 'PYTORCH_CUDA_ALLOC_CONF': 'expandable_segments:True,roundup_power2_divisions:[32:256,64:128,256:64,>:32]', 'WANDB_PROJECT': 'open-maize-vision1', 'WANDB_NAME': 'vision_multiGPU_globalbz16_epochs18', 'OPENSLOTH_WORLD_SIZE': '2', 'OPENSLOTH_FORWARD_BZ': '2', 'OPENSLOTH_GLOBAL_BZ': '16', 'OPENSLOTH_ACCUMULATION_STEPS': '8', 'OPENSLOTH_PER_DEVICE_TRAIN_BZ': '1', 'OPENSLOTH_OUTPUT_DIR': 'outputs/vision_multiGPU_experiment', 'OPENSLOTH_LOG_LEVEL': 'info', 'CUDA_VISIBLE_DEVICES': '0', 'CUDA_MODULE_LOADING': 'LAZY', 'ENABLE_RUNTIME_UPTIME_TELEMETRY': '1', 'TF2_BEHAVIOR': '1', 'TPU_ML_PLATFORM': 'Tensorflow', 'TPU_ML_PLATFORM_VERSION': '2.18.0', 'KMP_DUPLICATE_LIB_OK': 'True', 'KMP_INIT_AT_FORK': 'FALSE', 'TF_USE_LEGACY_KERAS': '1', 'OPENSLOTH_LOCAL_RANK': '0', 'PROTOCOL_BUFFERS_PYTHON_IMPLEMENTATION': 'python', 'UNSLOTH_IS_PRESENT': '1', 'HF_HUB_ENABLE_HF_TRANSFER': '1', 'HF_XET_HIGH_PERFORMANCE': '1', 'HF_XET_CHUNK_CACHE_SIZE_BYTES': '0', 'HF_XET_RECONSTRUCT_WRITE_SEQUENTIALLY': '0', 'HF_XET_NUM_CONCURRENT_RANGE_GETS': '64', 'TRITON_DISABLE_LINE_INFO': '1', 'TRITON_FRONT_END_DEBUGGING': '0', 'UNSLOTH_ZOO_IS_PRESENT': '1', 'UNSLOTH_PATCHED': '1', 'TORCHINDUCTOR_FX_GRAPH_CACHE': '1', 'TORCHINDUCTOR_AUTOTUNE_REMOTE_CACHE': '1', 'ENABLE_AOT_AUTOGRAD_CACHE': '1', 'TORCHINDUCTOR_CACHE_DIR': '/tmp/torchinductor_root', 'UNSLOTH_IGNORED_TOKENIZER_NAMES': 'unsloth/qwen2.5-coder-7b-instruct\\nunsloth/qwen2.5-coder-7b-instruct-bnb-4bit\\nunsloth/qwen2.5-coder-1.5b-instruct\\nunsloth/qwen2.5-coder-1.5b-instruct-bnb-4bit', 'UNSLOTH_DISABLE_STATIC_GENERATION': '1', 'UNSLOTH_FORCE_CUSTOM_DTYPE': \"float16;torch.float16;torch.float16;if name.endswith(('.conv')): module;from unsloth_zoo.temporary_patches.gemma3n import patch_Gemma3nConvNormAct_forward; patch_Gemma3nConvNormAct_forward()\", 'UNSLOTH_FORCE_FLOAT32': '0', 'UNSLOTH_RETURN_LOGITS': '0', 'UNSLOTH_FULLGRAPH': '1', 'UNSLOTH_USE_NEW_MODEL': '1', 'UNSLOTH_ENABLE_FULL_FINETUNING': '0', 'UNSLOTH_MIXED_PRECISION': 'float32', 'MASTER_ADDR': '127.0.0.1', 'MASTER_PORT': '29501'})\n\u001b[32m21:03:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:70\u001b[0m | \u001b[1mModel loaded on device cuda:0, tokenizer: Gemma3nProcessor\u001b[0m\nUnsloth: Making `model.base_model.model.model.language_model` require gradients\nUnsloth: Making `model.base_model.model.model.language_model` require gradients\n\u001b[32m21:03:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  lora_setup: 5.12s\u001b[0m\n\u001b[32m21:03:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_init: 59.31s\u001b[0m\n\u001b[32m21:03:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:195\u001b[0m | \u001b[1mLoading dataset from data/cached_vision_dataset_hf\u001b[0m\n\u001b[32m21:03:24\u001b[0m | \u001b[33m\u001b[1mWARNING \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:172\u001b[0m | \u001b[33m\u001b[1mDataset does not have 'labels' feature. This may affect training. Please check your dataset preparation.\u001b[0m\n\u001b[32m21:03:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:219\u001b[0m | \u001b[1mTrainer setup completed successfully\u001b[0m\n[LOCAL_RANK=0] Patching log. Dir: outputs/vision_multiGPU_experiment, GPUs: 2\n[LOCAL_RANK=0] Log patch initialization complete.\n🔧 Patching Trainer to use RandomSamplerSeededByEpoch\n\u001b[32m21:03:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36minit_modules.py:151\u001b[0m | \u001b[1mAdd callback ShuffleData to Trainer UnslothSFTTrainer\u001b[0m\n\u001b[32m21:03:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  total_setup: 1.2m\u001b[0m\n\u001b[32m21:03:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  model_and_training_setup: 1.2m\u001b[0m\n\u001b[32m21:03:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mopensloth_sft_trainer.py:68\u001b[0m | \u001b[1mUsing gradient sync callback for GPU 0\u001b[0m\n[LOCAL_RANK=1] Patching log. Dir: outputs/vision_multiGPU_experiment, GPUs: 2\n[LOCAL_RANK=1] Log patch initialization complete.\n🔧 Patching Trainer to use RandomSamplerSeededByEpoch\n[RANK=1] Patching trainer.save_model, trainer._save, and trainer._maybe_log_save_evaluate to no-op on non-master rank.\n\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The `run_name` is currently set to the same value as `TrainingArguments.output_dir`. If this was not intended, please specify a different run name by setting the `TrainingArguments.run_name` parameter.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mjdmasciano2\u001b[0m (\u001b[33mjdmasciano2-university-of-lagos\u001b[0m) to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n  0%|                                                   | 0/198 [00:00<?, ?it/s]\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.20.1\n\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/kaggle/working/wandb/run-20250722_210328-9lh3iajg\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33moutputs/vision_multiGPU_experiment\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/jdmasciano2-university-of-lagos/open-maize-vision1\u001b[0m\n\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/jdmasciano2-university-of-lagos/open-maize-vision1/runs/9lh3iajg\u001b[0m\n  0%|                                                   | 0/198 [00:00<?, ?it/s]\u001b[32m21:03:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 1\u001b[0m\n\u001b[32m21:03:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 0: emitting 176 indices\nFirst ids dataset samples: [148, 36, 15, 124, 13, 155, 128, 121, 103, 19]\n...Last ids: [22, 139, 26, 35, 57, 62, 70, 6, 28, 163]\u001b[0m\n\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:03:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:03:52\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 0: emitting 176 indices\nFirst ids dataset samples: [148, 36, 15, 124, 13, 155, 128, 121, 103, 19]\n...Last ids: [22, 139, 26, 35, 57, 62, 70, 6, 28, 163]\u001b[0m\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n`use_cache=True` is incompatible with gradient checkpointing. Setting `use_cache=False`.\n[Step 0] Effective Tokens: 0.0M/0.0M\n[Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 13.19729995727539, 'grad_norm': nan, 'learning_rate': 9.792746113989638e-05, 'epoch': 0.91}\n  5%|██                                      | 10/198 [05:40<1:24:34, 26.99s/it]\u001b[32m21:09:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 0: dataset_size=176\n   📋 First 10 indices: [148, 36, 15, 124, 13, 155, 128, 121, 103, 19]\n   📋 Last 10 indices: [22, 139, 26, 35, 57, 62, 70, 6, 28, 163]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n  6%|██▏                                     | 11/198 [06:03<1:20:57, 25.98s/it]\u001b[32m21:09:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 2.0\u001b[0m\n\u001b[32m21:09:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 1: emitting 176 indices\nFirst ids dataset samples: [44, 83, 28, 8, 168, 65, 109, 97, 115, 154]\n...Last ids: [155, 127, 153, 116, 24, 94, 118, 36, 73, 9]\u001b[0m\n  6%|██▏                                     | 11/198 [06:05<1:20:52, 25.95s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:09:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:09:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 1: emitting 176 indices\nFirst ids dataset samples: [44, 83, 28, 8, 168, 65, 109, 97, 115, 154]\n...Last ids: [155, 127, 153, 116, 24, 94, 118, 36, 73, 9]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n  6%|██▍                                     | 12/198 [06:32<1:21:37, 26.33s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 6.728400230407715, 'grad_norm': 432.2044372558594, 'learning_rate': 9.27461139896373e-05, 'epoch': 1.82}\n 11%|████▏                                   | 21/198 [10:43<1:16:41, 26.00s/it]\u001b[32m21:14:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 1: dataset_size=176\n   📋 First 10 indices: [44, 83, 28, 8, 168, 65, 109, 97, 115, 154]\n   📋 Last 10 indices: [155, 127, 153, 116, 24, 94, 118, 36, 73, 9]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 11%|████▍                                   | 22/198 [11:05<1:13:40, 25.12s/it]\u001b[32m21:14:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 3.0\u001b[0m\n\u001b[32m21:14:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 2: emitting 176 indices\nFirst ids dataset samples: [23, 13, 67, 100, 10, 173, 76, 65, 95, 112]\n...Last ids: [169, 7, 74, 57, 97, 45, 29, 138, 133, 104]\u001b[0m\n 11%|████▍                                   | 22/198 [11:07<1:14:15, 25.31s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Maize Phosphorus Deficiency.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:14:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:14:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 2: emitting 176 indices\nFirst ids dataset samples: [23, 13, 67, 100, 10, 173, 76, 65, 95, 112]\n...Last ids: [169, 7, 74, 57, 97, 45, 29, 138, 133, 104]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 12%|████▋                                   | 23/198 [11:35<1:15:43, 25.96s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.3950999975204468, 'grad_norm': 14.286477088928223, 'learning_rate': 8.756476683937824e-05, 'epoch': 2.73}\n 16%|██████▍                                 | 32/198 [15:44<1:11:50, 25.97s/it]\u001b[32m21:19:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 2: dataset_size=176\n   📋 First 10 indices: [23, 13, 67, 100, 10, 173, 76, 65, 95, 112]\n   📋 Last 10 indices: [169, 7, 74, 57, 97, 45, 29, 138, 133, 104]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 17%|██████▋                                 | 33/198 [16:06<1:09:40, 25.34s/it]\u001b[32m21:19:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 4.0\u001b[0m\n 17%|██████▋                                 | 33/198 [16:08<1:09:26, 25.25s/it]\u001b[32m21:19:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 3: emitting 176 indices\nFirst ids dataset samples: [52, 85, 60, 133, 47, 112, 90, 128, 55, 160]\n...Last ids: [123, 18, 5, 86, 77, 20, 65, 124, 106, 69]\u001b[0m\n\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:19:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:19:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 3: emitting 176 indices\nFirst ids dataset samples: [52, 85, 60, 133, 47, 112, 90, 128, 55, 160]\n...Last ids: [123, 18, 5, 86, 77, 20, 65, 124, 106, 69]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 17%|██████▊                                 | 34/198 [16:36<1:11:14, 26.07s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.27469998598098755, 'grad_norm': 6.821955680847168, 'learning_rate': 8.238341968911918e-05, 'epoch': 3.64}\n 22%|████████▋                               | 43/198 [20:48<1:07:39, 26.19s/it]\u001b[32m21:24:30\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 3: dataset_size=176\n   📋 First 10 indices: [52, 85, 60, 133, 47, 112, 90, 128, 55, 160]\n   📋 Last 10 indices: [123, 18, 5, 86, 77, 20, 65, 124, 106, 69]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 22%|████████▉                               | 44/198 [21:09<1:04:55, 25.30s/it]\u001b[32m21:24:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 5.0\u001b[0m\n\u001b[32m21:24:39\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 4: emitting 176 indices\nFirst ids dataset samples: [108, 170, 123, 55, 11, 61, 82, 107, 5, 152]\n...Last ids: [149, 38, 132, 162, 58, 172, 150, 10, 102, 19]\u001b[0m\n 22%|████████▉                               | 44/198 [21:11<1:05:06, 25.37s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:25:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:25:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 4: emitting 176 indices\nFirst ids dataset samples: [108, 170, 123, 55, 11, 61, 82, 107, 5, 152]\n...Last ids: [149, 38, 132, 162, 58, 172, 150, 10, 102, 19]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 23%|█████████                               | 45/198 [21:39<1:06:08, 25.94s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.2768999934196472, 'grad_norm': 0.5916773080825806, 'learning_rate': 7.72020725388601e-05, 'epoch': 4.55}\n 27%|██████████▉                             | 54/198 [25:49<1:02:24, 26.00s/it]\u001b[32m21:29:33\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 4: dataset_size=176\n   📋 First 10 indices: [108, 170, 123, 55, 11, 61, 82, 107, 5, 152]\n   📋 Last 10 indices: [149, 38, 132, 162, 58, 172, 150, 10, 102, 19]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 28%|███████████                             | 55/198 [26:12<1:00:34, 25.42s/it]\u001b[32m21:29:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 6.0\u001b[0m\n\u001b[32m21:29:42\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 5: emitting 176 indices\nFirst ids dataset samples: [47, 76, 147, 118, 13, 20, 162, 88, 117, 127]\n...Last ids: [98, 131, 65, 87, 146, 116, 141, 110, 16, 90]\u001b[0m\n\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:30:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:30:04\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 5: emitting 176 indices\nFirst ids dataset samples: [47, 76, 147, 118, 13, 20, 162, 88, 117, 127]\n...Last ids: [98, 131, 65, 87, 146, 116, 141, 110, 16, 90]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 28%|███████████▎                            | 56/198 [26:40<1:01:00, 25.78s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.27079999446868896, 'grad_norm': 0.7690781354904175, 'learning_rate': 7.202072538860104e-05, 'epoch': 5.45}\n 33%|█████████████▊                            | 65/198 [30:53<57:48, 26.08s/it]\u001b[32m21:34:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 5: dataset_size=176\n   📋 First 10 indices: [47, 76, 147, 118, 13, 20, 162, 88, 117, 127]\n   📋 Last 10 indices: [98, 131, 65, 87, 146, 116, 141, 110, 16, 90]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 33%|██████████████                            | 66/198 [31:14<55:00, 25.00s/it]\u001b[32m21:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 7.0\u001b[0m\n\u001b[32m21:34:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 6: emitting 176 indices\nFirst ids dataset samples: [45, 79, 145, 78, 8, 151, 11, 46, 157, 26]\n...Last ids: [42, 111, 49, 129, 77, 136, 142, 33, 80, 140]\u001b[0m\n 33%|██████████████                            | 66/198 [31:16<55:28, 25.22s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:35:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:35:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 6: emitting 176 indices\nFirst ids dataset samples: [45, 79, 145, 78, 8, 151, 11, 46, 157, 26]\n...Last ids: [42, 111, 49, 129, 77, 136, 142, 33, 80, 140]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 34%|██████████████▏                           | 67/198 [31:44<57:03, 26.13s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.26930001378059387, 'grad_norm': 0.9503353834152222, 'learning_rate': 6.683937823834198e-05, 'epoch': 6.36}\n 38%|████████████████                          | 76/198 [35:53<52:38, 25.89s/it]\u001b[32m21:39:35\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 6: dataset_size=176\n   📋 First 10 indices: [45, 79, 145, 78, 8, 151, 11, 46, 157, 26]\n   📋 Last 10 indices: [42, 111, 49, 129, 77, 136, 142, 33, 80, 140]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 39%|████████████████▎                         | 77/198 [36:14<50:25, 25.01s/it]\u001b[32m21:39:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 8.0\u001b[0m\n\u001b[32m21:39:44\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 7: emitting 176 indices\nFirst ids dataset samples: [26, 18, 134, 168, 5, 48, 155, 172, 171, 113]\n...Last ids: [70, 11, 12, 130, 141, 82, 28, 105, 88, 17]\u001b[0m\n 39%|████████████████▎                         | 77/198 [36:16<50:40, 25.13s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Maize Phosphorus Deficiency.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:40:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:40:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 7: emitting 176 indices\nFirst ids dataset samples: [26, 18, 134, 168, 5, 48, 155, 172, 171, 113]\n...Last ids: [70, 11, 12, 130, 141, 82, 28, 105, 88, 17]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 39%|████████████████▌                         | 78/198 [36:44<51:34, 25.79s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.26819998025894165, 'grad_norm': 0.42340952157974243, 'learning_rate': 6.16580310880829e-05, 'epoch': 7.27}\n 44%|██████████████████▍                       | 87/198 [40:52<47:40, 25.77s/it]\u001b[32m21:44:36\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 7: dataset_size=176\n   📋 First 10 indices: [26, 18, 134, 168, 5, 48, 155, 172, 171, 113]\n   📋 Last 10 indices: [70, 11, 12, 130, 141, 82, 28, 105, 88, 17]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 44%|██████████████████▋                       | 88/198 [41:15<45:46, 24.97s/it]\u001b[32m21:44:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 9.0\u001b[0m\n\u001b[32m21:44:45\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 8: emitting 176 indices\nFirst ids dataset samples: [91, 128, 134, 120, 147, 165, 24, 18, 34, 29]\n...Last ids: [81, 137, 21, 84, 121, 62, 163, 93, 68, 127]\u001b[0m\n 44%|██████████████████▋                       | 88/198 [41:18<46:08, 25.17s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:45:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:45:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 8: emitting 176 indices\nFirst ids dataset samples: [91, 128, 134, 120, 147, 165, 24, 18, 34, 29]\n...Last ids: [81, 137, 21, 84, 121, 62, 163, 93, 68, 127]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 45%|██████████████████▉                       | 89/198 [41:45<47:01, 25.88s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.2678999900817871, 'grad_norm': 0.5242642164230347, 'learning_rate': 5.6476683937823835e-05, 'epoch': 8.18}\n 49%|████████████████████▊                     | 98/198 [45:58<43:43, 26.23s/it]\u001b[32m21:49:40\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 8: dataset_size=176\n   📋 First 10 indices: [91, 128, 134, 120, 147, 165, 24, 18, 34, 29]\n   📋 Last 10 indices: [81, 137, 21, 84, 121, 62, 163, 93, 68, 127]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 50%|█████████████████████                     | 99/198 [46:19<41:55, 25.41s/it]\u001b[32m21:49:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 10.0\u001b[0m\n\u001b[32m21:49:49\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 9: emitting 176 indices\nFirst ids dataset samples: [33, 117, 142, 2, 78, 9, 36, 109, 91, 34]\n...Last ids: [140, 101, 65, 173, 59, 175, 41, 141, 128, 62]\u001b[0m\n 50%|█████████████████████                     | 99/198 [46:21<41:54, 25.40s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Maize Phosphorus Deficiency.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:50:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:50:11\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 9: emitting 176 indices\nFirst ids dataset samples: [33, 117, 142, 2, 78, 9, 36, 109, 91, 34]\n...Last ids: [140, 101, 65, 173, 59, 175, 41, 141, 128, 62]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 51%|████████████████████▋                    | 100/198 [46:48<42:22, 25.94s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.2685999870300293, 'grad_norm': 1.7443647384643555, 'learning_rate': 5.129533678756477e-05, 'epoch': 9.09}\n 51%|████████████████████▋                    | 100/198 [47:08<53:10, 32.56s/it]\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m The get_url method is deprecated and will be removed in a future release. Please use `run.url` instead.\n 55%|██████████████████████▌                  | 109/198 [51:05<38:33, 26.00s/it]\u001b[32m21:54:48\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 9: dataset_size=176\n   📋 First 10 indices: [33, 117, 142, 2, 78, 9, 36, 109, 91, 34]\n   📋 Last 10 indices: [140, 101, 65, 173, 59, 175, 41, 141, 128, 62]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n{'loss': 0.2671999931335449, 'grad_norm': 0.8728222250938416, 'learning_rate': 4.61139896373057e-05, 'epoch': 10.0}\n 56%|██████████████████████▊                  | 110/198 [51:27<37:01, 25.24s/it]\u001b[32m21:54:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 11.0\u001b[0m\n\u001b[32m21:54:57\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 10: emitting 176 indices\nFirst ids dataset samples: [65, 114, 6, 171, 42, 24, 89, 152, 34, 4]\n...Last ids: [105, 40, 35, 8, 104, 94, 123, 131, 13, 68]\u001b[0m\n\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m21:55:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m21:55:19\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 10: emitting 176 indices\nFirst ids dataset samples: [65, 114, 6, 171, 42, 24, 89, 152, 34, 4]\n...Last ids: [105, 40, 35, 8, 104, 94, 123, 131, 13, 68]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 56%|██████████████████████▉                  | 111/198 [51:56<37:38, 25.96s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.2678999900817871, 'grad_norm': 0.3736442029476166, 'learning_rate': 4.093264248704664e-05, 'epoch': 10.91}\n 61%|████████████████████████▊                | 120/198 [56:07<33:49, 26.02s/it]\u001b[32m21:59:50\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 10: dataset_size=176\n   📋 First 10 indices: [65, 114, 6, 171, 42, 24, 89, 152, 34, 4]\n   📋 Last 10 indices: [105, 40, 35, 8, 104, 94, 123, 131, 13, 68]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 61%|█████████████████████████                | 121/198 [56:30<32:23, 25.25s/it]\u001b[32m22:00:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 12.0\u001b[0m\n\u001b[32m22:00:00\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 11: emitting 176 indices\nFirst ids dataset samples: [160, 101, 147, 134, 158, 78, 2, 170, 163, 131]\n...Last ids: [9, 7, 173, 92, 133, 123, 128, 116, 55, 157]\u001b[0m\n 61%|█████████████████████████                | 121/198 [56:32<32:26, 25.28s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m22:00:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m22:00:22\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 11: emitting 176 indices\nFirst ids dataset samples: [160, 101, 147, 134, 158, 78, 2, 170, 163, 131]\n...Last ids: [9, 7, 173, 92, 133, 123, 128, 116, 55, 157]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 62%|█████████████████████████▎               | 122/198 [56:59<32:41, 25.81s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.2685000002384186, 'grad_norm': 0.2556520700454712, 'learning_rate': 3.575129533678757e-05, 'epoch': 11.82}\n 66%|█████████████████████████▊             | 131/198 [1:01:09<28:48, 25.80s/it]\u001b[32m22:04:53\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 11: dataset_size=176\n   📋 First 10 indices: [160, 101, 147, 134, 158, 78, 2, 170, 163, 131]\n   📋 Last 10 indices: [9, 7, 173, 92, 133, 123, 128, 116, 55, 157]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 67%|██████████████████████████             | 132/198 [1:01:32<27:29, 25.00s/it]\u001b[32m22:05:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 13.0\u001b[0m\n\u001b[32m22:05:02\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 12: emitting 176 indices\nFirst ids dataset samples: [47, 4, 28, 41, 165, 108, 85, 38, 72, 103]\n...Last ids: [84, 114, 56, 97, 124, 123, 76, 142, 112, 35]\u001b[0m\n 67%|██████████████████████████             | 132/198 [1:01:34<27:51, 25.33s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m22:05:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m22:05:24\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 12: emitting 176 indices\nFirst ids dataset samples: [47, 4, 28, 41, 165, 108, 85, 38, 72, 103]\n...Last ids: [84, 114, 56, 97, 124, 123, 76, 142, 112, 35]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 67%|██████████████████████████▏            | 133/198 [1:02:02<28:09, 26.00s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.26759999990463257, 'grad_norm': 0.24820248782634735, 'learning_rate': 3.05699481865285e-05, 'epoch': 12.73}\n 72%|███████████████████████████▉           | 142/198 [1:06:12<24:18, 26.04s/it]\u001b[32m22:09:54\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 12: dataset_size=176\n   📋 First 10 indices: [47, 4, 28, 41, 165, 108, 85, 38, 72, 103]\n   📋 Last 10 indices: [84, 114, 56, 97, 124, 123, 76, 142, 112, 35]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 72%|████████████████████████████▏          | 143/198 [1:06:33<22:58, 25.07s/it]\u001b[32m22:10:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 14.0\u001b[0m\n\u001b[32m22:10:03\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 13: emitting 176 indices\nFirst ids dataset samples: [130, 12, 86, 14, 135, 134, 164, 24, 55, 64]\n...Last ids: [120, 90, 22, 172, 47, 20, 77, 38, 50, 23]\u001b[0m\n 72%|████████████████████████████▏          | 143/198 [1:06:35<23:05, 25.20s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m22:10:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m22:10:26\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 13: emitting 176 indices\nFirst ids dataset samples: [130, 12, 86, 14, 135, 134, 164, 24, 55, 64]\n...Last ids: [120, 90, 22, 172, 47, 20, 77, 38, 50, 23]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 73%|████████████████████████████▎          | 144/198 [1:07:03<23:19, 25.92s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.2678999900817871, 'grad_norm': 0.2899806499481201, 'learning_rate': 2.538860103626943e-05, 'epoch': 13.64}\n 77%|██████████████████████████████▏        | 153/198 [1:11:13<19:24, 25.88s/it]\u001b[32m22:14:56\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 13: dataset_size=176\n   📋 First 10 indices: [130, 12, 86, 14, 135, 134, 164, 24, 55, 64]\n   📋 Last 10 indices: [120, 90, 22, 172, 47, 20, 77, 38, 50, 23]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 78%|██████████████████████████████▎        | 154/198 [1:11:35<18:33, 25.31s/it]\u001b[32m22:15:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 15.0\u001b[0m\n\u001b[32m22:15:06\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 14: emitting 176 indices\nFirst ids dataset samples: [28, 10, 102, 29, 120, 109, 122, 104, 156, 79]\n...Last ids: [24, 59, 107, 138, 77, 133, 167, 121, 2, 143]\u001b[0m\n\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Maize Phosphorus Deficiency.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m22:15:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m22:15:27\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 14: emitting 176 indices\nFirst ids dataset samples: [28, 10, 102, 29, 120, 109, 122, 104, 156, 79]\n...Last ids: [24, 59, 107, 138, 77, 133, 167, 121, 2, 143]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 78%|██████████████████████████████▌        | 155/198 [1:12:04<18:28, 25.79s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.267799973487854, 'grad_norm': 0.4004894495010376, 'learning_rate': 2.0207253886010365e-05, 'epoch': 14.55}\n 83%|████████████████████████████████▎      | 164/198 [1:16:14<14:35, 25.74s/it]\u001b[32m22:19:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 14: dataset_size=176\n   📋 First 10 indices: [28, 10, 102, 29, 120, 109, 122, 104, 156, 79]\n   📋 Last 10 indices: [24, 59, 107, 138, 77, 133, 167, 121, 2, 143]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 83%|████████████████████████████████▌      | 165/198 [1:16:37<13:54, 25.29s/it]\u001b[32m22:20:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 16.0\u001b[0m\n\u001b[32m22:20:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 15: emitting 176 indices\nFirst ids dataset samples: [153, 107, 133, 75, 108, 167, 41, 83, 61, 77]\n...Last ids: [74, 122, 82, 131, 57, 4, 154, 150, 94, 10]\u001b[0m\n\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m22:20:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m22:20:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 15: emitting 176 indices\nFirst ids dataset samples: [153, 107, 133, 75, 108, 167, 41, 83, 61, 77]\n...Last ids: [74, 122, 82, 131, 57, 4, 154, 150, 94, 10]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 84%|████████████████████████████████▋      | 166/198 [1:17:05<13:44, 25.78s/it][Step 0] Effective Tokens: 0.0M/0.0M\n 88%|██████████████████████████████████▍    | 175/198 [1:21:16<09:54, 25.84s/it]\u001b[32m22:24:58\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 15: dataset_size=176\n   📋 First 10 indices: [153, 107, 133, 75, 108, 167, 41, 83, 61, 77]\n   📋 Last 10 indices: [74, 122, 82, 131, 57, 4, 154, 150, 94, 10]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 89%|██████████████████████████████████▋    | 176/198 [1:21:37<09:10, 25.03s/it]\u001b[32m22:25:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 17.0\u001b[0m\n\u001b[32m22:25:07\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 16: emitting 176 indices\nFirst ids dataset samples: [167, 129, 145, 24, 27, 43, 156, 21, 109, 39]\n...Last ids: [90, 103, 115, 49, 10, 51, 164, 52, 50, 148]\u001b[0m\n\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m22:25:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m22:25:29\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 16: emitting 176 indices\nFirst ids dataset samples: [167, 129, 145, 24, 27, 43, 156, 21, 109, 39]\n...Last ids: [90, 103, 115, 49, 10, 51, 164, 52, 50, 148]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 89%|██████████████████████████████████▊    | 177/198 [1:22:06<08:57, 25.60s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.26749998331069946, 'grad_norm': 0.38175168633461, 'learning_rate': 9.84455958549223e-06, 'epoch': 16.36}\n 94%|████████████████████████████████████▋  | 186/198 [1:26:15<05:11, 25.94s/it]\u001b[32m22:29:59\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 16: dataset_size=176\n   📋 First 10 indices: [167, 129, 145, 24, 27, 43, 156, 21, 109, 39]\n   📋 Last 10 indices: [90, 103, 115, 49, 10, 51, 164, 52, 50, 148]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n 94%|████████████████████████████████████▊  | 187/198 [1:26:38<04:35, 25.08s/it]\u001b[32m22:30:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:21\u001b[0m | \u001b[1m🔄 Starting epoch 18.0\u001b[0m\n\u001b[32m22:30:08\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 17: emitting 176 indices\nFirst ids dataset samples: [56, 24, 36, 104, 13, 139, 17, 48, 123, 82]\n...Last ids: [2, 159, 171, 76, 34, 5, 117, 166, 21, 57]\u001b[0m\n 94%|████████████████████████████████████▊  | 187/198 [1:26:40<04:37, 25.25s/it]\n=== EXAMPLE #1 ===\n\u001b[93m<bos><bos><start_of_turn>user\nWhat is the condition of this maize plant?<image_soft_token><end_of_turn>\n<start_of_turn>model\nThis is a Healthy Maize Plant.<end_of_turn>\n\u001b[0m\n\nMore training debug examples written to .log/dataloader_examples.html\n\u001b[32m22:30:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:28\u001b[0m | \u001b[1m📋 Dataloader examples logged to .log/dataloader_examples.html\u001b[0m\n\u001b[32m22:30:31\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:52\u001b[0m | \u001b[1m🎲 Sampler epoch 17: emitting 176 indices\nFirst ids dataset samples: [56, 24, 36, 104, 13, 139, 17, 48, 123, 82]\n...Last ids: [2, 159, 171, 76, 34, 5, 117, 166, 21, 57]\u001b[0m\n[Step 0] Effective Tokens: 0.0M/0.0M\n 95%|█████████████████████████████████████  | 188/198 [1:27:08<04:18, 25.84s/it][Step 0] Effective Tokens: 0.0M/0.0M\n{'loss': 0.2667999863624573, 'grad_norm': 0.13726773858070374, 'learning_rate': 4.663212435233161e-06, 'epoch': 17.27}\n 99%|██████████████████████████████████████▊| 197/198 [1:31:18<00:25, 25.92s/it]\u001b[32m22:35:01\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mpatch_sampler.py:61\u001b[0m | \u001b[1m🎲 Sampler epoch 17: dataset_size=176\n   📋 First 10 indices: [56, 24, 36, 104, 13, 139, 17, 48, 123, 82]\n   📋 Last 10 indices: [2, 159, 171, 76, 34, 5, 117, 166, 21, 57]\u001b[0m\n[Step 10] Effective Tokens: 0.0M/0.0M\n[Step 10] Effective Tokens: 0.0M/0.0M\n100%|███████████████████████████████████████| 198/198 [1:31:42<00:00, 27.79s/it]\n[rank1]:[W722 22:35:12.262089436 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\n{'train_runtime': 5507.7046, 'train_samples_per_second': 0.575, 'train_steps_per_second': 0.036, 'train_loss': 0.6269946122410321, 'epoch': 18.0}\n100%|███████████████████████████████████████| 198/198 [1:31:46<00:00, 27.81s/it]\n\u001b[32m22:35:16\u001b[0m | \u001b[1mINFO    \u001b[0m | \u001b[36mGPU0\u001b[0m | \u001b[36mlogging_config.py:161\u001b[0m | \u001b[1m⏱️  actual_training: 1.5h\u001b[0m\n\u001b[3m                        \u001b[0m\u001b[1;3;32m⏱️  Training Step Timing Summary\u001b[0m\u001b[3m                         \u001b[0m\n┏━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━┳━━━━━━━━━━━━━┳━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┓\n┃\u001b[1;35m                         \u001b[0m┃\u001b[1;35m        \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mAvg        \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mTotal     \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m                \u001b[0m┃\n┃\u001b[1;35m \u001b[0m\u001b[1;35mStep                   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mCount \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDuration   \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mDuration  \u001b[0m\u001b[1;35m \u001b[0m┃\u001b[1;35m \u001b[0m\u001b[1;35mMin/Max       \u001b[0m\u001b[1;35m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━╇━━━━━━━━━━━━━╇━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━┩\n│\u001b[36m \u001b[0m\u001b[36mmodel_and_training_set…\u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1     \u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1.2m       \u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m1.2m      \u001b[0m\u001b[34m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m1.2m/1.2m     \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[36mactual_training        \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m1     \u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m1.5h       \u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[34m1.5h      \u001b[0m\u001b[34m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m1.5h/1.5h     \u001b[0m\u001b[35m \u001b[0m│\n│\u001b[36m \u001b[0m\u001b[1;36mTOTAL TRAINING\u001b[0m\u001b[36m         \u001b[0m\u001b[36m \u001b[0m│\u001b[33m \u001b[0m\u001b[33m-     \u001b[0m\u001b[33m \u001b[0m│\u001b[32m \u001b[0m\u001b[32m-          \u001b[0m\u001b[32m \u001b[0m│\u001b[34m \u001b[0m\u001b[1;34m1.6h\u001b[0m\u001b[34m      \u001b[0m\u001b[34m \u001b[0m│\u001b[35m \u001b[0m\u001b[35m-             \u001b[0m\u001b[35m \u001b[0m│\n└─────────────────────────┴────────┴─────────────┴────────────┴────────────────┘\n\u001b[1;34mwandb\u001b[0m: \n\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33moutputs/vision_multiGPU_experiment\u001b[0m at: \u001b[34mhttps://wandb.ai/jdmasciano2-university-of-lagos/open-maize-vision1/runs/9lh3iajg\u001b[0m\n\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250722_210328-9lh3iajg/logs\u001b[0m\n[rank0]:[W722 22:35:23.906563650 ProcessGroupNCCL.cpp:1496] Warning: WARNING: destroy_process_group() was not called before program exit, which can leak resources. For more info, please see https://pytorch.org/docs/stable/distributed.html#shutdown (function operator())\nAll processes finished\n","output_type":"stream"}],"execution_count":11},{"cell_type":"code","source":"# ==============================================================================\n# ALTERNATIVE: Using torchrun for proper multi-GPU training\n# ==============================================================================\n\n%%writefile run_multiGPU_training.sh\n#!/bin/bash\n\n# Proper way to run multi-GPU training with torchrun\ntorchrun --nproc_per_node=2 --nnodes=1 train_vision_multiGPU.py\n\n# Make the script executable and run it\n!chmod +x run_multiGPU_training.sh\n# !./run_multiGPU_training.sh  # Uncomment to run with torchrun","metadata":{"trusted":true},"outputs":[],"execution_count":null},{"cell_type":"code","source":"%%writefile test_trained_model.py\n# ==============================================================================\n# STEP 3: Inference with the Fine-Tuned LoRA Model (DEFINITIVE FINAL VERSION)\n# ==============================================================================\n\n\"\"\"\nThis script uses the definitive, correct method for Gemma3N inference by\nexplicitly separating chat templating from data processing to resolve the\ntoken/image mismatch error.\n\"\"\"\n\nfrom unsloth import FastVisionModel\nfrom transformers import AutoProcessor\nfrom PIL import Image\nimport torch\nimport requests\nfrom io import BytesIO\n\ndef load_image_from_url(url: str) -> Image:\n    \"\"\"A helper function to load an image from a URL.\"\"\"\n    try:\n        response = requests.get(url)\n        response.raise_for_status()\n        image = Image.open(BytesIO(response.content)).convert(\"RGB\")\n        print(\"Image loaded successfully from URL.\")\n        return image\n    except requests.exceptions.RequestException as e:\n        print(f\"Error loading image from URL: {e}\")\n        return None\n    except Image.UnidentifiedImageError:\n        print(\"Error: The content downloaded from the URL is not a valid image.\")\n        return None\n\ndef run_inference():\n    # Define paths\n    base_model_name = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\n    adapter_path = \"outputs/vision_multiGPU_experiment\"\n    \n    # Load base model and processor\n    print(f\"Loading base model: {base_model_name}\")\n    model, processor = FastVisionModel.from_pretrained(\n        model_name = base_model_name,\n        max_seq_length = 2048,\n        load_in_4bit = True,\n        dtype = None,\n    )\n    \n    print(\"\\nPreparing model for inference...\")\n    FastVisionModel.for_inference(model)\n    \n    # Load LoRA adapter\n    print(f\"Loading adapter from: {adapter_path}\")\n    model.load_adapter(adapter_path)\n    \n    print(\"\\nModel and adapter loaded successfully!\")\n    \n    # --- Test with a sample image ---\n    test_image_url = \"https://github.com/surfiniaburger/tune/blob/main/sample_images/phosphorus_deficiency_test_2.jpg?raw=true\"\n    image = load_image_from_url(test_image_url)\n    \n    if image is None:\n        return\n\n    # ** THE FINAL, CRUCIAL FIX IS HERE **\n\n    # 1. First, create the full multimodal message structure.\n    #    This includes the image object, which is critical.\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is the condition of this maize plant?\"},\n                {\"type\": \"image\", \"image\": image},\n            ],\n        }\n    ]\n    \n    # 2. Use the tokenizer's templating engine to generate the prompt string.\n    #    This is the step that will correctly insert the `<image>` token into the text.\n    text_prompt = processor.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    # 3. Now, call the main processor with the correctly formatted text and the image.\n    #    This will tokenize the text (including the `<image>` token) and process the image.\n    inputs = processor(\n        text=text_prompt,\n        images=image, # The processor can handle a single image here\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    print(\"\\nGenerating response...\")\n    with torch.inference_mode():\n        outputs = model.generate(\n            **inputs,\n            max_new_tokens=128,\n            use_cache=True,\n        )\n    \n    response = processor.batch_decode(outputs, skip_special_tokens=True)\n    full_response = response[0]\n    \n    prompt_marker = \"model\\n\"\n    answer_start_index = full_response.rfind(prompt_marker)\n    \n    if answer_start_index != -1:\n        final_answer = full_response[answer_start_index + len(prompt_marker):].strip()\n    else:\n        final_answer = \"Could not parse the model's response.\"\n\n    print(\"=\"*40)\n    print(f\"✅ Model's Answer: {final_answer}\")\n    print(\"=\"*40)\n\nif __name__ == \"__main__\":\n    run_inference()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:10:17.821297Z","iopub.execute_input":"2025-07-23T00:10:17.822062Z","iopub.status.idle":"2025-07-23T00:10:17.829074Z","shell.execute_reply.started":"2025-07-23T00:10:17.822031Z","shell.execute_reply":"2025-07-23T00:10:17.828312Z"}},"outputs":[{"name":"stdout","text":"Overwriting test_trained_model.py\n","output_type":"stream"}],"execution_count":46},{"cell_type":"code","source":"!python test_trained_model.py  # Uncomment to test the model","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:10:23.867426Z","iopub.execute_input":"2025-07-23T00:10:23.867714Z","iopub.status.idle":"2025-07-23T00:12:06.670329Z","shell.execute_reply.started":"2025-07-23T00:10:23.867693Z","shell.execute_reply":"2025-07-23T00:12:06.669539Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n2025-07-23 00:10:28.733637: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753229428.756383    3348 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753229428.763162    3348 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n🦥 Unsloth Zoo will now patch everything to make training faster!\nLoading base model: unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\n==((====))==  Unsloth 2025.7.7: Fast Gemma3N patching. Transformers: 4.54.0.dev0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.14s/it]\n\nPreparing model for inference...\nLoading adapter from: outputs/vision_multiGPU_experiment\n\nModel and adapter loaded successfully!\nImage loaded successfully from URL.\n\nGenerating response...\n========================================\n✅ Model's Answer: Based on the image, the maize plant appears to be suffering from **leaf blight**, also known as **Fusarium wilt**. \n\nHere's why:\n\n* **Purple/Red streaks:** The prominent purple or reddish streaks on the leaves are a characteristic symptom of Fusarium wilt. This fungal disease causes the vascular system of the plant to be damaged, leading to these discoloration patterns.\n* **Yellowing/Chlorotic areas:** While not entirely visible in this close-up, Fusarium wilt often results in yellowing or chlorotic areas alongside the purple streaks. \n\n**Fusarium wilt is a common fungal disease affecting maize plants.**\n========================================\n","output_type":"stream"}],"execution_count":47},{"cell_type":"code","source":"%%writefile app.py\n# ==============================================================================\n# GRADIO APP FOR MAIZE DIAGNOSIS (DEFINITIVE CORRECTED VERSION)\n# ==============================================================================\n\n\"\"\"\nThis script launches a Gradio web interface for the fine-tuned maize\nvision model, using the definitive, correct two-step processing logic.\n\"\"\"\n\nimport gradio as gr\nfrom unsloth import FastVisionModel\nfrom transformers import AutoProcessor\nfrom PIL import Image\nimport torch\nimport os\n\n# --- 1. Global Setup: Load Model and Processor ---\n# This section runs only ONCE when the application starts.\n\nprint(\"Performing initial model setup...\")\n\nBASE_MODEL_NAME = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\nADAPTER_PATH = \"outputs/vision_multiGPU_experiment\"\nmodel = None\nprocessor = None\n\ntry:\n    print(f\"Loading base model: {BASE_MODEL_NAME}\")\n    model, processor = FastVisionModel.from_pretrained(\n        model_name=BASE_MODEL_NAME,\n        max_seq_length=2048,\n        load_in_4bit=True,\n        dtype=None,\n    )\n    FastVisionModel.for_inference(model)\n    print(f\"Loading adapter from: {ADAPTER_PATH}\")\n    model.load_adapter(ADAPTER_PATH)\n    print(\"\\n✅ Model and adapter loaded successfully!\")\n\nexcept Exception as e:\n    print(f\"❌ Critical error during model loading: {e}\")\n\n\n# --- 2. Define the Core Prediction Function (Corrected Logic) ---\n\ndef diagnose_maize_plant(uploaded_image: Image.Image) -> str:\n    \"\"\"\n    Takes a PIL Image, runs it through the model, and returns the diagnosis.\n    \"\"\"\n    if model is None or processor is None or uploaded_image is None:\n        return \"Model is not loaded or no image was uploaded. Please check the console for errors.\"\n\n    image = uploaded_image.convert(\"RGB\")\n    \n    # ** THE FINAL, CRUCIAL FIX IS HERE **\n\n    # 1. Create the multimodal message structure including the image object.\n    #    This is the required input for the chat templating engine.\n    messages = [\n        {\n            \"role\": \"user\",\n            \"content\": [\n                {\"type\": \"text\", \"text\": \"What is the condition of this maize plant?\"},\n                {\"type\": \"image\", \"image\": image},\n            ],\n        }\n    ]\n\n    # 2. Use the tokenizer to apply the chat template.\n    #    This correctly creates the final prompt string with the `<image>` placeholder.\n    text_prompt = processor.tokenizer.apply_chat_template(\n        messages,\n        tokenize=False,\n        add_generation_prompt=True\n    )\n\n    # 3. Call the main processor with the pre-formatted text and the image.\n    #    This is the robust method that provides the exact inputs the model needs.\n    inputs = processor(\n        text=text_prompt,\n        images=image,\n        return_tensors=\"pt\"\n    ).to(model.device)\n\n    # Generate the response\n    with torch.inference_mode():\n        outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n    \n    # Decode and clean the final answer\n    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n    prompt_marker = \"model\\n\"\n    answer_start_index = response.rfind(prompt_marker)\n    \n    if answer_start_index != -1:\n        final_answer = response[answer_start_index + len(prompt_marker):].strip()\n    else:\n        final_answer = \"Could not parse model's response. Raw output: \" + response\n\n    return final_answer\n\n\n# --- 3. Build and Launch the Gradio Interface ---\n\nprint(\"Building Gradio interface...\")\n\nexample_images = [\n    [\"https://raw.githubusercontent.com/surfiniaburger/tune/main/sample_images/healthy_maize_test_1.jpg\"],\n    [\"https://raw.githubusercontent.com/surfiniaburger/tune/main/sample_images/phosphorus_deficiency_test_2.jpg\"]\n]\n\ndemo = gr.Interface(\n    fn=diagnose_maize_plant,\n    inputs=gr.Image(type=\"pil\", label=\"Upload Maize Plant Image\"),\n    outputs=gr.Textbox(label=\"Diagnosis\", lines=3),\n    title=\"🌽 Maize Health Diagnosis Assistant\",\n    description=\"Upload an image of a maize plant, and the AI will analyze its condition. This tool is powered by a fine-tuned Gemma3N vision model.\",\n    article=\"Built with Unsloth, OpenSloth, and Gradio.\",\n    examples=example_images,\n    allow_flagging=\"never\",\n)\n\nprint(\"Launching Gradio app... Access it at the URL provided below.\")\ndemo.launch(share=True)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:21:14.990073Z","iopub.execute_input":"2025-07-23T00:21:14.990356Z","iopub.status.idle":"2025-07-23T00:21:14.998054Z","shell.execute_reply.started":"2025-07-23T00:21:14.990331Z","shell.execute_reply":"2025-07-23T00:21:14.997430Z"}},"outputs":[{"name":"stdout","text":"Overwriting app.py\n","output_type":"stream"}],"execution_count":51},{"cell_type":"code","source":"!pip install gradio","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:12:51.712536Z","iopub.execute_input":"2025-07-23T00:12:51.713271Z","iopub.status.idle":"2025-07-23T00:12:55.177552Z","shell.execute_reply.started":"2025-07-23T00:12:51.713248Z","shell.execute_reply":"2025-07-23T00:12:55.176820Z"}},"outputs":[{"name":"stdout","text":"Requirement already satisfied: gradio in /usr/local/lib/python3.11/dist-packages (5.31.0)\nRequirement already satisfied: aiofiles<25.0,>=22.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (22.1.0)\nRequirement already satisfied: anyio<5.0,>=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.9.0)\nRequirement already satisfied: fastapi<1.0,>=0.115.2 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.115.13)\nRequirement already satisfied: ffmpy in /usr/local/lib/python3.11/dist-packages (from gradio) (0.6.0)\nRequirement already satisfied: gradio-client==1.10.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.10.1)\nRequirement already satisfied: groovy~=0.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.2)\nRequirement already satisfied: httpx>=0.24.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.28.1)\nRequirement already satisfied: huggingface-hub>=0.28.1 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.33.1)\nRequirement already satisfied: jinja2<4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.1.6)\nRequirement already satisfied: markupsafe<4.0,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.0.2)\nRequirement already satisfied: numpy<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (1.26.4)\nRequirement already satisfied: orjson~=3.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (3.10.18)\nRequirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from gradio) (24.2)\nRequirement already satisfied: pandas<3.0,>=1.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.2.3)\nRequirement already satisfied: pillow<12.0,>=8.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (11.2.1)\nRequirement already satisfied: pydantic<2.12,>=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.11.7)\nRequirement already satisfied: pydub in /usr/local/lib/python3.11/dist-packages (from gradio) (0.25.1)\nRequirement already satisfied: python-multipart>=0.0.18 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.0.20)\nRequirement already satisfied: pyyaml<7.0,>=5.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (6.0.2)\nRequirement already satisfied: ruff>=0.9.3 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.12.0)\nRequirement already satisfied: safehttpx<0.2.0,>=0.1.6 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.1.6)\nRequirement already satisfied: semantic-version~=2.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (2.10.0)\nRequirement already satisfied: starlette<1.0,>=0.40.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.46.2)\nRequirement already satisfied: tomlkit<0.14.0,>=0.12.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.13.3)\nRequirement already satisfied: typer<1.0,>=0.12 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.16.0)\nRequirement already satisfied: typing-extensions~=4.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (4.14.0)\nRequirement already satisfied: uvicorn>=0.14.0 in /usr/local/lib/python3.11/dist-packages (from gradio) (0.34.3)\nRequirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (2025.3.0)\nRequirement already satisfied: websockets<16.0,>=10.0 in /usr/local/lib/python3.11/dist-packages (from gradio-client==1.10.1->gradio) (15.0.1)\nRequirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (3.10)\nRequirement already satisfied: sniffio>=1.1 in /usr/local/lib/python3.11/dist-packages (from anyio<5.0,>=3.0->gradio) (1.3.1)\nRequirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (2025.6.15)\nRequirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx>=0.24.1->gradio) (1.0.9)\nRequirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx>=0.24.1->gradio) (0.16.0)\nRequirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (3.18.0)\nRequirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (2.32.4)\nRequirement already satisfied: tqdm>=4.42.1 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (4.67.1)\nRequirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub>=0.28.1->gradio) (1.1.5)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2025.2.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy<3.0,>=1.0->gradio) (2.4.1)\nRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2.9.0.post0)\nRequirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0,>=1.0->gradio) (2025.2)\nRequirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.7.0)\nRequirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (2.33.2)\nRequirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12,>=2.0->gradio) (0.4.1)\nRequirement already satisfied: click>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (8.2.1)\nRequirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (1.5.4)\nRequirement already satisfied: rich>=10.11.0 in /usr/local/lib/python3.11/dist-packages (from typer<1.0,>=0.12->gradio) (14.0.0)\nRequirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.8.2->pandas<3.0,>=1.0->gradio) (1.17.0)\nRequirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (3.0.0)\nRequirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich>=10.11.0->typer<1.0,>=0.12->gradio) (2.19.2)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy<3.0,>=1.0->gradio) (2022.2.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy<3.0,>=1.0->gradio) (1.4.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: charset_normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (3.4.2)\nRequirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->huggingface-hub>=0.28.1->gradio) (2.5.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy<3.0,>=1.0->gradio) (2024.2.0)\nRequirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich>=10.11.0->typer<1.0,>=0.12->gradio) (0.1.2)\n","output_type":"stream"}],"execution_count":49},{"cell_type":"code","source":"!python app.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:21:19.189085Z","iopub.execute_input":"2025-07-23T00:21:19.189331Z","iopub.status.idle":"2025-07-23T00:32:41.301248Z","shell.execute_reply.started":"2025-07-23T00:21:19.189314Z","shell.execute_reply":"2025-07-23T00:32:41.300487Z"}},"outputs":[{"name":"stdout","text":"🦥 Unsloth: Will patch your computer to enable 2x faster free finetuning.\n2025-07-23 00:21:26.404383: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1753230086.427747    3454 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1753230086.434953    3454 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n🦥 Unsloth Zoo will now patch everything to make training faster!\nPerforming initial model setup...\nLoading base model: unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\n==((====))==  Unsloth 2025.7.7: Fast Gemma3N patching. Transformers: 4.54.0.dev0.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\nUnsloth: Gemma3N does not support SDPA - switching to eager!\nLoading checkpoint shards: 100%|██████████████████| 3/3 [00:03<00:00,  1.16s/it]\nLoading adapter from: outputs/vision_multiGPU_experiment\n\n✅ Model and adapter loaded successfully!\nBuilding Gradio interface...\n/usr/local/lib/python3.11/dist-packages/gradio/interface.py:416: UserWarning: The `allow_flagging` parameter in `Interface` is deprecated.Use `flagging_mode` instead.\n  warnings.warn(\nLaunching Gradio app... Access it at the URL provided below.\n* Running on local URL:  http://127.0.0.1:7860\n* Running on public URL: https://a5c6c288459b5b0d85.gradio.live\n\nThis share link expires in 1 week. For free permanent hosting and GPU upgrades, run `gradio deploy` from the terminal in the working directory to deploy to Hugging Face Spaces (https://huggingface.co/spaces)\n^C\nKeyboard interruption in main thread... closing server.\nKilling tunnel 127.0.0.1:7860 <> https://a5c6c288459b5b0d85.gradio.live\n","output_type":"stream"}],"execution_count":52},{"cell_type":"code","source":"%%writefile upload_adapter.py\n\nfrom huggingface_hub import HfApi, create_repo\n\n# --- CONFIGURATION ---\n# Your username and a new name for this 2-GPU model version.\nHF_USERNAME = \"surfiniaburger\"\nREPO_NAME = \"AuraMind-Maize-2GPU\"\n\n# The local folder containing your adapter\nLOCAL_ADAPTER_FOLDER = \"outputs/vision_multiGPU_experiment\"\nHF_REPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n\n# --- SCRIPT LOGIC ---\napi = HfApi()\n\nprint(f\"Creating repository '{HF_REPO_ID}' on the Hugging Face Hub...\")\ntry:\n    create_repo(repo_id=HF_REPO_ID, repo_type=\"model\", exist_ok=True)\n    print(\"Repository created successfully (or already exists).\")\nexcept Exception as e:\n    print(f\"Error creating repository: {e}\")\n    exit()\n\nprint(f\"\\nUploading files from '{LOCAL_ADAPTER_FOLDER}' to '{HF_REPO_ID}'...\")\ntry:\n    api.upload_folder(\n        folder_path=LOCAL_ADAPTER_FOLDER,\n        repo_id=HF_REPO_ID,\n        repo_type=\"model\",\n    )\n    print(f\"\\n✅ Successfully uploaded adapter to: https://huggingface.co/{HF_REPO_ID}\")\nexcept Exception as e:\n    print(f\"Error uploading files: {e}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:32:41.302617Z","iopub.execute_input":"2025-07-23T00:32:41.302884Z","iopub.status.idle":"2025-07-23T00:32:41.308787Z","shell.execute_reply.started":"2025-07-23T00:32:41.302847Z","shell.execute_reply":"2025-07-23T00:32:41.308137Z"}},"outputs":[{"name":"stdout","text":"Writing upload_adapter.py\n","output_type":"stream"}],"execution_count":53},{"cell_type":"code","source":"!python upload_adapter.py","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:32:49.976634Z","iopub.execute_input":"2025-07-23T00:32:49.976953Z","iopub.status.idle":"2025-07-23T00:32:50.617741Z","shell.execute_reply.started":"2025-07-23T00:32:49.976929Z","shell.execute_reply":"2025-07-23T00:32:50.617025Z"}},"outputs":[{"name":"stdout","text":"Creating repository 'surfiniaburger/AuraMind-Maize-2GPU' on the Hugging Face Hub...\nError creating repository: 401 Client Error: Unauthorized for url: https://huggingface.co/api/repos/create (Request ID: Root=1-68802db2-58be25962cc8809a4b52cac0;3d35a5c8-3580-4e46-821a-34b42da7ad87)\n\nInvalid username or password.\n","output_type":"stream"}],"execution_count":54},{"cell_type":"code","source":"# ==============================================================================\n# STEP 1: Securely Upload Your Adapter to the Hugging Face Hub\n# ==============================================================================\nfrom kaggle_secrets import UserSecretsClient\nfrom huggingface_hub import login, HfApi, create_repo\n\n# --- 1. Secure Login ---\nprint(\"Attempting secure login to Hugging Face Hub...\")\ntry:\n    user_secrets = UserSecretsClient()\n    secret_value = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n    login(token=secret_value)\n    print(\"✅ Secure login successful!\")\nexcept Exception as e:\n    print(f\"❌ Could not log in. Please ensure 'HUGGINGFACE_API_KEY' is set in Kaggle Secrets. Error: {e}\")\n    # We exit here if login fails, as the rest cannot proceed.\n    exit()\n\n# --- 2. Define the Upload Script Configuration ---\nHF_USERNAME = \"surfiniaburger\"\nREPO_NAME = \"AuraMind-Maize-2GPU\"\nLOCAL_ADAPTER_FOLDER = \"outputs/vision_multiGPU_experiment\"\nHF_REPO_ID = f\"{HF_USERNAME}/{REPO_NAME}\"\n\n# --- 3. Run the Upload Logic ---\napi = HfApi()\n\nprint(f\"\\nCreating repository '{HF_REPO_ID}' on the Hugging Face Hub...\")\ntry:\n    create_repo(repo_id=HF_REPO_ID, repo_type=\"model\", exist_ok=True)\n    print(\"Repository created successfully (or already exists).\")\nexcept Exception as e:\n    print(f\"Error creating repository: {e}\")\n    exit()\n\nprint(f\"\\nUploading files from '{LOCAL_ADAPTER_FOLDER}' to '{HF_REPO_ID}'...\")\ntry:\n    api.upload_folder(\n        folder_path=LOCAL_ADAPTER_FOLDER,\n        repo_id=HF_REPO_ID,\n        repo_type=\"model\",\n    )\n    print(f\"\\n✅✅ Successfully uploaded adapter to: https://huggingface.co/{HF_REPO_ID}\")\nexcept Exception as e:\n    print(f\"Error uploading files: {e}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:35:25.461230Z","iopub.execute_input":"2025-07-23T00:35:25.461819Z","iopub.status.idle":"2025-07-23T00:35:36.581791Z","shell.execute_reply.started":"2025-07-23T00:35:25.461789Z","shell.execute_reply":"2025-07-23T00:35:36.580916Z"}},"outputs":[{"name":"stdout","text":"Attempting secure login to Hugging Face Hub...\n✅ Secure login successful!\n\nCreating repository 'surfiniaburger/AuraMind-Maize-2GPU' on the Hugging Face Hub...\nRepository created successfully (or already exists).\n\nUploading files from 'outputs/vision_multiGPU_experiment' to 'surfiniaburger/AuraMind-Maize-2GPU'...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/91.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"eeb5eb7e5bf246b9b1a8c60c6c733e23"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2647a55ba2ef4a0b80e586fabdfe7080"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b98167e52cfc4175ab7fe7aac0b2a52c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/91.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9218e775f66542bea95ca89456426a91"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Upload 20 LFS files:   0%|          | 0/20 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"24f24c03d726455197d372b03de61ff4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"601f12298d6940c39223afc1d0d92806"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"548c343212f14d88b3f2d6fa0e6ad22a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f65904f57aa049d285c6c2783dad00dd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"fadcb24686e747a7a9be30da54305a9b"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"e25706f068c74eb988fe237a32196db5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"adapter_model.safetensors:   0%|          | 0.00/91.7M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7e9a898b0362424187e7c11150538796"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"optimizer.pt:   0%|          | 0.00/169M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"dae4eaacae4245dd9ef39ea0e9d74575"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"rng_state.pth:   0%|          | 0.00/14.2k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"a8b2ac1ac81147528cae41c2ccf9ef68"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scaler.pt:   0%|          | 0.00/988 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb733b7159374a09a9a5bb04f535300a"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"scheduler.pt:   0%|          | 0.00/1.06k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bc1bf8610ddc4ff4b2de942562f1ed41"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"afd1c3dd3482437d99356f78fa9d6301"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"099b6a863b814df49624d5ebad118621"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"54222cca258b4be48f0a9ddc8bbbefba"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"2ffea6c9230c419287238a2f92ead0fe"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"06aea1a83d1147dc990393e047c176af"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"training_args.bin:   0%|          | 0.00/5.82k [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5316225b3e694c84aee504905efabeb7"}},"metadata":{}},{"name":"stdout","text":"\n✅✅ Successfully uploaded adapter to: https://huggingface.co/surfiniaburger/AuraMind-Maize-2GPU\n","output_type":"stream"}],"execution_count":55},{"cell_type":"code","source":"%%writefile app.py\n# ==============================================================================\n# AURA-MIND: MAIZE HEALTH DIAGNOSIS APP (DEPLOYMENT-READY)\n# ==============================================================================\n\nimport gradio as gr\nfrom unsloth import FastVisionModel\nfrom transformers import AutoProcessor\nfrom PIL import Image\nimport torch\nimport os\n\n# --- 1. Global Setup: Load Model and Processor from Hub ---\n\nprint(\"Performing initial AuraMind model setup...\")\n\nBASE_MODEL_NAME = \"unsloth/gemma-3n-E2B-it-unsloth-bnb-4bit\"\nADAPTER_PATH = \"surfiniaburger/AuraMind-Maize-2GPU\"\n\nmodel = None\nprocessor = None\n\ntry:\n    print(f\"Loading base model: {BASE_MODEL_NAME}\")\n    model, processor = FastVisionModel.from_pretrained(\n        model_name=BASE_MODEL_NAME, max_seq_length=2048, load_in_4bit=True, dtype=None\n    )\n    FastVisionModel.for_inference(model)\n    \n    print(f\"Loading AuraMind adapter from Hub: {ADAPTER_PATH}\")\n    model.load_adapter(ADAPTER_PATH)\n    \n    print(\"\\n✅ AuraMind model and adapter loaded successfully!\")\nexcept Exception as e:\n    print(f\"❌ Critical error during model loading: {e}\")\n\n\n# --- 2. Define the Core Prediction Function ---\ndef diagnose_maize_plant(uploaded_image: Image.Image) -> str:\n    if model is None or processor is None or uploaded_image is None:\n        return \"Model is not loaded or no image was uploaded. Please check the console for errors.\"\n\n    image = uploaded_image.convert(\"RGB\")\n    messages = [{\"role\": \"user\", \"content\": [{\"type\": \"text\", \"text\": \"What is the condition of this maize plant?\"}, {\"type\": \"image\", \"image\": image}]}]\n    text_prompt = processor.tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n    inputs = processor(text=text_prompt, images=image, return_tensors=\"pt\").to(model.device)\n\n    with torch.inference_mode():\n        outputs = model.generate(**inputs, max_new_tokens=128, use_cache=True)\n    \n    response = processor.batch_decode(outputs, skip_special_tokens=True)[0]\n    prompt_marker = \"model\\n\"\n    answer_start_index = response.rfind(prompt_marker)\n    \n    final_answer = response[answer_start_index + len(prompt_marker):].strip() if answer_start_index != -1 else \"Could not parse model's response.\"\n    return final_answer\n\n# --- 3. Build and Launch the Gradio Interface ---\nprint(\"Building Gradio interface...\")\n\ndemo = gr.Interface(\n    fn=diagnose_maize_plant,\n    inputs=gr.Image(type=\"pil\", label=\"Upload Maize Plant Image\"),\n    outputs=gr.Textbox(label=\"Diagnosis\", lines=3),\n    title=\"🌽 AuraMind: Maize Health Diagnosis (2-GPU Ver.)\",\n    description=\"Upload an image of a maize plant, and the AuraMind AI will analyze its condition. This model was fine-tuned on two GPUs for enhanced performance.\",\n    article=\"Built with Unsloth and Gradio by surfiniaburger.\",\n    allow_flagging=\"never\",\n)\n\nif __name__ == \"__main__\":\n    print(\"Launching Gradio app for deployment...\")\n    # For deployment, we don't need a share link. Gradio deploy handles it.\n    demo.launch()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-07-23T00:37:16.820597Z","iopub.execute_input":"2025-07-23T00:37:16.821210Z","iopub.status.idle":"2025-07-23T00:37:16.827573Z","shell.execute_reply.started":"2025-07-23T00:37:16.821186Z","shell.execute_reply":"2025-07-23T00:37:16.826911Z"}},"outputs":[{"name":"stdout","text":"Overwriting app.py\n","output_type":"stream"}],"execution_count":56}]}