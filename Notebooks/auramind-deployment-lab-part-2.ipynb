{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":253771701,"sourceType":"kernelVersion"}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# ==============================================================================\n# NOTEBOOK 1: Create and Save Merged Model\n# ==============================================================================\n\n# --- CELL 1: Setup ---\nimport os\n# Force the environment to see only one GPU to guarantee stability\nos.environ[\"CUDA_VISIBLE_DEVICES\"] = \"0\"\n\n# Install all necessary libraries\n!pip install \"transformers==4.54.1\" -qU\n!pip install \"wandb>=0.17.0\" -qU\n!pip install --no-deps \"bitsandbytes>=0.43.1\" \"accelerate>=0.31.0\" \"xformers==0.0.29.post3\" \"trl>=0.9.4\" triton -q\n!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth.git -q\n!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth-zoo.git -q\n!pip install -U peft -q\n!pip install \"timm>=1.0.16\" -qU\n\n\n!apt-get install git-lfs -y > /dev/null\n!git config --global credential.helper store","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T05:16:31.949887Z","iopub.execute_input":"2025-08-02T05:16:31.950574Z","iopub.status.idle":"2025-08-02T05:17:10.401210Z","shell.execute_reply.started":"2025-08-02T05:16:31.950531Z","shell.execute_reply":"2025-08-02T05:17:10.400024Z"}},"outputs":[{"name":"stdout","text":"  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for unsloth_zoo (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"# ==============================================================================\n# CELL 2: Prepare for Slicing (Space-Efficient Version)\n# ==============================================================================\nimport os\nimport json\nimport re\nimport torch\nimport gc\nfrom huggingface_hub import snapshot_download\nfrom safetensors import safe_open\nfrom safetensors.torch import save_file\nfrom tqdm.auto import tqdm\nfrom transformers import AutoConfig\n\n# --- Step 1: Define Paths ---\n# Read-only input path for the merged model. We will read shards directly from here.\ninput_merged_model_path = \"/kaggle/input/auramind-deployment-lab-part-1/AuraMind-E4B-Finetuned-Merged/\"\n\n# The final output path for the sliced model.\nfinal_sliced_e2b_path = \"/kaggle/working/AuraMind-E2B-Finetuned-Sliced/\"\n\n# A temporary path for the configuration files we need to modify.\nwritable_config_path = \"/kaggle/working/temp_configs/\"\n\nos.makedirs(final_sliced_e2b_path, exist_ok=True)\nos.makedirs(writable_config_path, exist_ok=True)\n\n# The original base model from Hugging Face (to get original configs)\nMODEL_NAME_E4B = \"unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit\"\n\n# --- Step 2: Prepare Configuration Files ---\nprint(\"Preparing configuration files...\")\n# Download original base model configs\nbase_model_cache_path = snapshot_download(MODEL_NAME_E4B)\n\n# Copy necessary configs and tokenizer files to a writable location\n!cp {base_model_cache_path}/*.json {writable_config_path}\n!cp {base_model_cache_path}/*.jinja {writable_config_path}\n!cp {input_merged_model_path}/tokenizer* {writable_config_path}\nprint(\"✅ Configuration and tokenizer files are ready.\")\n\n# --- Step 3: Slice the Model by Reading Directly from Source ---\n# Point directly to the safetensor files in the read-only input directory\nsafetensor_files = [os.path.join(input_merged_model_path, f) for f in os.listdir(input_merged_model_path) if f.endswith('.safetensors')]\n\n# Slicing parameters (unchanged)\nlayers_to_skip = [20, 21, 22, 23, 24]\nfinal_num_layers = 35 - len(layers_to_skip)\nffn_hidden_dims = [8192] * final_num_layers\nkept_layers_indices = [i for i in range(35) if i not in layers_to_skip]\nlayer_rename_map = {old_idx: new_idx for new_idx, old_idx in enumerate(kept_layers_indices)}\nweight_map = {}\nnew_shard_state_dict = {}\n\nprint(\"Slicing model shards directly from source (no copy)...\")\n# This loop now reads from the original, read-only path\nfor shard_path in tqdm(safetensor_files, desc=\"Processing shards\"):\n    with safe_open(shard_path, framework=\"pt\", device=\"cpu\") as f:\n        for tensor_name in f.keys():\n            new_tensor_name = tensor_name\n            tensor = f.get_tensor(tensor_name)\n            match = re.search(r'\\.layers\\.(\\d+)\\.', tensor_name)\n            if match:\n                old_layer_idx = int(match.group(1))\n                if old_layer_idx in layers_to_skip: continue\n                new_layer_idx = layer_rename_map[old_layer_idx]\n                new_tensor_name = tensor_name.replace(f'.layers.{old_layer_idx}.', f'.layers.{new_layer_idx}.')\n                target_ffn_dim = ffn_hidden_dims[new_layer_idx]\n                if 'mlp.gate_proj.weight' in new_tensor_name or 'mlp.up_proj.weight' in new_tensor_name:\n                    tensor = tensor[:target_ffn_dim, :].contiguous()\n                elif 'mlp.down_proj.weight' in new_tensor_name:\n                    tensor = tensor[:, :target_ffn_dim].contiguous()\n            elif 'per_layer_model_projection' in tensor_name:\n                reshaped = tensor.reshape((35, tensor.shape[0] // 35, tensor.shape[1]))\n                tensor = reshaped[kept_layers_indices, :, :].reshape(-1, tensor.shape[-1]).contiguous()\n            elif 'embed_tokens_per_layer' in tensor_name:\n                reshaped = tensor.reshape((tensor.shape[0], 35, tensor.shape[1] // 35))\n                tensor = reshaped[:, kept_layers_indices, :].reshape(tensor.shape[0], -1).contiguous()\n            new_shard_state_dict[new_tensor_name] = tensor\n\n# --- Step 4: Save the Sliced Model ---\nprint(\"Saving sliced model to final destination...\")\nshard_filename = \"model-00001-of-00001.safetensors\"\nsave_file(new_shard_state_dict, os.path.join(final_sliced_e2b_path, shard_filename), metadata={'format': 'pt'})\nfor k in new_shard_state_dict.keys():\n    weight_map[k] = shard_filename\nprint(\"✅ New safetensor shard saved.\")\n\n# --- Step 5: Finalize Configuration and Cleanup ---\nprint(\"Finalizing sliced model files...\")\n# Copy the tokenizer files from our temp location to the final destination\n!cp {writable_config_path}/tokenizer* {final_sliced_e2b_path}\n\n# Create the new index file\nindex_json = { \"metadata\": {\"total_size\": sum(t.numel() * t.element_size() for t in new_shard_state_dict.values())}, \"weight_map\": weight_map }\nwith open(os.path.join(final_sliced_e2b_path, \"model.safetensors.index.json\"), \"w\") as f:\n    json.dump(index_json, f, indent=2)\n\nprint(\"Correcting the configuration for the new 30-layer model...\")\n# Load config from our temporary writable path\nnew_config = AutoConfig.from_pretrained(writable_config_path)\nnew_config.text_config.num_hidden_layers = final_num_layers\nnew_config.text_config.intermediate_size = ffn_hidden_dims\ncount_kv_sharing = sum(1 for layer in layers_to_skip if layer >= 20)\nnew_config.text_config.num_kv_shared_layers -= count_kv_sharing\ncount_activation_sparsity = sum(1 for layer in layers_to_skip if layer <= 9)\nactivation_sparsity_list = [0.95] * (10 - count_activation_sparsity) + [0] * (final_num_layers - 10 + count_activation_sparsity)\nnew_config.text_config.activation_sparsity_pattern = activation_sparsity_list\n\n# Save the final, corrected config to the output directory\nnew_config.save_pretrained(final_sliced_e2b_path)\nprint(\"✅ Corrected config.json saved.\")\n\n# Clean up the temporary config directory\n!rm -r {writable_config_path}\nprint(\"✅ Temporary files cleaned up.\")\n\nprint(f\"\\n✅ Model slicing complete. New standalone model saved in: {final_sliced_e2b_path}\")\n\n","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-02T05:17:10.403121Z","iopub.execute_input":"2025-08-02T05:17:10.403469Z","iopub.status.idle":"2025-08-02T05:19:22.697468Z","shell.execute_reply.started":"2025-08-02T05:17:10.403432Z","shell.execute_reply":"2025-08-02T05:19:22.696213Z"}},"outputs":[{"name":"stdout","text":"Preparing configuration files...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Fetching 15 files:   0%|          | 0/15 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"85a1f807cd6243a597871fddba5e9247"}},"metadata":{}},{"name":"stdout","text":"✅ Configuration and tokenizer files are ready.\nSlicing model shards directly from source (no copy)...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Processing shards:   0%|          | 0/4 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"883ce3a277364d04b39b4c7db0a6ba0a"}},"metadata":{}},{"name":"stdout","text":"Saving sliced model to final destination...\n✅ New safetensor shard saved.\nFinalizing sliced model files...\nCorrecting the configuration for the new 30-layer model...\n✅ Corrected config.json saved.\n✅ Temporary files cleaned up.\n\n✅ Model slicing complete. New standalone model saved in: /kaggle/working/AuraMind-E2B-Finetuned-Sliced/\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"# ==============================================================================\n# CELL 1: Securely Login to Hugging Face\n# ==============================================================================\nfrom huggingface_hub import login\n\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\n# This securely retrieves the secret you stored in the Colab Secrets Manager.\n# Make sure the secret name is exactly \"HF_API_KEY\".\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\n\n# Login to the Hugging Face Hub\nlogin(token=hf_token)\n\nprint(\"✅ Successfully logged into Hugging Face.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T05:22:18.392523Z","iopub.execute_input":"2025-08-02T05:22:18.393192Z","iopub.status.idle":"2025-08-02T05:22:18.641055Z","shell.execute_reply.started":"2025-08-02T05:22:18.393166Z","shell.execute_reply":"2025-08-02T05:22:18.640300Z"}},"outputs":[{"name":"stdout","text":"✅ Successfully logged into Hugging Face.\n","output_type":"stream"}],"execution_count":4},{"cell_type":"code","source":"# ==============================================================================\n# CELL 5: Install Git LFS, Verify, and Upload the Final Model\n# ==============================================================================\n# THE DEFINITIVE FIX: Explicitly install git-lfs in the Colab environment.\n# This ensures the huggingface_hub library can correctly handle large file uploads.\n!apt-get install git-lfs -y\n!git-lfs install\n\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom huggingface_hub import HfApi, ModelCard, ModelCardData\nimport os\n\n# The path to your successfully sliced model\nfinal_sliced_e2b_path = \"/kaggle/working/AuraMind-E2B-Finetuned-Sliced/\"\n\n# --- Step 7: Verification (SKIPPED) ---\nprint(\"--- Step 7: Verification Skipped ---\")\nprint(\"Proceeding directly to upload.\")\n\nprint(\"\\n--- Step 8: Preparing to upload to Hugging Face Hub ---\")\n# Define your new model's name on the Hub\nhf_repo_id = \"surfiniaburger/AuraMind-Maize-Expert-E2B-Finetuned\"\n\n# Create a model card\ncard_content = f\"\"\"\n---\nlicense: apache-2.0\nlanguage: en\ntags:\n- gemma-3n\n- unsloth\n- social-impact\n- agriculture\n- computer-vision\n---\n\n# AuraMind: Fine-Tuned Gemma 3n Maize Expert (E2B)\n\nThis is a specialized, fine-tuned version of Google's Gemma 3n model, optimized for diagnosing maize plant health conditions in Nigeria. It was developed as part of the **Google - The Gemma 3n Impact Challenge**.\n\n## Model Derivation\n\nThis model was created using an advanced \"fine-tune then slice\" approach, leveraging the native MatFormer architecture of Gemma 3n:\n1.  The full `unsloth/gemma-3n-E4B-it-unsloth-bnb-4bit` model was fine-tuned on a custom dataset of local Nigerian maize diseases.\n2.  The fine-tuning was performed using LoRA adapters from a hyperparameter sweep, with the champion run (`icy-sweep-2`) achieving **100% validation accuracy**.\n3.  The trained adapters were merged into the full E4B model.\n4.  Finally, the E2B sub-model was surgically extracted using the principles from Google's official MatFormer Lab, resulting in this efficient, deployable, and highly accurate expert model.\n\nThis process ensures the model has both the high performance of our fine-tuning and the clean, convertible architecture of the official E2B release, making it ideal for on-device deployment with tools like Google AI Edge and MediaPipe.\n\n**Project Link:** [https://github.com/surfiniaburger/AuraMind](https://github.com/surfiniaburger/AuraMind)\n\"\"\"\ncard = ModelCard(card_content)\ncard.save(os.path.join(final_sliced_e2b_path, 'README.md'))\nprint(\"✅ Model card (README.md) created.\")\n\n# Upload the final model to the Hub\nprint(f\"Uploading model to: {hf_repo_id}\")\napi = HfApi()\napi.create_repo(repo_id=hf_repo_id, exist_ok=True)\napi.upload_folder(\n    folder_path=final_sliced_e2b_path,\n    repo_id=hf_repo_id,\n    commit_message=\"Upload final fine-tuned and sliced AuraMind E2B model\"\n)\n\nprint(f\"\\n✅ SUCCESS! Your final model is now available on the Hugging Face Hub at: https://huggingface.co/{hf_repo_id}\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-02T05:23:04.431353Z","iopub.execute_input":"2025-08-02T05:23:04.431866Z","iopub.status.idle":"2025-08-02T05:27:26.327562Z","shell.execute_reply.started":"2025-08-02T05:23:04.431839Z","shell.execute_reply":"2025-08-02T05:27:26.326637Z"}},"outputs":[{"name":"stdout","text":"Reading package lists... Done\nBuilding dependency tree... Done\nReading state information... Done\ngit-lfs is already the newest version (3.0.2-1ubuntu0.3).\n0 upgraded, 0 newly installed, 0 to remove and 38 not upgraded.\nGit LFS initialized.\n--- Step 7: Verification Skipped ---\nProceeding directly to upload.\n\n--- Step 8: Preparing to upload to Hugging Face Hub ---\n✅ Model card (README.md) created.\nUploading model to: surfiniaburger/AuraMind-Maize-Expert-E2B-Finetuned\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Upload 3 LFS files:   0%|          | 0/3 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bf9dc01402bb44ecbee6e72668dd2528"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/33.4M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"67661057409341cab2000fdada485001"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.model:   0%|          | 0.00/4.70M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"55ae19fbf56f43e2bfd0ee2e560ea997"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00001.safetensors:   0%|          | 0.00/10.9G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"c5887a69871645c2b241693e9afbda0c"}},"metadata":{}},{"name":"stdout","text":"\n✅ SUCCESS! Your final model is now available on the Hugging Face Hub at: https://huggingface.co/surfiniaburger/AuraMind-Maize-Expert-E2B-Finetuned\n","output_type":"stream"}],"execution_count":5}]}