{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "b1076dfc-b9ad-4769-8c92-a6c4dae69d19",
    "_uuid": "8f2839f25d086af736a60e9eeb907d3b93b6e0e5",
    "execution": {
     "iopub.execute_input": "2025-08-05T01:24:49.965462Z",
     "iopub.status.busy": "2025-08-05T01:24:49.965164Z",
     "iopub.status.idle": "2025-08-05T01:33:05.809904Z",
     "shell.execute_reply": "2025-08-05T01:33:05.807519Z",
     "shell.execute_reply.started": "2025-08-05T01:24:49.965436Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m41.7/41.7 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m11.2/11.2 MB\u001b[0m \u001b[31m35.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m20.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "datasets 3.6.0 requires fsspec[http]<=2025.3.0,>=2023.1.0, but you have fsspec 2025.5.1 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.9/40.9 kB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.5/16.5 MB\u001b[0m \u001b[31m65.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m81.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m64.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m2.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0mm\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m3.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m27.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m10.8/10.8 MB\u001b[0m \u001b[31m36.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m0:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m11.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m425.8/425.8 kB\u001b[0m \u001b[31m15.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m8.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m46.0/46.0 kB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m86.8/86.8 kB\u001b[0m \u001b[31m4.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\n",
      "google-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n",
      "google-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\n",
      "cesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\n",
      "pandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\n",
      "google-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\n",
      "gcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\n",
      "google-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\n",
      "dataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\n",
      "tensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\n",
      "bigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\n",
      "bigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m54.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m4.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m81.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m3.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m4.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m20.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m7.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m89.3/89.3 kB\u001b[0m \u001b[31m4.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m52.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m826.7/826.7 MB\u001b[0m \u001b[31m1.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m706.8/706.8 MB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m43.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m287.2/287.2 MB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m322.3/322.3 MB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.6/124.6 MB\u001b[0m \u001b[31m7.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m154.7/154.7 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m4.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3.4/3.4 MB\u001b[0m \u001b[31m2.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
      "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.9.0.dev20250804+cu126 which is incompatible.\u001b[0m\u001b[31m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m60.8/60.8 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m48.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n",
      "\u001b[?25h"
     ]
    }
   ],
   "source": [
    "!pip install \"transformers==4.54.1\" -qU\n",
    "!pip install optimum[onnxruntime] -qU\n",
    "!pip install transformers[torch] -qU\n",
    "!pip install optimum -qU\n",
    "!pip install --pre torch torchvision torchaudio --index-url https://download.pytorch.org/whl/nightly/cu126 -qU\n",
    "!pip install -U bitsandbytes -qU\n",
    "!pip install --no-deps --upgrade timm -qU # Only for Gemma 3N"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ran out of memory on kaggle. A system with a lot higher RAM would complete the conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "trusted": true
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, PretrainedConfig\n",
    "from optimum.exporters.onnx import export as onnx_export\n",
    "from optimum.exporters.onnx.config import TextDecoderOnnxConfig\n",
    "from optimum.utils import (\n",
    "    DummyVisionInputGenerator,\n",
    "    DummyTextInputGenerator,\n",
    "    DummyPastKeyValuesGenerator,\n",
    "    NormalizedTextConfig,\n",
    "    NormalizedVisionConfig,\n",
    ")\n",
    "from typing import Dict\n",
    "import gc\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the PyTorch model on Kaggle\n",
    "pytorch_model_path = \"/kaggle/input/model-name-auramind-maize-expert-e2b/pytorch/default/1/AuraMind-E2B-Finetuned-Sliced\"\n",
    "# Path where the final ONNX model will be saved\n",
    "onnx_output_path = \"./onnx_multimodal_model\"\n",
    "\n",
    "print(f\"PyTorch model path: {pytorch_model_path}\")\n",
    "print(f\"ONNX output path: {onnx_output_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CELL 1: DEFINE THE CUSTOM ONNX CONFIGURATION FOR IMAGE-TO-TEXT\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 1: Defining a custom ONNX configuration for the image-to-text model...\")\n",
    "\n",
    "class CustomGemma3NImageToTextOnnxConfig(TextDecoderOnnxConfig):\n",
    "    \"\"\"\n",
    "    This ONNX configuration is for the Gemma3N model for image-to-text tasks.\n",
    "    It handles the vision (pixel_values) and text (input_ids) modalities.\n",
    "    \"\"\"\n",
    "\n",
    "    NORMALIZED_CONFIG_CLASS = NormalizedTextConfig.with_args(\n",
    "        num_layers=\"num_hidden_layers\",\n",
    "        num_attention_heads=\"num_attention_heads\",\n",
    "        hidden_size=\"hidden_size\",\n",
    "    )\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig, task: str = \"default\", **kwargs):\n",
    "        super().__init__(config=config.text_config, task=task, **kwargs)\n",
    "        self.text_config = config.text_config\n",
    "        self.vision_config = config.vision_config\n",
    "        self.config = config\n",
    "\n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        text_inputs = super().inputs\n",
    "        vision_inputs = {\n",
    "            \"pixel_values\": {0: \"batch_size\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}\n",
    "        }\n",
    "        return {**vision_inputs, **text_inputs}\n",
    "\n",
    "    def generate_dummy_inputs(self, batch_size: int = 1, sequence_length: int = 260, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        \"\"\"\n",
    "        Generates dummy inputs for vision and text modalities.\n",
    "        A batch_size of 1 is used to reduce memory consumption during export.\n",
    "        \"\"\"\n",
    "        # 1. Generate Text Inputs\n",
    "        text_input_generator = DummyTextInputGenerator(\n",
    "            self.task,\n",
    "            self._normalized_config,\n",
    "            batch_size=batch_size,\n",
    "            sequence_length=sequence_length,\n",
    "            **self.text_config.to_dict(),\n",
    "        )\n",
    "        dummy_inputs = {\n",
    "            \"input_ids\": text_input_generator.generate(input_name=\"input_ids\", framework=\"pt\"),\n",
    "            \"attention_mask\": text_input_generator.generate(input_name=\"attention_mask\", framework=\"pt\"),\n",
    "        }\n",
    "\n",
    "        # 2. Inject Special Image Tokens\n",
    "        image_token_id = self.config.image_token_id\n",
    "        tokens_per_image = self.config.vision_soft_tokens_per_image\n",
    "        if sequence_length < tokens_per_image:\n",
    "            raise ValueError(f\"Sequence length must be at least {tokens_per_image} to hold image tokens.\")\n",
    "        for i in range(batch_size):\n",
    "            dummy_inputs[\"input_ids\"][i, :tokens_per_image] = image_token_id\n",
    "\n",
    "        # 3. Generate Past Key-Values if needed\n",
    "        if self.use_past:\n",
    "            past_key_values_generator = DummyPastKeyValuesGenerator(\n",
    "                self.task, \n",
    "                self._normalized_config,\n",
    "                batch_size=batch_size,\n",
    "                sequence_length=sequence_length,\n",
    "            )\n",
    "            past_dummy_inputs = past_key_values_generator.generate(framework=\"pt\")\n",
    "            dummy_inputs.update(past_dummy_inputs)\n",
    "\n",
    "        # 4. Generate Vision Inputs\n",
    "        normalized_vision_config = NormalizedVisionConfig(self.vision_config)\n",
    "        image_size = getattr(self.vision_config, \"image_size\", 224)\n",
    "        num_channels = getattr(self.vision_config, \"num_channels\", 3)\n",
    "        vision_input_generator = DummyVisionInputGenerator(\n",
    "            self.task, \n",
    "            normalized_vision_config,\n",
    "            batch_size=batch_size,\n",
    "            num_channels=num_channels,\n",
    "            height=image_size,\n",
    "            width=image_size,\n",
    "        )\n",
    "        dummy_inputs[\"pixel_values\"] = vision_input_generator.generate(input_name=\"pixel_values\", framework=\"pt\")\n",
    "\n",
    "        return dummy_inputs\n",
    "\n",
    "print(\"   - CustomGemma3NImageToTextOnnxConfig defined.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CELL 2: PREPARE AND RUN THE ONNX EXPORT\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 2: Preparing and running the ONNX export...\")\n",
    "\n",
    "try:\n",
    "    # --- Step 2.1: Clean the Configuration ---\n",
    "    print(\"   - Cleaning the model configuration...\")\n",
    "    cleaned_config = AutoConfig.from_pretrained(pytorch_model_path, trust_remote_code=True)\n",
    "\n",
    "    if hasattr(cleaned_config, \"quantization_config\"):\n",
    "        delattr(cleaned_config, \"quantization_config\")\n",
    "    if hasattr(cleaned_config, \"unsloth_fixed\"):\n",
    "        delattr(cleaned_config, \"unsloth_fixed\")\n",
    "    \n",
    "    print(\"   - Configuration cleaned successfully.\")\n",
    "\n",
    "    # --- Step 2.2: Load the Model with the Cleaned Config ---\n",
    "    print(\"   - Loading model with the cleaned configuration...\")\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pytorch_model_path,\n",
    "        config=cleaned_config,\n",
    "        torch_dtype=torch.float16, # Use float16 for memory optimization\n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval() # Set model to evaluation mode before export\n",
    "    print(\"   - Model loaded successfully!\")\n",
    "\n",
    "    # --- Step 2.3: Configure and Run the ONNX Export ---\n",
    "    print(\"   - Configuring the ONNX export for the image-to-text model...\")\n",
    "    custom_onnx_config = CustomGemma3NImageToTextOnnxConfig(\n",
    "        config=model.config, \n",
    "        task=\"text-generation\"\n",
    "    )\n",
    "\n",
    "    # --- Step 2.4: Clean up memory before export ---\n",
    "    print(\"   - Cleaning up memory before starting export...\")\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"   - Starting ONNX export...\")\n",
    "    onnx_export(\n",
    "        model=model,\n",
    "        config=custom_onnx_config,\n",
    "        output=Path(onnx_output_path),\n",
    "        opset=14,\n",
    "    )\n",
    "    print(\"\\n✅ ONNX conversion process completed successfully!\")\n",
    "    print(f\"   The exported model is saved in: {Path(onnx_output_path).resolve()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred during the ONNX conversion process: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "execution": {
     "execution_failed": "2025-08-05T01:40:12.354Z",
     "iopub.execute_input": "2025-08-05T01:33:05.815533Z",
     "iopub.status.busy": "2025-08-05T01:33:05.814990Z"
    },
    "trusted": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2025-08-05 01:33:19.909590: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n",
      "WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n",
      "E0000 00:00:1754357600.214134      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n",
      "E0000 00:00:1754357600.304432      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'MessageFactory' object has no attribute 'GetPrototype'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/_internal/registration.py:162: OnnxExporterWarning: Symbolic function 'aten::scaled_dot_product_attention' already registered for opset 14. Replacing the existing function with new function. This is unexpected. Please report it on https://github.com/pytorch/pytorch/issues.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Targeting SLICED model path: /kaggle/input/model-name-auramind-maize-expert-e2b/pytorch/default/1/AuraMind-E2B-Finetuned-Sliced\n",
      "ONNX output path: ./onnx_multimodal_model\n",
      "\n",
      "Step 1: Defining a custom ONNX configuration for the image-to-text model...\n",
      "   - CustomGemma3NImageToTextOnnxConfig defined.\n",
      "\n",
      "Step 2: Preparing and running the ONNX export...\n",
      "   - Loading and fixing configuration for the SLICED model...\n",
      "   - Removed 'quantization_config'.\n",
      "   - Removed 'unsloth_fixed'.\n",
      "   - Manually set model_type to: gemma3n\n",
      "   - Loading SLICED model with the corrected configuration...\n",
      "   - Sliced model loaded successfully!\n",
      "   - Configuring the ONNX export...\n",
      "   - Cleaning up memory before starting export...\n",
      "   - Starting ONNX export...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.11/dist-packages/timm/models/mobilenetv5.py:94: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if feat_size[0] < high_resolution[0] or feat_size[1] < high_resolution[1]:\n",
      "/usr/local/lib/python3.11/dist-packages/timm/models/mobilenetv5.py:101: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if high_resolution[0] != self.output_resolution[0] or high_resolution[1] != self.output_resolution[1]:\n",
      "/usr/local/lib/python3.11/dist-packages/timm/models/mobilenetv5.py:104: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  high_resolution[0] % self.output_resolution[0] != 0 or\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3n/modeling_gemma3n.py:2068: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if not is_torchdynamo_compiling() and inputs_embeds[special_image_mask].numel() != image_features.numel():\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3n/modeling_gemma3n.py:1747: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if per_layer_projection.shape != per_layer_inputs.shape:\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/masking_utils.py:188: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if (padding_length := kv_length + kv_offset - attention_mask.shape[-1]) > 0:\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/masking_utils.py:216: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  if padding_mask is not None and padding_mask.shape[-1] > kv_length:\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3n/modeling_gemma3n.py:1650: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  epsilon_tensor = torch.tensor(1e-5)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/integrations/sdpa_attention.py:74: TracerWarning: Converting a tensor to a Python boolean might cause the trace to be incorrect. We can't record the data flow of Python values, so this value will be treated as a constant in the future. This means that the trace might not generalize to other inputs!\n",
      "  is_causal = query.shape[2] > 1 and attention_mask is None and getattr(module, \"is_causal\", True)\n",
      "/usr/local/lib/python3.11/dist-packages/transformers/models/gemma3n/modeling_gemma3n.py:1039: TracerWarning: torch.tensor results are registered as constants in the trace. You can safely ignore this warning if you use this function to create tensors out of constant variables that would be the same every time you call this function. In any other case, this might cause the trace to be incorrect.\n",
      "  target_sparsity_tensor = torch.tensor(self.activation_sparsity, dtype=torch.float32, device=inputs.device)\n",
      "/usr/local/lib/python3.11/dist-packages/torch/onnx/symbolic_opset9.py:5350: UserWarning: Exporting aten::index operator of advanced indexing in opset 14 is achieved by combination of multiple ONNX operators, including Reshape, Transpose, Concat, and Gather. If indices include negative values, the exported graph will produce incorrect results.\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from pathlib import Path\n",
    "from transformers import AutoConfig, AutoModelForCausalLM, PretrainedConfig\n",
    "from optimum.exporters.onnx import export as onnx_export\n",
    "from optimum.exporters.onnx.config import TextDecoderOnnxConfig\n",
    "from optimum.utils import (\n",
    "    DummyVisionInputGenerator,\n",
    "    DummyTextInputGenerator,\n",
    "    DummyPastKeyValuesGenerator,\n",
    "    NormalizedTextConfig,\n",
    "    NormalizedVisionConfig,\n",
    ")\n",
    "from typing import Dict\n",
    "import gc\n",
    "\n",
    "# --- Configuration ---\n",
    "# Path to the SLICED model from your second notebook. This is the only one that will fit in memory.\n",
    "pytorch_model_path = \"/kaggle/input/model-name-auramind-maize-expert-e2b/pytorch/default/1/AuraMind-E2B-Finetuned-Sliced\"\n",
    "\n",
    "# Path where the final ONNX model will be saved\n",
    "onnx_output_path = \"./onnx_multimodal_model\"\n",
    "\n",
    "print(f\"Targeting SLICED model path: {pytorch_model_path}\")\n",
    "print(f\"ONNX output path: {onnx_output_path}\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CELL 1: DEFINE THE CUSTOM ONNX CONFIGURATION FOR IMAGE-TO-TEXT\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 1: Defining a custom ONNX configuration for the image-to-text model...\")\n",
    "\n",
    "class CustomGemma3NImageToTextOnnxConfig(TextDecoderOnnxConfig):\n",
    "    NORMALIZED_CONFIG_CLASS = NormalizedTextConfig.with_args(\n",
    "        num_layers=\"num_hidden_layers\",\n",
    "        num_attention_heads=\"num_attention_heads\",\n",
    "        hidden_size=\"hidden_size\",\n",
    "    )\n",
    "\n",
    "    def __init__(self, config: PretrainedConfig, task: str = \"default\", **kwargs):\n",
    "        super().__init__(config=config.text_config, task=task, **kwargs)\n",
    "        self.text_config = config.text_config\n",
    "        self.vision_config = config.vision_config\n",
    "        self.config = config\n",
    "\n",
    "    @property\n",
    "    def inputs(self) -> Dict[str, Dict[int, str]]:\n",
    "        text_inputs = super().inputs\n",
    "        vision_inputs = {\n",
    "            \"pixel_values\": {0: \"batch_size\", 1: \"num_channels\", 2: \"height\", 3: \"width\"}\n",
    "        }\n",
    "        return {**vision_inputs, **text_inputs}\n",
    "\n",
    "    # Using a minimal sequence length to conserve memory during export\n",
    "    def generate_dummy_inputs(self, batch_size: int = 1, sequence_length: int = 260, **kwargs) -> Dict[str, torch.Tensor]:\n",
    "        text_input_generator = DummyTextInputGenerator(\n",
    "            self.task, self._normalized_config, batch_size=batch_size, sequence_length=sequence_length, **self.text_config.to_dict(),\n",
    "        )\n",
    "        dummy_inputs = {\n",
    "            \"input_ids\": text_input_generator.generate(input_name=\"input_ids\", framework=\"pt\"),\n",
    "            \"attention_mask\": text_input_generator.generate(input_name=\"attention_mask\", framework=\"pt\"),\n",
    "        }\n",
    "        image_token_id = self.config.image_token_id\n",
    "        tokens_per_image = self.config.vision_soft_tokens_per_image\n",
    "        if sequence_length < tokens_per_image:\n",
    "            raise ValueError(f\"Sequence length must be at least {tokens_per_image} to hold image tokens.\")\n",
    "        for i in range(batch_size):\n",
    "            dummy_inputs[\"input_ids\"][i, :tokens_per_image] = image_token_id\n",
    "        if self.use_past:\n",
    "            past_key_values_generator = DummyPastKeyValuesGenerator(\n",
    "                self.task, self._normalized_config, batch_size=batch_size, sequence_length=sequence_length,\n",
    "            )\n",
    "            dummy_inputs.update(past_key_values_generator.generate(framework=\"pt\"))\n",
    "        normalized_vision_config = NormalizedVisionConfig(self.vision_config)\n",
    "        vision_input_generator = DummyVisionInputGenerator(\n",
    "            self.task, normalized_vision_config, batch_size=batch_size, num_channels=3, height=224, width=224,\n",
    "        )\n",
    "        dummy_inputs[\"pixel_values\"] = vision_input_generator.generate(input_name=\"pixel_values\", framework=\"pt\")\n",
    "        return dummy_inputs\n",
    "\n",
    "print(\"   - CustomGemma3NImageToTextOnnxConfig defined.\")\n",
    "\n",
    "# ==============================================================================\n",
    "# CELL 2: PREPARE AND RUN THE ONNX EXPORT\n",
    "# ==============================================================================\n",
    "print(\"\\nStep 2: Preparing and running the ONNX export...\")\n",
    "\n",
    "try:\n",
    "    # --- Step 2.1: Load, Clean, and Fix the Sliced Model's Configuration ---\n",
    "    print(\"   - Loading and fixing configuration for the SLICED model...\")\n",
    "    config = AutoConfig.from_pretrained(pytorch_model_path, trust_remote_code=True)\n",
    "    \n",
    "    # Clean the config by removing Unsloth and quantization artifacts\n",
    "    if hasattr(config, \"quantization_config\"):\n",
    "        delattr(config, \"quantization_config\")\n",
    "        print(\"   - Removed 'quantization_config'.\")\n",
    "    if hasattr(config, \"unsloth_fixed\"):\n",
    "        delattr(config, \"unsloth_fixed\")\n",
    "        print(\"   - Removed 'unsloth_fixed'.\")\n",
    "\n",
    "    # Manually set the model_type to fix the recognition issue\n",
    "    config.model_type = \"gemma3n\"\n",
    "    print(f\"   - Manually set model_type to: {config.model_type}\")\n",
    "\n",
    "    # --- Step 2.2: Load the Sliced Model with the Corrected Config ---\n",
    "    print(\"   - Loading SLICED model with the corrected configuration...\")\n",
    "    # Using float16 to reduce memory footprint\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        pytorch_model_path,\n",
    "        config=config, # Pass the fixed config object here\n",
    "        torch_dtype=torch.float16, \n",
    "        trust_remote_code=True,\n",
    "    )\n",
    "    model.eval()\n",
    "    print(\"   - Sliced model loaded successfully!\")\n",
    "\n",
    "    # --- Step 2.3: Configure and Run the ONNX Export ---\n",
    "    print(\"   - Configuring the ONNX export...\")\n",
    "    custom_onnx_config = CustomGemma3NImageToTextOnnxConfig(\n",
    "        config=model.config, \n",
    "        task=\"text-generation\"\n",
    "    )\n",
    "\n",
    "    # --- Step 2.4: Clean up memory before export ---\n",
    "    print(\"   - Cleaning up memory before starting export...\")\n",
    "    gc.collect()\n",
    "    if torch.cuda.is_available():\n",
    "        torch.cuda.empty_cache()\n",
    "\n",
    "    print(\"   - Starting ONNX export...\")\n",
    "    onnx_export(\n",
    "        model=model,\n",
    "        config=custom_onnx_config,\n",
    "        output=Path(onnx_output_path),\n",
    "        opset=14,\n",
    "    )\n",
    "    print(\"\\n✅ ONNX conversion process completed successfully!\")\n",
    "    print(f\"   The exported model is saved in: {Path(onnx_output_path).resolve()}\")\n",
    "\n",
    "except Exception as e:\n",
    "    print(f\"\\n❌ An error occurred during the ONNX conversion process: {e}\")\n",
    "    import traceback\n",
    "    traceback.print_exc()\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "none",
   "dataSources": [
    {
     "datasetId": 8008873,
     "sourceId": 12673265,
     "sourceType": "datasetVersion"
    },
    {
     "sourceId": 253771701,
     "sourceType": "kernelVersion"
    },
    {
     "isSourceIdPinned": true,
     "modelId": 418878,
     "modelInstanceId": 400668,
     "sourceId": 504461,
     "sourceType": "modelInstanceVersion"
    }
   ],
   "dockerImageVersionId": 31090,
   "isGpuEnabled": false,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
