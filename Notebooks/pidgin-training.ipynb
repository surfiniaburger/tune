{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":12683693,"sourceType":"datasetVersion","datasetId":8015675}],"dockerImageVersionId":31090,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"    # Do this only in Colab notebooks! Otherwise use pip install unsloth\n!pip install --no-deps bitsandbytes accelerate xformers==0.0.29.post3 peft trl triton cut_cross_entropy -qU\n!pip install sentencepiece protobuf \"datasets>=3.4.1,<4.0.0\" \"huggingface_hub>=0.34.0\" hf_transfer -qU\n!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth-zoo.git -qU\n!pip install --force-reinstall --no-deps git+https://github.com/unslothai/unsloth.git -qU\n!pip install snac -qU","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-08-05T20:28:38.601777Z","iopub.execute_input":"2025-08-05T20:28:38.602029Z","iopub.status.idle":"2025-08-05T20:30:58.262005Z","shell.execute_reply.started":"2025-08-05T20:28:38.602008Z","shell.execute_reply":"2025-08-05T20:30:58.261273Z"}},"outputs":[{"name":"stdout","text":"\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m43.4/43.4 MB\u001b[0m \u001b[31m42.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m72.9/72.9 MB\u001b[0m \u001b[31m24.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m367.1/367.1 kB\u001b[0m \u001b[31m28.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m503.9/503.9 kB\u001b[0m \u001b[31m33.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m511.9/511.9 kB\u001b[0m \u001b[31m33.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m155.5/155.5 MB\u001b[0m \u001b[31m10.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m321.1/321.1 kB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m558.8/558.8 kB\u001b[0m \u001b[31m25.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m193.6/193.6 kB\u001b[0m \u001b[31m14.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nbigframes 2.8.0 requires google-cloud-bigquery-storage<3.0.0,>=2.30.0, which is not installed.\ntrl 0.21.0 requires transformers>=4.55.0, but you have transformers 4.52.4 which is incompatible.\ngoogle-api-core 1.34.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<4.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\ngoogle-cloud-translate 3.12.1 requires protobuf!=3.20.0,!=3.20.1,!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<5.0.0dev,>=3.19.5, but you have protobuf 6.31.1 which is incompatible.\ncesium 0.12.4 requires numpy<3.0,>=2.0, but you have numpy 1.26.4 which is incompatible.\npandas-gbq 0.29.1 requires google-api-core<3.0.0,>=2.10.2, but you have google-api-core 1.34.1 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cublas-cu12==12.4.5.8; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cublas-cu12 12.5.3.2 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-cupti-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-cupti-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-nvrtc-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-nvrtc-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cuda-runtime-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cuda-runtime-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cudnn-cu12==9.1.0.70; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cudnn-cu12 9.3.0.75 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cufft-cu12==11.2.1.3; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cufft-cu12 11.2.3.61 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-curand-cu12==10.3.5.147; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-curand-cu12 10.3.6.82 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusolver-cu12==11.6.1.9; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusolver-cu12 11.6.3.83 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-cusparse-cu12==12.3.1.170; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-cusparse-cu12 12.5.1.3 which is incompatible.\ntorch 2.6.0+cu124 requires nvidia-nvjitlink-cu12==12.4.127; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have nvidia-nvjitlink-cu12 12.5.82 which is incompatible.\ntorch 2.6.0+cu124 requires triton==3.2.0; platform_system == \"Linux\" and platform_machine == \"x86_64\", but you have triton 3.4.0 which is incompatible.\ngoogle-ai-generativelanguage 0.6.15 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.2, but you have protobuf 6.31.1 which is incompatible.\ngcsfs 2025.3.2 requires fsspec==2025.3.2, but you have fsspec 2025.3.0 which is incompatible.\ngoogle-cloud-storage 2.19.0 requires google-api-core<3.0.0dev,>=2.15.0, but you have google-api-core 1.34.1 which is incompatible.\ndataproc-spark-connect 0.7.5 requires google-api-core>=2.19, but you have google-api-core 1.34.1 which is incompatible.\ntensorflow 2.18.0 requires protobuf!=4.21.0,!=4.21.1,!=4.21.2,!=4.21.3,!=4.21.4,!=4.21.5,<6.0.0dev,>=3.20.3, but you have protobuf 6.31.1 which is incompatible.\nbigframes 2.8.0 requires google-cloud-bigquery[bqstorage,pandas]>=3.31.0, but you have google-cloud-bigquery 3.25.0 which is incompatible.\nbigframes 2.8.0 requires rich<14,>=12.4.4, but you have rich 14.0.0 which is incompatible.\u001b[0m\u001b[31m\n\u001b[0m  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for unsloth_zoo (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n  Building wheel for unsloth (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m4.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m0:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m65.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m44.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m2.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m0:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m8.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m67.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[2K   \u001b[90mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m \u001b[32m253.2/253.2 MB\u001b[0m \u001b[31m6.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m:00:01\u001b[0m00:01\u001b[0m\n\u001b[?25h\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\nunsloth-zoo 2025.8.1 requires msgspec, which is not installed.\nunsloth-zoo 2025.8.1 requires tyro, which is not installed.\u001b[0m\u001b[31m\n\u001b[0m","output_type":"stream"}],"execution_count":1},{"cell_type":"code","source":"from unsloth import FastLanguageModel\nimport torch\n\nfourbit_models = [\n    # 4bit dynamic quants for superior accuracy and low memory use\n    \"unsloth/gemma-3-4b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-12b-it-unsloth-bnb-4bit\",\n    \"unsloth/gemma-3-27b-it-unsloth-bnb-4bit\",\n    # Qwen3 new models\n    \"unsloth/Qwen3-4B-unsloth-bnb-4bit\",\n    \"unsloth/Qwen3-8B-unsloth-bnb-4bit\",\n    # Other very popular models!\n    \"unsloth/Llama-3.1-8B\",\n    \"unsloth/Llama-3.2-3B\",\n    \"unsloth/Llama-3.3-70B\",\n    \"unsloth/mistral-7b-instruct-v0.3\",\n    \"unsloth/Phi-4\",\n] # More models at https://huggingface.co/unsloth\n\nmodel, tokenizer = FastLanguageModel.from_pretrained(\n    model_name = \"unsloth/orpheus-3b-0.1-ft\",\n    max_seq_length= 2048, # Choose any for long context!\n    dtype = None, # Select None for auto detection\n    load_in_4bit = False, # Select True for 4bit which reduces memory usage\n    # token = \"hf_...\", # use one if using gated models like meta-llama/Llama-2-7b-hf\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T20:30:58.263399Z","iopub.execute_input":"2025-08-05T20:30:58.263646Z","iopub.status.idle":"2025-08-05T20:32:35.423204Z","shell.execute_reply.started":"2025-08-05T20:30:58.263621Z","shell.execute_reply":"2025-08-05T20:32:35.422642Z"}},"outputs":[{"name":"stdout","text":"ğŸ¦¥ Unsloth: Will patch your computer to enable 2x faster free finetuning.\n","output_type":"stream"},{"name":"stderr","text":"2025-08-05 20:31:12.536407: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\nWARNING: All log messages before absl::InitializeLog() is called are written to STDERR\nE0000 00:00:1754425872.764661      36 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\nE0000 00:00:1754425872.817960      36 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","output_type":"stream"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"traceback":["\u001b[0;31m---------------------------------------------------------------------------\u001b[0m","\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)","\u001b[0;31mAttributeError\u001b[0m: 'MessageFactory' object has no attribute 'GetPrototype'"],"ename":"AttributeError","evalue":"'MessageFactory' object has no attribute 'GetPrototype'","output_type":"error"},{"name":"stdout","text":"ğŸ¦¥ Unsloth Zoo will now patch everything to make training faster!\n==((====))==  Unsloth 2025.8.1: Fast Llama patching. Transformers: 4.52.4.\n   \\\\   /|    Tesla T4. Num GPUs = 2. Max memory: 14.741 GB. Platform: Linux.\nO^O/ \\_/ \\    Torch: 2.6.0+cu124. CUDA: 7.5. CUDA Toolkit: 12.4. Triton: 3.2.0\n\\        /    Bfloat16 = FALSE. FA [Xformers = 0.0.29.post3. FA2 = False]\n \"-____-\"     Free license: http://github.com/unslothai/unsloth\nUnsloth: Fast downloading is enabled - ignore downloading bars which are red colored!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"model.safetensors.index.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b46fd4cc84924458ae07281e9c979e75"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"f5ceddbd91ba4a7884d10c48843384e4"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"ce9aeee4ef0e422493afb8aa6eddc1bd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Loading checkpoint shards:   0%|          | 0/2 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb9b94580a0b4ae0a34c7710de3f2470"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"generation_config.json:   0%|          | 0.00/230 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"df43dbbbac3c4621b709077ea98b6d6c"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer_config.json: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"bb52f91b3613420aaab5a665a0cb3d35"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/22.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"11f6f2945dfe49898e24468c0b8bee46"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"special_tokens_map.json:   0%|          | 0.00/508 [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"49f78bab7fa249da9cc4f4baf1a5b5b5"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"chat_template.jinja: 0.00B [00:00, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"936bc1fbb9ee401b91fc4774df61a506"}},"metadata":{}}],"execution_count":2},{"cell_type":"code","source":"model = FastLanguageModel.get_peft_model(\n    model,\n    r = 64,  # A higher rank to capture more vocal nuance.\n    target_modules = [\"q_proj\", \"k_proj\", \"v_proj\", \"o_proj\",\n                      \"gate_proj\", \"up_proj\", \"down_proj\",],\n    lora_alpha = 64, # Keep alpha equal to r for a balanced starting point.\n    lora_dropout = 0.05, # Add a small amount of dropout to prevent overfitting.\n    bias = \"none\",\n    use_gradient_checkpointing = \"unsloth\",\n    random_state = 3407,\n    use_rslora = False,\n    loftq_config = None,\n)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T20:32:35.423937Z","iopub.execute_input":"2025-08-05T20:32:35.424154Z","iopub.status.idle":"2025-08-05T20:32:42.699149Z","shell.execute_reply.started":"2025-08-05T20:32:35.424136Z","shell.execute_reply":"2025-08-05T20:32:42.698364Z"}},"outputs":[{"name":"stderr","text":"Unsloth: Dropout = 0 is supported for fast patching. You are using dropout = 0.05.\nUnsloth will patch all other layers, except LoRA matrices, causing a performance hit.\nUnsloth 2025.8.1 patched 28 layers with 0 QKV layers, 0 O layers and 0 MLP layers.\n","output_type":"stream"}],"execution_count":3},{"cell_type":"code","source":"from datasets import load_dataset, Audio\nimport os\nimport locale\nimport torch\nimport torchaudio.transforms as T\nfrom snac import SNAC\n\n# --- 1. Load Your Custom Dataset ---\nprint(\"Loading custom dataset from Kaggle...\")\n\n# Define the base path where your dataset is located\nbase_path = \"/kaggle/input/audio-datasets/audio_dataset/\"\nmetadata_path = os.path.join(base_path, \"metadata.csv\")\n\n# Load the dataset from your metadata.csv file\n# This initially creates a dataset with 'filename' and 'text' columns.\ndataset = load_dataset(\"csv\", data_files=metadata_path, split=\"train\")\n\n# The 'datasets' library needs the full path to load the audio.\n# We'll create a new column 'full_path' with the absolute path to each wav file.\ndef create_full_path(example):\n    example[\"full_path\"] = os.path.join(base_path, example[\"filename\"])\n    return example\n\ndataset = dataset.map(create_full_path)\n\n# Now, cast the 'full_path' column to the Audio type.\n# This loads the audio data into the dataset, creating the 'audio' column\n# that the original script expects. We set the sampling rate to 24000 to match the model.\ndataset = dataset.cast_column(\"full_path\", Audio(sampling_rate=24000))\n\n# Rename the 'full_path' column to 'audio' to match the original script's structure\ndataset = dataset.rename_column(\"full_path\", \"audio\")\n\nprint(\"Custom dataset loaded successfully!\")\nprint(f\"Dataset features: {dataset.features}\")\nprint(f\"First example: {dataset[0]}\")\n\n\n# --- 2. The Rest of Your Preprocessing Script (with minor adjustments) ---\n\n# The rest of the script can now proceed, as our 'dataset' object\n# has the required 'audio' and 'text' columns.\n\nlocale.getpreferredencoding = lambda: \"UTF-8\"\n# The sample rate is now 24000, as we cast it during loading.\nds_sample_rate = 24000\n\nprint(\"Initializing SNAC model for audio tokenization...\")\nsnac_model = SNAC.from_pretrained(\"hubertsiuzdak/snac_24khz\")\nsnac_model = snac_model.to(\"cuda\")\nprint(\"SNAC model loaded.\")\n\ndef tokenise_audio(waveform):\n  waveform = torch.from_numpy(waveform).unsqueeze(0)\n  waveform = waveform.to(dtype=torch.float32)\n\n  # No need to resample, we already loaded at 24kHz\n  # resample_transform = T.Resample(orig_freq=ds_sample_rate, new_freq=24000)\n  # waveform = resample_transform(waveform)\n\n  waveform = waveform.unsqueeze(0).to(\"cuda\")\n\n  #generate the codes from snac\n  with torch.inference_mode():\n    codes = snac_model.encode(waveform)\n\n  all_codes = []\n  for i in range(codes[0].shape[1]):\n    all_codes.append(codes[0][0][i].item()+128266)\n    all_codes.append(codes[1][0][2*i].item()+128266+4096)\n    all_codes.append(codes[2][0][4*i].item()+128266+(2*4096))\n    all_codes.append(codes[2][0][(4*i)+1].item()+128266+(3*4096))\n    all_codes.append(codes[1][0][(2*i)+1].item()+128266+(4*4096))\n    all_codes.append(codes[2][0][(4*i)+2].item()+128266+(5*4096))\n    all_codes.append(codes[2][0][(4*i)+3].item()+128266+(6*4096))\n\n  return all_codes\n\ndef add_codes(example):\n    codes_list = None\n    try:\n        answer_audio = example.get(\"audio\")\n        if answer_audio and \"array\" in answer_audio:\n            audio_array = answer_audio[\"array\"]\n            codes_list = tokenise_audio(audio_array)\n    except Exception as e:\n        print(f\"Skipping row due to error: {e}\")\n    example[\"codes_list\"] = codes_list\n    return example\n\nprint(\"Tokenizing audio data...\")\ndataset = dataset.map(add_codes, remove_columns=[\"audio\"])\nprint(\"Audio tokenization complete.\")\n\ntokeniser_length = 128256\nstart_of_text = 128000\nend_of_text = 128009\nstart_of_speech = tokeniser_length + 1\nend_of_speech = tokeniser_length + 2\nstart_of_human = tokeniser_length + 3\nend_of_human = tokeniser_length + 4\nstart_of_ai = tokeniser_length + 5\nend_of_ai =  tokeniser_length + 6\npad_token = tokeniser_length + 7\naudio_tokens_start = tokeniser_length + 10\n\nprint(\"Filtering out any failed rows...\")\ndataset = dataset.filter(lambda x: x[\"codes_list\"] is not None)\ndataset = dataset.filter(lambda x: len(x[\"codes_list\"]) > 0)\n\ndef remove_duplicate_frames(example):\n    vals = example[\"codes_list\"]\n    if len(vals) % 7 != 0:\n        # Pad with the last frame if it's not divisible by 7\n        remainder = len(vals) % 7\n        if remainder != 0:\n            padding = vals[-7:] * (7 - remainder)\n            vals.extend(padding)\n\n    result = vals[:7]\n    for i in range(7, len(vals), 7):\n        if vals[i] != result[-7]:\n            result.extend(vals[i:i+7])\n    example[\"codes_list\"] = result\n    return example\n\nprint(\"Removing duplicate audio frames...\")\ndataset = dataset.map(remove_duplicate_frames)\n\ntok_info = '''*** Using single-speaker prompt format.\nYour dataset does not have a 'source' field, so the prompt will be formatted as:\nf\"{example['text']}\"\nThis is the correct setting for a single-speaker (your voice) model.\n'''\nprint(tok_info)\n\n# --- STEP 1: FIX THE DATA PREPROCESSING (FINAL) ---\n\n# The Unsloth log clearly states the effective max sequence length is 2048.\n# We will use this value to manually truncate our data.\nmax_seq_length = 2048\nprint(f\"Using effective max sequence length: {max_seq_length}\")\n\n# This function creates inputs and labels, and truncates them if they are too long.\ndef create_and_truncate_inputs(example):\n    text_prompt = example[\"text\"]\n    text_ids = tokenizer.encode(text_prompt, add_special_tokens=True)\n    text_ids.append(end_of_text)\n\n    input_ids = (\n        [start_of_human]\n        + text_ids\n        + [end_of_human]\n        + [start_of_ai]\n        + [start_of_speech]\n        + example[\"codes_list\"]\n        + [end_of_speech]\n        + [end_of_ai]\n    )\n    labels = list(input_ids)\n\n    # *** THE CRITICAL FIX ***\n    # Manually truncate BOTH input_ids and labels if they exceed the max length.\n    if len(input_ids) > max_seq_length:\n        print(f\"Truncating sample with original length {len(input_ids)} to {max_seq_length}.\")\n        input_ids = input_ids[:max_seq_length]\n        labels = labels[:max_seq_length]\n\n    example[\"input_ids\"] = input_ids\n    example[\"labels\"] = labels\n    example[\"attention_mask\"] = [1] * len(input_ids)\n    return example\n\nprint(\"\\nApplying final preprocessing with manual truncation to 2048 tokens...\")\ndataset = dataset.map(create_and_truncate_inputs, remove_columns=[\"text\", \"codes_list\", \"filename\"])\n\nprint(\"\\nPreprocessing complete. All sequences are now guaranteed to be <= 2048.\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T20:48:04.677486Z","iopub.execute_input":"2025-08-05T20:48:04.678109Z","iopub.status.idle":"2025-08-05T20:48:08.077055Z","shell.execute_reply.started":"2025-08-05T20:48:04.678086Z","shell.execute_reply":"2025-08-05T20:48:08.076266Z"}},"outputs":[{"name":"stdout","text":"Loading custom dataset from Kaggle...\nCustom dataset loaded successfully!\nDataset features: {'filename': Value(dtype='string', id=None), 'text': Value(dtype='string', id=None), 'audio': Audio(sampling_rate=24000, mono=True, decode=True, id=None)}\nFirst example: {'filename': 'wavs/recording.wav', 'text': 'How You Fit Solve Phosphorus Problem for Your Corn (Maize)', 'audio': {'path': '/kaggle/input/audio-datasets/audio_dataset/wavs/recording.wav', 'array': array([-1.53477231e-12, -9.09494702e-13, -2.87059265e-12, ...,\n       -2.81774439e-04, -2.39072251e-04, -2.44010967e-04]), 'sampling_rate': 24000}}\nInitializing SNAC model for audio tokenization...\nSNAC model loaded.\nTokenizing audio data...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"98e3ae1f999f45b1ac85f26f9424f8f3"}},"metadata":{}},{"name":"stdout","text":"Audio tokenization complete.\nFiltering out any failed rows...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/17 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"9be807471b7b41bcbb4a0f22311ffbcd"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Filter:   0%|          | 0/17 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"061a616613584e89a13069d0971c9c96"}},"metadata":{}},{"name":"stdout","text":"Removing duplicate audio frames...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"3e312d37013a46a6b4290d00acf04363"}},"metadata":{}},{"name":"stdout","text":"*** Using single-speaker prompt format.\nYour dataset does not have a 'source' field, so the prompt will be formatted as:\nf\"{example['text']}\"\nThis is the correct setting for a single-speaker (your voice) model.\n\nUsing effective max sequence length: 2048\n\nApplying final preprocessing with manual truncation to 2048 tokens...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Map:   0%|          | 0/17 [00:00<?, ? examples/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"7505136336104726bd954889c24a9881"}},"metadata":{}},{"name":"stdout","text":"Truncating sample with original length 2250 to 2048.\nTruncating sample with original length 3624 to 2048.\nTruncating sample with original length 2381 to 2048.\nTruncating sample with original length 2085 to 2048.\nTruncating sample with original length 4875 to 2048.\nTruncating sample with original length 3917 to 2048.\nTruncating sample with original length 2505 to 2048.\n\nPreprocessing complete. All sequences are now guaranteed to be <= 2048.\n","output_type":"stream"}],"execution_count":6},{"cell_type":"code","source":"# --- STEP 2: RUN THE TRAINER (FINAL) ---\n\nfrom transformers import TrainingArguments, Trainer, DataCollatorForSeq2Seq\n\n# 1. Ensure the tokenizer has the correct pad token ID set.\npad_token_id = 128263\ntokenizer.pad_token_id = pad_token_id\ntokenizer.pad_token = tokenizer.convert_ids_to_tokens(pad_token_id)\n\n# 2. Use DataCollatorForSeq2Seq to correctly pad both inputs and labels.\ndata_collator = DataCollatorForSeq2Seq(\n    tokenizer,\n    model=model,\n    label_pad_token_id=-100,\n    pad_to_multiple_of=8\n)\n\n# 3. Create the Trainer.\ntrainer = Trainer(\n    model=model,\n    train_dataset=dataset,\n    args=TrainingArguments(\n        per_device_train_batch_size=2,\n        gradient_accumulation_steps=4,\n        warmup_steps=10,\n        max_steps=150,\n        learning_rate=2e-4,\n        logging_steps=10,\n        optim=\"adamw_8bit\",\n        weight_decay=0.01,\n        lr_scheduler_type=\"linear\",\n        seed=3407,\n        output_dir=\"outputs\",\n        report_to=\"none\",\n    ),\n    data_collator=data_collator,\n)\n\n# 4. Start training.\nprint(\"\\nStarting training with correctly sized and prepared data...\")\ntrainer_stats = trainer.train()\nprint(\"Training complete!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T20:48:17.517149Z","iopub.execute_input":"2025-08-05T20:48:17.517481Z","iopub.status.idle":"2025-08-05T22:09:59.717052Z","shell.execute_reply.started":"2025-08-05T20:48:17.517422Z","shell.execute_reply":"2025-08-05T22:09:59.716274Z"}},"outputs":[{"name":"stderr","text":"==((====))==  Unsloth - 2x faster free finetuning | Num GPUs used = 1\n   \\\\   /|    Num examples = 17 | Num Epochs = 75 | Total steps = 150\nO^O/ \\_/ \\    Batch size per device = 4 | Gradient accumulation steps = 4\n\\        /    Data Parallel GPUs = 1 | Total batch size (4 x 4 x 1) = 16\n \"-____-\"     Trainable parameters = 97,255,424 of 3,398,122,496 (2.86% trained)\n","output_type":"stream"},{"name":"stdout","text":"\nStarting training with correctly sized and prepared data...\nUnsloth: Will smartly offload gradients to save VRAM!\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n    <div>\n      \n      <progress value='150' max='150' style='width:300px; height:20px; vertical-align: middle;'></progress>\n      [150/150 1:20:37, Epoch 75/75]\n    </div>\n    <table border=\"1\" class=\"dataframe\">\n  <thead>\n <tr style=\"text-align: left;\">\n      <th>Step</th>\n      <th>Training Loss</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <td>10</td>\n      <td>5.216500</td>\n    </tr>\n    <tr>\n      <td>20</td>\n      <td>5.155900</td>\n    </tr>\n    <tr>\n      <td>30</td>\n      <td>5.196900</td>\n    </tr>\n    <tr>\n      <td>40</td>\n      <td>5.105800</td>\n    </tr>\n    <tr>\n      <td>50</td>\n      <td>5.145500</td>\n    </tr>\n    <tr>\n      <td>60</td>\n      <td>5.015500</td>\n    </tr>\n    <tr>\n      <td>70</td>\n      <td>4.845500</td>\n    </tr>\n    <tr>\n      <td>80</td>\n      <td>4.625200</td>\n    </tr>\n    <tr>\n      <td>90</td>\n      <td>4.383300</td>\n    </tr>\n    <tr>\n      <td>100</td>\n      <td>4.149800</td>\n    </tr>\n    <tr>\n      <td>110</td>\n      <td>4.067300</td>\n    </tr>\n    <tr>\n      <td>120</td>\n      <td>3.645300</td>\n    </tr>\n    <tr>\n      <td>130</td>\n      <td>2.969600</td>\n    </tr>\n    <tr>\n      <td>140</td>\n      <td>2.572100</td>\n    </tr>\n    <tr>\n      <td>150</td>\n      <td>2.580100</td>\n    </tr>\n  </tbody>\n</table><p>"},"metadata":{}},{"name":"stdout","text":"Training complete!\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"# It is CRITICAL to merge the LoRA adapters into the base model to create a\n# standalone model that you can easily use for inference with MLX.\n# The 'merged_16bit' format is the best choice for quality and compatibility.\n\nprint(\"Merging LoRA adapters and saving the final model to 16-bit format...\")\n\n# --- A. (Optional but Recommended) Save a copy inside the Kaggle environment ---\n# This gives you a backup you can download directly if needed.\nmodel.save_pretrained_merged(\"fine-tuned-orpheus-voice-16bit\", tokenizer, save_method=\"merged_16bit\")\nprint(\"Local merged model saved to 'fine-tuned-orpheus-voice-16bit'.\")\n\n\n# --- B. Push the Merged Model to the Hugging Face Hub ---\n# This is the best way to get the model to your local machine.\n\n# 1. Add your Hugging Face WRITE token as a \"Secret\" in your Kaggle Notebook.\n#    - In the right-hand pane, click \"Add-ons\" -> \"Secrets\" -> \"Add a new secret\"\n#    - Secret Label: HF_TOKEN\n#    - Secret Value: Paste your Hugging Face token (e.g., \"hf_...\")\n\n# 2. Log in to Hugging Face using your secret token.\nfrom huggingface_hub import login\nfrom kaggle_secrets import UserSecretsClient\nuser_secrets = UserSecretsClient()\nhf_token = user_secrets.get_secret(\"HUGGINGFACE_API_KEY\")\nlogin(token = hf_token)\nprint(\"Successfully logged into Hugging Face Hub.\")\n\n# 3. Push the model to the Hub.\n#    Replace \"YourHuggingFaceUsername\" with your actual username.\n#    Give your model a descriptive name.\nhf_model_repo = \"surfiniaburger/orpheus-3b-pidgin-voice-v1\"\nprint(f\"Pushing merged model to: {hf_model_repo}\")\n\nmodel.push_to_hub_merged(hf_model_repo, tokenizer, save_method=\"merged_16bit\")\n\nprint(\"Model successfully pushed to the Hugging Face Hub!\")","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-08-05T22:09:59.718245Z","iopub.execute_input":"2025-08-05T22:09:59.718598Z","iopub.status.idle":"2025-08-05T22:12:41.037536Z","shell.execute_reply.started":"2025-08-05T22:09:59.718577Z","shell.execute_reply":"2025-08-05T22:12:41.036852Z"}},"outputs":[{"name":"stdout","text":"Merging LoRA adapters and saving the final model to 16-bit format...\nFound HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\nSuccessfully copied all 2 files from cache to fine-tuned-orpheus-voice-16bit.\nDownloading safetensors index for unsloth/orpheus-3b-0.1-ft...\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:36<00:00, 18.26s/it]\n","output_type":"stream"},{"name":"stdout","text":"Local merged model saved to 'fine-tuned-orpheus-voice-16bit'.\nSuccessfully logged into Hugging Face Hub.\nPushing merged model to: surfiniaburger/orpheus-3b-pidgin-voice-v1\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"288a1dcaffa9413d8af47c4a5f3fa5be"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"tokenizer.json:   0%|          | 0.00/22.8M [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"5bbd0ebf54114c8bbad074198d18680f"}},"metadata":{}},{"name":"stdout","text":"Found HuggingFace hub cache directory: /root/.cache/huggingface/hub\nChecking cache directory for required files...\nSuccessfully copied all 2 files from cache to surfiniaburger/orpheus-3b-pidgin-voice-v1.\nDownloading safetensors index for unsloth/orpheus-3b-0.1-ft...\n","output_type":"stream"},{"name":"stderr","text":"Unsloth: Merging weights into 16bit:   0%|          | 0/2 [00:00<?, ?it/s]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"70fb6ee5d07c4e56ae4e694263775daa"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00001-of-00002.safetensors:   0%|          | 0.00/4.99G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"8e795558b55845f7995515ca04de7ce7"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit:  50%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ     | 1/2 [01:01<01:01, 61.66s/it]","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"  0%|          | 0/1 [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"532893b12926463aa6cb12a00f74ff81"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"model-00002-of-00002.safetensors:   0%|          | 0.00/1.61G [00:00<?, ?B/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"631da94df0dc427f9739445ef46f63aa"}},"metadata":{}},{"name":"stderr","text":"Unsloth: Merging weights into 16bit: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [01:22<00:00, 41.27s/it]\n","output_type":"stream"},{"name":"stdout","text":"Model successfully pushed to the Hugging Face Hub!\n","output_type":"stream"}],"execution_count":8}]}